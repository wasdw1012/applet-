#!/usr/bin/env python3
"""
JavaScriptæ·±åº¦æŒ–æ˜å¼•æ“ (Deep JavaScript Mining Engine)
ä¸“é—¨ä»ç°ä»£Webåº”ç”¨çš„JSæ–‡ä»¶ä¸­æŒ–æ˜éšè—çš„APIç«¯ç‚¹å’Œè·¯å¾„
è¿™æ˜¯é’ˆå¯¹SPAå’Œç°ä»£Webåº”ç”¨çš„æ ¸å¿ƒæ­¦å™¨ï¼
"""

import asyncio
import aiohttp
import ssl
import re
import json
import time
from datetime import datetime
from typing import List, Dict, Set, Any
from urllib.parse import urljoin, urlparse
import jsbeautifier
from playwright.async_api import async_playwright

class JavaScriptMiningEngine:
    def __init__(self, target: str):
        self.target = target.rstrip('/')
        self.discovered_js_files: Set[str] = set()
        self.extracted_endpoints: Set[str] = set()
        self.extracted_paths: Set[str] = set()
        self.session = None
        self.results = []
        
        # APIç«¯ç‚¹åŒ¹é…æ­£åˆ™è¡¨è¾¾å¼
        self.api_patterns = [
            # REST APIæ¨¡å¼
            r'["\'/](api|apis?)["\'/][a-zA-Z0-9/_-]+',
            r'["\'/]v\d+["\'/][a-zA-Z0-9/_-]+',
            r'["\'/](graphql|graph)["\'/]?',
            r'["\'/](rest|REST)["\'/][a-zA-Z0-9/_-]+',
            
            # åå°ç®¡ç†è·¯å¾„
            r'["\'/](admin|administrator|manage|manager|backend|dashboard)["\'/][a-zA-Z0-9/_-]*',
            r'["\'/](panel|control|console|cp)["\'/][a-zA-Z0-9/_-]*',
            
            # åŒ»ç–—ç³»ç»Ÿç‰¹å®šAPI
            r'["\'/](patient|patients|medical|doctor|doctors|appointment|appointments)["\'/][a-zA-Z0-9/_-]*',
            r'["\'/](record|records|diagnosis|prescription|treatment)["\'/][a-zA-Z0-9/_-]*',
            r'["\'/](staff|nurse|clinic|hospital)["\'/][a-zA-Z0-9/_-]*',
            
            # å¸¸è§åŠŸèƒ½è·¯å¾„
            r'["\'/](auth|login|logout|register|profile|user|users)["\'/][a-zA-Z0-9/_-]*',
            r'["\'/](upload|download|file|files|media|image|images)["\'/][a-zA-Z0-9/_-]*',
            r'["\'/](search|query|filter|export|import)["\'/][a-zA-Z0-9/_-]*',
            r'["\'/](config|settings|preference|option)["\'/][a-zA-Z0-9/_-]*',
            
            # æ•°æ®æ“ä½œè·¯å¾„  
            r'["\'/](create|read|update|delete|get|post|put|patch)["\'/][a-zA-Z0-9/_-]*',
            r'["\'/](list|view|edit|add|remove|save)["\'/][a-zA-Z0-9/_-]*',
            
            # é€šç”¨è·¯å¾„æ¨¡å¼
            r'["\']\/[a-zA-Z][a-zA-Z0-9/_-]{2,}["\']',
            
            # URLå‚æ•°å’ŒæŸ¥è¯¢
            r'["\'][a-zA-Z0-9/_-]+\?[a-zA-Z0-9&=_-]+["\']',
            
            # åŸŸåç›¸å¯¹è·¯å¾„
            r'["\']\.\/[a-zA-Z0-9/_.-]+["\']',
            r'["\']\.\.\/[a-zA-Z0-9/_.-]+["\']'
        ]
        
        # æ•æ„Ÿä¿¡æ¯åŒ¹é…æ­£åˆ™
        self.sensitive_patterns = [
            # APIå¯†é’¥å’Œä»¤ç‰Œ
            r'(api[_-]?key|apikey)["\']?\s*[:=]\s*["\'][a-zA-Z0-9_-]{10,}["\']',
            r'(access[_-]?token|accesstoken)["\']?\s*[:=]\s*["\'][a-zA-Z0-9_.-]{10,}["\']',
            r'(secret[_-]?key|secretkey)["\']?\s*[:=]\s*["\'][a-zA-Z0-9_-]{10,}["\']',
            
            # æ•°æ®åº“è¿æ¥
            r'(database[_-]?url|db[_-]?url)["\']?\s*[:=]\s*["\'][^"\']+["\']',
            r'(connection[_-]?string)["\']?\s*[:=]\s*["\'][^"\']+["\']',
            
            # æœåŠ¡å™¨ä¿¡æ¯
            r'(server[_-]?url|base[_-]?url)["\']?\s*[:=]\s*["\']https?://[^"\']+["\']',
            r'(endpoint|host)["\']?\s*[:=]\s*["\']https?://[^"\']+["\']',
            
            # è®¤è¯ä¿¡æ¯
            r'(username|user[_-]?name)["\']?\s*[:=]\s*["\'][^"\']+["\']',
            r'(password|passwd)["\']?\s*[:=]\s*["\'][^"\']+["\']',
            
            # é…ç½®ä¿¡æ¯
            r'(config|configuration)["\']?\s*[:=]\s*\{[^}]+\}',
            r'(env|environment)["\']?\s*[:=]\s*["\'][^"\']+["\']'
        ]
        
    async def run_js_mining(self):
        """è¿è¡ŒJavaScriptæŒ–æ˜"""
        print("ğŸ” JavaScriptæ·±åº¦æŒ–æ˜å¼•æ“å¯åŠ¨")
        print("=" * 60)
        print(f"ğŸ¯ ç›®æ ‡: {self.target}")
        print("ğŸš€ ç­–ç•¥: å‘ç°ç°ä»£Webåº”ç”¨çš„éšè—APIå’Œè·¯å¾„")
        print("=" * 60)
        
        start_time = time.time()
        
        # Phase 1: ä½¿ç”¨æµè§ˆå™¨å¼•æ“æ”¶é›†æ‰€æœ‰JSæ–‡ä»¶
        await self._phase1_collect_js_files()
        
        # Phase 2: ä¸‹è½½å¹¶åˆ†æJSæ–‡ä»¶å†…å®¹
        await self._phase2_analyze_js_content()
        
        # Phase 3: éªŒè¯å‘ç°çš„ç«¯ç‚¹
        await self._phase3_validate_endpoints()
        
        duration = time.time() - start_time
        self._generate_mining_report(duration)
        
    async def _phase1_collect_js_files(self):
        """Phase 1: æ”¶é›†æ‰€æœ‰JSæ–‡ä»¶"""
        print("\nğŸ“‚ Phase 1: æ”¶é›†JavaScriptæ–‡ä»¶")
        print("-" * 50)
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()
            
            # ç›‘å¬ç½‘ç»œè¯·æ±‚ï¼Œæ”¶é›†JSæ–‡ä»¶
            js_files = set()
            
            async def handle_response(response):
                url = response.url
                content_type = response.headers.get('content-type', '')
                
                # è¯†åˆ«JavaScriptæ–‡ä»¶
                if (url.endswith('.js') or 
                    'javascript' in content_type or 
                    'application/javascript' in content_type or
                    'text/javascript' in content_type):
                    js_files.add(url)
                    print(f"   ğŸ“„ å‘ç°JSæ–‡ä»¶: {url}")
                    
            page.on('response', handle_response)
            
            try:
                # è®¿é—®ç›®æ ‡é¡µé¢
                await page.goto(self.target, wait_until='networkidle', timeout=30000)
                await page.wait_for_timeout(5000)  # ç­‰å¾…åŠ¨æ€åŠ è½½
                
                # å°è¯•å¯¼èˆªåˆ°å¯èƒ½çš„å­é¡µé¢
                potential_pages = [
                    f"{self.target}/about",
                    f"{self.target}/contact", 
                    f"{self.target}/services",
                    f"{self.target}/login",
                    f"{self.target}/admin"
                ]
                
                for page_url in potential_pages:
                    try:
                        await page.goto(page_url, wait_until='domcontentloaded', timeout=10000)
                        await page.wait_for_timeout(2000)
                    except:
                        continue
                        
            except Exception as e:
                print(f"   âš ï¸ æµè§ˆå™¨è®¿é—®é”™è¯¯: {e}")
                
            await browser.close()
            
        self.discovered_js_files.update(js_files)
        print(f"   âœ… æ€»å…±å‘ç° {len(self.discovered_js_files)} ä¸ªJSæ–‡ä»¶")
        
    async def _phase2_analyze_js_content(self):
        """Phase 2: åˆ†æJSæ–‡ä»¶å†…å®¹"""
        print("\nğŸ” Phase 2: æ·±åº¦åˆ†æJavaScriptå†…å®¹")
        print("-" * 50)
        
        connector = aiohttp.TCPConnector(ssl=ssl.create_default_context())
        timeout = aiohttp.ClientTimeout(total=30)
        
        self.session = aiohttp.ClientSession(connector=connector, timeout=timeout)
        
        try:
            for js_url in self.discovered_js_files:
                await self._analyze_single_js_file(js_url)
        finally:
            await self.session.close()
            
    async def _analyze_single_js_file(self, js_url: str):
        """åˆ†æå•ä¸ªJSæ–‡ä»¶"""
        try:
            print(f"ğŸ”¬ åˆ†æ: {js_url}")
            
            async with self.session.get(js_url) as response:
                if response.status == 200:
                    content = await response.text()
                    
                    # å°è¯•ç¾åŒ–å‹ç¼©çš„JSä»£ç 
                    if len(content) > 1000 and '\n' not in content[:1000]:
                        try:
                            content = jsbeautifier.beautify(content)
                            print(f"   âœ¨ ä»£ç ç¾åŒ–æˆåŠŸ")
                        except:
                            print(f"   âš ï¸ ä»£ç ç¾åŒ–å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å†…å®¹")
                    
                    # æå–APIç«¯ç‚¹å’Œè·¯å¾„
                    endpoints = self._extract_endpoints(content, js_url)
                    paths = self._extract_paths(content, js_url)
                    sensitive_info = self._extract_sensitive_info(content, js_url)
                    
                    result = {
                        "js_file": js_url,
                        "size": len(content),
                        "endpoints_found": len(endpoints),
                        "paths_found": len(paths), 
                        "sensitive_info_found": len(sensitive_info),
                        "endpoints": list(endpoints)[:20],  # åªè®°å½•å‰20ä¸ª
                        "paths": list(paths)[:20],
                        "sensitive_info": sensitive_info,
                        "timestamp": datetime.now().isoformat()
                    }
                    
                    self.results.append(result)
                    
                    if endpoints or paths or sensitive_info:
                        print(f"   ğŸ¯ ç«¯ç‚¹: {len(endpoints)}, è·¯å¾„: {len(paths)}, æ•æ„Ÿä¿¡æ¯: {len(sensitive_info)}")
                    
                    self.extracted_endpoints.update(endpoints)
                    self.extracted_paths.update(paths)
                    
        except Exception as e:
            print(f"   âŒ åˆ†æå¤±è´¥: {e}")
            
    def _extract_endpoints(self, content: str, js_url: str) -> Set[str]:
        """æå–APIç«¯ç‚¹"""
        endpoints = set()
        
        for pattern in self.api_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            for match in matches:
                # æ¸…ç†åŒ¹é…ç»“æœ
                endpoint = match.strip('\'"/')
                if len(endpoint) > 2 and not endpoint.startswith('http'):
                    # è½¬æ¢ä¸ºå®Œæ•´URL
                    if endpoint.startswith('/'):
                        full_url = f"{self.target}{endpoint}"
                    else:
                        full_url = f"{self.target}/{endpoint}"
                    endpoints.add(full_url)
                    
        return endpoints
        
    def _extract_paths(self, content: str, js_url: str) -> Set[str]:
        """æå–è·¯å¾„ä¿¡æ¯"""
        paths = set()
        
        # æ›´å®½æ³›çš„è·¯å¾„åŒ¹é…
        path_patterns = [
            r'["\']\/[a-zA-Z][a-zA-Z0-9\/._-]{3,}["\']',
            r'["\'][a-zA-Z][a-zA-Z0-9\/._-]{3,}\.html?["\']',
            r'["\'][a-zA-Z][a-zA-Z0-9\/._-]{3,}\.php["\']',
            r'["\'][a-zA-Z][a-zA-Z0-9\/._-]{3,}\.jsp["\']',
            r'["\'][a-zA-Z][a-zA-Z0-9\/._-]{3,}\.asp["\']'
        ]
        
        for pattern in path_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                path = match.strip('\'"')
                if len(path) > 3:
                    paths.add(path)
                    
        return paths
        
    def _extract_sensitive_info(self, content: str, js_url: str) -> List[Dict[str, str]]:
        """æå–æ•æ„Ÿä¿¡æ¯"""
        sensitive_info = []
        
        for pattern in self.sensitive_patterns:
            matches = re.finditer(pattern, content, re.IGNORECASE)
            for match in matches:
                info = {
                    "type": "sensitive_data",
                    "pattern": pattern,
                    "match": match.group(0)[:100],  # åªè®°å½•å‰100ä¸ªå­—ç¬¦
                    "position": match.start()
                }
                sensitive_info.append(info)
                
        return sensitive_info
        
    async def _phase3_validate_endpoints(self):
        """Phase 3: éªŒè¯å‘ç°çš„ç«¯ç‚¹"""
        print("\nâœ… Phase 3: éªŒè¯å‘ç°çš„ç«¯ç‚¹")
        print("-" * 50)
        
        if not self.extracted_endpoints:
            print("   ğŸ“‹ æ²¡æœ‰å‘ç°ç«¯ç‚¹éœ€è¦éªŒè¯")
            return
            
        connector = aiohttp.TCPConnector(ssl=ssl.create_default_context())
        timeout = aiohttp.ClientTimeout(total=15)
        
        async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
            tasks = []
            for endpoint in list(self.extracted_endpoints)[:50]:  # åªéªŒè¯å‰50ä¸ª
                tasks.append(self._validate_endpoint(session, endpoint))
                
            await asyncio.gather(*tasks, return_exceptions=True)
            
    async def _validate_endpoint(self, session: aiohttp.ClientSession, endpoint: str):
        """éªŒè¯å•ä¸ªç«¯ç‚¹"""
        try:
            async with session.get(endpoint, allow_redirects=False) as response:
                if response.status in [200, 201, 301, 302, 401, 403]:
                    result = {
                        "endpoint": endpoint,
                        "status": response.status,
                        "accessible": response.status in [200, 201],
                        "size": response.headers.get('content-length', 0),
                        "content_type": response.headers.get('content-type', ''),
                        "validated": True,
                        "timestamp": datetime.now().isoformat()
                    }
                    
                    self.results.append(result)
                    
                    if response.status in [200, 201]:
                        print(f"   âœ… å¯è®¿é—®: {endpoint} ({response.status})")
                    elif response.status in [401, 403]:
                        print(f"   ğŸ” å—ä¿æŠ¤: {endpoint} ({response.status})")
                    else:
                        print(f"   ğŸ”„ é‡å®šå‘: {endpoint} ({response.status})")
                        
        except Exception as e:
            pass
            
    def _generate_mining_report(self, duration: float):
        """ç”ŸæˆæŒ–æ˜æŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"js_mining_results_{timestamp}.json"
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_js_files = len(self.discovered_js_files)
        total_endpoints = len(self.extracted_endpoints)
        total_paths = len(self.extracted_paths)
        
        # éªŒè¯ç»“æœç»Ÿè®¡
        validated_results = [r for r in self.results if r.get("validated")]
        accessible_count = len([r for r in validated_results if r.get("accessible")])
        protected_count = len([r for r in validated_results if r.get("status") in [401, 403]])
        
        report = {
            "scan_info": {
                "target": self.target,
                "timestamp": datetime.now().isoformat(),
                "duration": duration,
                "js_files_analyzed": total_js_files,
                "endpoints_extracted": total_endpoints,
                "paths_extracted": total_paths,
                "endpoints_validated": len(validated_results),
                "accessible_endpoints": accessible_count,
                "protected_endpoints": protected_count
            },
            "discovered_js_files": list(self.discovered_js_files),
            "extracted_endpoints": list(self.extracted_endpoints),
            "extracted_paths": list(self.extracted_paths),
            "analysis_results": self.results
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
            
        print(f"\nğŸ¯ JavaScriptæŒ–æ˜å®Œæˆ!")
        print(f"ğŸ“‚ JSæ–‡ä»¶åˆ†æ: {total_js_files}")
        print(f"ğŸ”Œ ç«¯ç‚¹å‘ç°: {total_endpoints}")
        print(f"ğŸ“„ è·¯å¾„å‘ç°: {total_paths}")
        print(f"âœ… å¯è®¿é—®ç«¯ç‚¹: {accessible_count}")
        print(f"ğŸ” å—ä¿æŠ¤ç«¯ç‚¹: {protected_count}")
        print(f"â±ï¸ æŒ–æ˜è€—æ—¶: {duration:.2f}ç§’")
        print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Š: {filename}")
        
        if accessible_count > 0:
            print(f"\nğŸ”¥ å‘ç°å¯è®¿é—®çš„æ–°ç«¯ç‚¹!")
            for result in validated_results:
                if result.get("accessible"):
                    print(f"   âœ… {result['endpoint']}")

async def main():
    import sys
    
    if len(sys.argv) != 2:
        print("ä½¿ç”¨æ–¹æ³•: python js_mining_engine.py <target>")
        print("ç¤ºä¾‹: python js_mining_engine.py https://asanoha-clinic.com")
        return
        
    target = sys.argv[1]
    if not target.startswith(('http://', 'https://')):
        target = 'https://' + target
        
    engine = JavaScriptMiningEngine(target)
    await engine.run_js_mining()

if __name__ == "__main__":
    asyncio.run(main()) 