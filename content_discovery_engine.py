#!/usr/bin/env python3
"""
å…¨ç–†åŸŸå†…å®¹å‘ç°å¼•æ“ (Content Discovery Engine)
é›†æˆåWAFèƒ½åŠ›çš„å¹¶è¡Œå†…å®¹å‘ç°ç³»ç»Ÿ
ä¸ºç¬¬äºŒé˜¶æ®µè¡ŒåŠ¨å»ºç«‹å®Œæ•´çš„æ”»å‡»è·¯å¾„åœ°å›¾é›†
ç¤ºä¾‹åŸŸåè¯·æ›¿æ¢å®é™…éƒ¨ç½²åŸŸå
"""

import asyncio
import aiohttp
import ssl
import random
import time
import json
import os
from datetime import datetime
from typing import Dict, List, Optional, Set, Tuple
from urllib.parse import urljoin, urlparse
import concurrent.futures
from pathlib import Path

# å¯¼å…¥åWAFå¼•æ“
from anti_waf_engine import AntiWAFEngine

class ContentDiscoveryEngine:
    def __init__(self, subdomains_file: str = None, target_domain: str = None):
        """
        åˆå§‹åŒ–å…¨ç–†åŸŸå†…å®¹å‘ç°å¼•æ“
        
        å‚æ•°:
        subdomains_file: å­åŸŸååˆ—è¡¨æ–‡ä»¶è·¯å¾„
        target_domain: ç›®æ ‡åŸŸåï¼ˆç”¨äºç”Ÿæˆè¾“å‡ºæ–‡ä»¶åï¼‰
        """
        self.target_domain = target_domain
        self.subdomains_file = subdomains_file
        self.subdomains = []
        self.discovered_paths = {}
        self.attack_surface_map = {}
        
        # é›†æˆåWAFå¼•æ“
        self.anti_waf = AntiWAFEngine()
        
        # è¾“å‡ºç›®å½•
        self.output_dir = "content_discovery_results"
        os.makedirs(self.output_dir, exist_ok=True)
        
        # æ‰«æé…ç½®
        self.config = {
                'concurrent_domains': 5,    # å¹¶å‘åŸŸåæ•°é‡
    'concurrent_paths': 12,     # æ¯ä¸ªåŸŸåçš„å¹¶å‘è·¯å¾„æ•°é‡ (çŒ¥çéƒ¨ç½²ï¼šé™ä½å¹¶å‘)
            'request_timeout': 10,      # è¯·æ±‚è¶…æ—¶æ—¶é—´
            'delay_range': (0.5, 2.0),  # éšæœºå»¶è¿ŸèŒƒå›´
            'max_retries': 2,           # æœ€å¤§é‡è¯•æ¬¡æ•°
            'interesting_status_codes': [200, 201, 204, 301, 302, 403, 401, 500, 503],
            'skip_extensions': ['.jpg', '.jpeg', '.png', '.gif', '.css', '.js', '.ico', '.woff', '.woff2']
        }
        
        # æ ¸å¿ƒè·¯å¾„å­—å…¸ - æ ¹æ®é˜¶æ®µ1å‘ç°ä¼˜åŒ–
        self.discovery_paths = [
            # ç®¡ç†ç•Œé¢
            '/admin', '/admin/', '/administrator', '/admin.php', '/admin/login',
            '/admin/index.php', '/admin/admin.php', '/admin/dashboard',
            '/wp-admin/', '/wp-admin/admin.php', '/phpmyadmin/', '/adminer.php',
            '/manager/html', '/manager/', '/console/', '/control/', '/cp/',
            
            # APIç«¯ç‚¹
            '/api/', '/api/v1/', '/api/v2/', '/api/v3/', '/api/docs',
            '/api/swagger', '/api/openapi', '/api/graphql', '/graphql',
            '/rest/', '/rest/api/', '/v1/', '/v2/', '/v3/',
            '/api/health', '/api/status', '/api/version', '/api/config',
            
            # åŒ»ç–—ç³»ç»Ÿç‰¹å®šè·¯å¾„ï¼ˆåŸºäºbiograph.comå‘ç°ï¼‰
            '/patient/', '/patients/', '/medical/', '/health/', '/records/',
            '/hipaa/', '/phi/', '/emr/', '/ehr/', '/dicom/', '/hl7/',
            '/billing/', '/insurance/', '/claims/', '/appointments/',
            '/provider/', '/physician/', '/doctor/', '/nurse/',
            
            # è®¤è¯ç›¸å…³
            '/login', '/login.php', '/login.asp', '/signin', '/auth/',
            '/authentication/', '/oauth/', '/sso/', '/saml/', '/ldap/',
            '/forgot-password', '/reset-password', '/change-password',
            
            # é…ç½®å’Œæ•æ„Ÿæ–‡ä»¶
            '/.env', '/.env.local', '/.env.production', '/config/',
            '/configuration/', '/settings/', '/setup/', '/install/',
            '/web.config', '/app.config', '/database.yml', '/secrets.yml',
            '/.git/', '/.svn/', '/.hg/', '/CVS/', '/.DS_Store',
            
            # å¤‡ä»½å’Œä¸´æ—¶æ–‡ä»¶
            '/backup/', '/backups/', '/bak/', '/old/', '/tmp/', '/temp/',
            '/cache/', '/log/', '/logs/', '/dump/', '/sql/', '/db/',
            '/backup.sql', '/backup.tar.gz', '/dump.sql', '/database.sql',
            
            # å¼€å‘å’Œæµ‹è¯•ç¯å¢ƒ
            '/dev/', '/development/', '/test/', '/testing/', '/stage/', '/staging/',
            '/qa/', '/uat/', '/demo/', '/sandbox/', '/debug/', '/trace/',
            '/phpinfo.php', '/info.php', '/test.php', '/debug.php',
            
            # æ–‡æ¡£å’Œå¸®åŠ©
            '/docs/', '/documentation/', '/help/', '/support/', '/manual/',
            '/readme', '/README', '/CHANGELOG', '/LICENSE', '/INSTALL',
            '/wiki/', '/knowledge/', '/faq/', '/about/',
            
            # æœåŠ¡å’Œå¥åº·æ£€æŸ¥
            '/health', '/healthcheck', '/status', '/ping', '/version',
            '/metrics', '/monitoring/', '/stats/', '/analytics/',
            '/prometheus/', '/grafana/', '/kibana/',
            
            # æ–‡ä»¶ä¸Šä¼ å’Œä¸‹è½½
            '/upload/', '/uploads/', '/files/', '/download/', '/downloads/',
            '/media/', '/images/', '/documents/', '/attachments/',
            '/export/', '/import/', '/transfer/',
            
            # ç¬¬ä¸‰æ–¹é›†æˆ
            '/webhook/', '/webhooks/', '/callback/', '/notify/',
            '/integration/', '/connector/', '/plugin/', '/addon/',
            '/oauth2/', '/openid/', '/cas/', '/radius/',
            
            # ç§»åŠ¨åº”ç”¨API
            '/mobile/', '/app/', '/android/', '/ios/', '/flutter/',
            '/cordova/', '/phonegap/', '/ionic/', '/react-native/',
            
            # äº‘æœåŠ¡ç›¸å…³
            '/aws/', '/azure/', '/gcp/', '/s3/', '/ec2/', '/lambda/',
            '/docker/', '/kubernetes/', '/k8s/', '/helm/', '/terraform/',
            
            # å®‰å…¨ç›¸å…³
            '/security/', '/audit/', '/compliance/', '/policy/',
            '/firewall/', '/waf/', '/ids/', '/ips/', '/siem/',
            '/vulnerability/', '/pentest/', '/security-headers/',
            
            # å¸¸è§CMSè·¯å¾„
            '/wp-content/', '/wp-includes/', '/wp-json/', '/xmlrpc.php',
            '/drupal/', '/joomla/', '/magento/', '/prestashop/',
            '/typo3/', '/concrete5/', '/modx/', '/craft/',
            
            # æ¡†æ¶ç‰¹å®šè·¯å¾„
            '/laravel/', '/symfony/', '/codeigniter/', '/cakephp/',
            '/rails/', '/django/', '/flask/', '/spring/', '/struts/',
            '/express/', '/next/', '/nuxt/', '/angular/', '/react/', '/vue/',
        ]
        
        # æ–‡ä»¶æ‰©å±•åå­—å…¸
        self.file_extensions = [
            '', '.php', '.asp', '.aspx', '.jsp', '.do', '.action',
            '.cfm', '.cgi', '.pl', '.py', '.rb', '.sh', '.bat',
            '.html', '.htm', '.xml', '.json', '.txt', '.log',
            '.sql', '.bak', '.old', '.orig', '.tmp', '.swp',
            '.zip', '.tar', '.gz', '.rar', '.7z', '.war', '.jar'
        ]

    def load_subdomains(self) -> List[str]:
        """ä»æ–‡ä»¶æˆ–ç›´æ¥è¾“å…¥åŠ è½½å­åŸŸååˆ—è¡¨"""
        subdomains = []
        
        if self.subdomains_file and os.path.exists(self.subdomains_file):
            print(f"ğŸ“‚ ä»æ–‡ä»¶åŠ è½½å­åŸŸå: {self.subdomains_file}")
            with open(self.subdomains_file, 'r', encoding='utf-8') as f:
                for line in f:
                    subdomain = line.strip()
                    if subdomain and not subdomain.startswith('#'):
                        subdomains.append(subdomain)
        else:
            # å¦‚æœæ²¡æœ‰æ–‡ä»¶ï¼Œå°è¯•ä»day1_reconç»“æœç›®å½•æŸ¥æ‰¾
            possible_files = [
                'subdomains_enhanced.txt',
                'output/subdomains_enhanced.txt',
                f'{self.target_domain}_subdomains.txt'
            ]
            
            for file_path in possible_files:
                if os.path.exists(file_path):
                    print(f"ğŸ“‚ è‡ªåŠ¨å‘ç°å­åŸŸåæ–‡ä»¶: {file_path}")
                    with open(file_path, 'r', encoding='utf-8') as f:
                        for line in f:
                            subdomain = line.strip()
                            if subdomain and not subdomain.startswith('#'):
                                subdomains.append(subdomain)
                    break
        
        if not subdomains and self.target_domain:
            print(f"âš ï¸  æœªæ‰¾åˆ°å­åŸŸåæ–‡ä»¶ï¼Œä½¿ç”¨ä¸»åŸŸå: {self.target_domain}")
            subdomains = [self.target_domain]
        
        self.subdomains = list(set(subdomains))  # å»é‡
        print(f"ğŸ¯ åŠ è½½å­åŸŸå: {len(self.subdomains)} ä¸ª")
        return self.subdomains

    async def check_path(self, session: aiohttp.ClientSession, domain: str, path: str) -> Optional[Dict]:
        """æ£€æŸ¥å•ä¸ªè·¯å¾„çš„å¯è®¿é—®æ€§"""
        for protocol in ['https', 'http']:
            url = f"{protocol}://{domain}{path}"
            
            try:
                # ä½¿ç”¨åWAFå¼•æ“çš„StealthHTTPClientè¿›è¡Œåˆ†å¸ƒå¼æ‰«æ
                response = await self.anti_waf.stealth_request(
                    session, 
                    "GET", 
                    url,
                    timeout=aiohttp.ClientTimeout(total=self.config['request_timeout']),
                    ssl=False,
                    allow_redirects=False
                )
                
                async with response:
                    
                    if response.status in self.config['interesting_status_codes']:
                        content_length = response.headers.get('Content-Length', '0')
                        content_type = response.headers.get('Content-Type', '')
                        server = response.headers.get('Server', '')
                        location = response.headers.get('Location', '')
                        
                        result = {
                            'url': url,
                            'status': response.status,
                            'content_length': content_length,
                            'content_type': content_type,
                            'server': server,
                            'location': location,
                            'protocol': protocol,
                            'path': path,
                            'domain': domain,
                            'timestamp': datetime.now().isoformat()
                        }
                        
                        # æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰è¶£çš„å“åº”
                        if self.is_interesting_response(result):
                            print(f"âœ… å‘ç°: {url} [{response.status}] {content_length}å­—èŠ‚")
                            return result
            
            except asyncio.TimeoutError:
                continue
            except Exception as e:
                continue
        
        return None

    def is_interesting_response(self, result: Dict) -> bool:
        """åˆ¤æ–­å“åº”æ˜¯å¦å€¼å¾—å…³æ³¨"""
        status = result['status']
        content_length = int(result.get('content_length', '0'))
        content_type = result.get('content_type', '').lower()
        url = result['url']
        
        # è·³è¿‡é™æ€èµ„æº
        if any(url.lower().endswith(ext) for ext in self.config['skip_extensions']):
            return False
        
        # è·³è¿‡å¤ªå°çš„å“åº”ï¼ˆå¯èƒ½æ˜¯é”™è¯¯é¡µé¢ï¼‰
        if status == 200 and content_length < 100:
            return False
        
        # é‡ç‚¹å…³æ³¨çš„çŠ¶æ€ç 
        interesting_statuses = [200, 201, 204, 401, 403, 500, 503]
        if status in interesting_statuses:
            return True
        
        # é‡å®šå‘ä½†åŒ…å«æ•æ„Ÿå…³é”®è¯
        if status in [301, 302] and any(keyword in url.lower() for keyword in 
                                      ['admin', 'login', 'api', 'config', 'dashboard']):
            return True
        
        return False

    async def scan_domain(self, domain: str) -> List[Dict]:
        """æ‰«æå•ä¸ªåŸŸåçš„æ‰€æœ‰è·¯å¾„"""
        print(f"\nğŸ” å¼€å§‹æ‰«æåŸŸå: {domain}")
        discovered = []
        
        # åˆ›å»ºSSLä¸Šä¸‹æ–‡ï¼ˆå¿½ç•¥è¯ä¹¦éªŒè¯ï¼‰
        ssl_context = ssl.create_default_context()
        ssl_context.check_hostname = False
        ssl_context.verify_mode = ssl.CERT_NONE
        
        # åˆ›å»ºè¿æ¥å™¨
        connector = aiohttp.TCPConnector(
            ssl=ssl_context,
            limit=50,
            limit_per_host=20
        )
        
        async with aiohttp.ClientSession(connector=connector) as session:
            # åˆ›å»ºä»»åŠ¡åˆ—è¡¨
            tasks = []
            
            # åŸºç¡€è·¯å¾„æ‰«æ
            for path in self.discovery_paths:
                task = asyncio.create_task(self.check_path(session, domain, path))
                tasks.append(task)
            
            # å¸¦æ‰©å±•åçš„è·¯å¾„æ‰«æ
            for base_path in ['/admin', '/config', '/backup', '/test', '/api']:
                for ext in self.file_extensions:
                    path = f"{base_path}{ext}"
                    task = asyncio.create_task(self.check_path(session, domain, path))
                    tasks.append(task)
            
            # å¹¶å‘æ‰§è¡Œï¼Œæ§åˆ¶å¹¶å‘æ•°é‡
            semaphore = asyncio.Semaphore(self.config['concurrent_paths'])
            
            async def bounded_check(task):
                async with semaphore:
                    return await task
            
            bounded_tasks = [bounded_check(task) for task in tasks]
            results = await asyncio.gather(*bounded_tasks, return_exceptions=True)
            
            # æ”¶é›†æœ‰æ•ˆç»“æœ
            for result in results:
                if result and isinstance(result, dict):
                    discovered.append(result)
        
        print(f"ğŸ“Š {domain} å‘ç°è·¯å¾„: {len(discovered)} ä¸ª")
        return discovered

    async def run_parallel_discovery(self) -> Dict:
        """å¹¶è¡Œæ‰§è¡Œå…¨ç–†åŸŸå†…å®¹å‘ç°"""
        print("\nğŸš€ å¯åŠ¨å…¨ç–†åŸŸå†…å®¹å‘ç°å¼•æ“")
        print("=" * 60)
        
        # åŠ è½½å­åŸŸå
        self.load_subdomains()
        
        if not self.subdomains:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•å­åŸŸåï¼Œæ— æ³•ç»§ç»­")
            return {}
        
        start_time = time.time()
        all_results = {}
        
        # æ§åˆ¶å¹¶å‘åŸŸåæ•°é‡
        semaphore = asyncio.Semaphore(self.config['concurrent_domains'])
        
        async def scan_with_semaphore(domain):
            async with semaphore:
                return domain, await self.scan_domain(domain)
        
        # åˆ›å»ºæ‰«æä»»åŠ¡
        tasks = [scan_with_semaphore(domain) for domain in self.subdomains]
        
        # æ‰§è¡Œå¹¶è¡Œæ‰«æ
        print(f"ğŸ¯ å¹¶è¡Œæ‰«æ {len(self.subdomains)} ä¸ªå­åŸŸå (å¹¶å‘: {self.config['concurrent_domains']})")
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # æ”¶é›†ç»“æœ
        total_paths = 0
        for result in results:
            if isinstance(result, tuple) and len(result) == 2:
                domain, paths = result
                if paths:
                    all_results[domain] = paths
                    total_paths += len(paths)
        
        elapsed_time = time.time() - start_time
        
        print("\n" + "=" * 60)
        print(f"âœ… å…¨ç–†åŸŸå†…å®¹å‘ç°å®Œæˆ!")
        print(f"ğŸ“Š æ‰«æåŸŸå: {len(self.subdomains)} ä¸ª")
        print(f"ğŸ¯ å‘ç°è·¯å¾„: {total_paths} ä¸ª")
        print(f"â±ï¸  è€—æ—¶: {elapsed_time:.2f} ç§’")
        
        # ä¿å­˜ç»“æœ
        self.save_results(all_results)
        self.generate_attack_surface_map(all_results)
        
        return all_results

    def save_results(self, results: Dict):
        """ä¿å­˜å‘ç°ç»“æœ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # è¯¦ç»†ç»“æœJSON
        detailed_file = f"{self.output_dir}/content_discovery_{timestamp}.json"
        with open(detailed_file, 'w', encoding='utf-8') as f:
            json.dump({
                'scan_info': {
                    'target_domain': self.target_domain,
                    'scan_time': datetime.now().isoformat(),
                    'total_domains': len(results),
                    'total_paths': sum(len(paths) for paths in results.values()),
                    'engine_version': 'ContentDiscoveryEngine v1.0'
                },
                'discovered_paths': results
            }, f, indent=2, ensure_ascii=False)
        
        # ç®€åŒ–è·¯å¾„åˆ—è¡¨
        simple_file = f"{self.output_dir}/discovered_paths_{timestamp}.txt"
        with open(simple_file, 'w', encoding='utf-8') as f:
            f.write(f"# å…¨ç–†åŸŸå†…å®¹å‘ç°ç»“æœ - {datetime.now()}\n")
            f.write(f"# ç›®æ ‡: {self.target_domain}\n")
            f.write(f"# å‘ç°åŸŸå: {len(results)} ä¸ª\n")
            f.write(f"# å‘ç°è·¯å¾„: {sum(len(paths) for paths in results.values())} ä¸ª\n\n")
            
            for domain, paths in results.items():
                f.write(f"\n[{domain}] - {len(paths)} ä¸ªè·¯å¾„\n")
                f.write("-" * 50 + "\n")
                for path_info in paths:
                    status = path_info['status']
                    url = path_info['url']
                    length = path_info['content_length']
                    f.write(f"{status:3d} | {length:>8} | {url}\n")
        
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜:")
        print(f"    è¯¦ç»†ç»“æœ: {detailed_file}")
        print(f"    è·¯å¾„åˆ—è¡¨: {simple_file}")

    def generate_attack_surface_map(self, results: Dict):
        """ç”Ÿæˆæ”»å‡»é¢åœ°å›¾"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        map_file = f"{self.output_dir}/attack_surface_map_{timestamp}.json"
        
        attack_map = {
            'metadata': {
                'generated_at': datetime.now().isoformat(),
                'target_domain': self.target_domain,
                'total_domains': len(results),
                'total_attack_vectors': 0
            },
            'attack_vectors': {
                'admin_interfaces': [],
                'api_endpoints': [],
                'authentication_pages': [],
                'configuration_files': [],
                'backup_files': [],
                'development_environments': [],
                'sensitive_directories': [],
                'medical_specific': [],
                'high_value_targets': []
            }
        }
        
        # åˆ†ç±»æ”»å‡»å‘é‡
        for domain, paths in results.items():
            for path_info in paths:
                url = path_info['url']
                path = path_info['path']
                status = path_info['status']
                
                # ç®¡ç†ç•Œé¢
                if any(keyword in path.lower() for keyword in ['admin', 'manage', 'control', 'dashboard']):
                    attack_map['attack_vectors']['admin_interfaces'].append({
                        'url': url, 'status': status, 'priority': 'HIGH'
                    })
                
                # APIç«¯ç‚¹
                elif any(keyword in path.lower() for keyword in ['api', 'rest', 'graphql', 'webhook']):
                    attack_map['attack_vectors']['api_endpoints'].append({
                        'url': url, 'status': status, 'priority': 'HIGH'
                    })
                
                # è®¤è¯é¡µé¢
                elif any(keyword in path.lower() for keyword in ['login', 'auth', 'signin', 'sso']):
                    attack_map['attack_vectors']['authentication_pages'].append({
                        'url': url, 'status': status, 'priority': 'MEDIUM'
                    })
                
                # é…ç½®æ–‡ä»¶
                elif any(keyword in path.lower() for keyword in ['config', 'settings', '.env', 'web.config']):
                    attack_map['attack_vectors']['configuration_files'].append({
                        'url': url, 'status': status, 'priority': 'HIGH'
                    })
                
                # å¤‡ä»½æ–‡ä»¶
                elif any(keyword in path.lower() for keyword in ['backup', 'bak', 'dump', 'sql']):
                    attack_map['attack_vectors']['backup_files'].append({
                        'url': url, 'status': status, 'priority': 'HIGH'
                    })
                
                # å¼€å‘ç¯å¢ƒ
                elif any(keyword in path.lower() for keyword in ['dev', 'test', 'stage', 'debug']):
                    attack_map['attack_vectors']['development_environments'].append({
                        'url': url, 'status': status, 'priority': 'MEDIUM'
                    })
                
                # åŒ»ç–—ç³»ç»Ÿç‰¹å®š
                elif any(keyword in path.lower() for keyword in ['patient', 'medical', 'health', 'hipaa', 'phi']):
                    attack_map['attack_vectors']['medical_specific'].append({
                        'url': url, 'status': status, 'priority': 'CRITICAL'
                    })
                
                # æ•æ„Ÿç›®å½•
                else:
                    attack_map['attack_vectors']['sensitive_directories'].append({
                        'url': url, 'status': status, 'priority': 'LOW'
                    })
                
                # é«˜ä»·å€¼ç›®æ ‡ï¼ˆåŸºäºçŠ¶æ€ç ï¼‰
                if status in [200, 401, 403, 500]:
                    attack_map['attack_vectors']['high_value_targets'].append({
                        'url': url, 'status': status, 'reason': f'Status {status}',
                        'priority': 'HIGH' if status in [200, 500] else 'MEDIUM'
                    })
        
        # è®¡ç®—æ€»æ”»å‡»å‘é‡æ•°
        total_vectors = sum(len(vectors) for vectors in attack_map['attack_vectors'].values())
        attack_map['metadata']['total_attack_vectors'] = total_vectors
        
        # ä¿å­˜æ”»å‡»é¢åœ°å›¾
        with open(map_file, 'w', encoding='utf-8') as f:
            json.dump(attack_map, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ—ºï¸  æ”»å‡»é¢åœ°å›¾å·²ç”Ÿæˆ: {map_file}")
        print(f"    æ€»æ”»å‡»å‘é‡: {total_vectors} ä¸ª")
        
        # è¾“å‡ºå…³é”®å‘ç°ç»Ÿè®¡
        print("\nğŸ“ˆ å…³é”®æ”»å‡»å‘é‡ç»Ÿè®¡:")
        for category, vectors in attack_map['attack_vectors'].items():
            if vectors:
                count = len(vectors)
                high_priority = len([v for v in vectors if v.get('priority') == 'HIGH'])
                print(f"    {category}: {count} ä¸ª (é«˜ä¼˜å…ˆçº§: {high_priority})")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='å…¨ç–†åŸŸå†…å®¹å‘ç°å¼•æ“')
    parser.add_argument('--subdomains-file', '-f', help='å­åŸŸååˆ—è¡¨æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--target-domain', '-t', help='ç›®æ ‡åŸŸå')
    parser.add_argument('--concurrent-domains', '-cd', type=int, default=5, help='å¹¶å‘åŸŸåæ•°é‡')
    parser.add_argument('--concurrent-paths', '-cp', type=int, default=12, help='æ¯ä¸ªåŸŸåçš„å¹¶å‘è·¯å¾„æ•°é‡')
    
    args = parser.parse_args()
    
    # åˆ›å»ºå¼•æ“å®ä¾‹
    engine = ContentDiscoveryEngine(
        subdomains_file=args.subdomains_file,
        target_domain=args.target_domain
    )
    
    # æ›´æ–°é…ç½®
    if args.concurrent_domains:
        engine.config['concurrent_domains'] = args.concurrent_domains
    if args.concurrent_paths:
        engine.config['concurrent_paths'] = args.concurrent_paths
    
    # è¿è¡Œå‘ç°
    try:
        results = asyncio.run(engine.run_parallel_discovery())
        
        if results:
            print("\nğŸ¯ å…¨ç–†åŸŸå†…å®¹å‘ç°å®Œæˆï¼æ”»å‡»è·¯å¾„åœ°å›¾å·²å°±ç»ªï¼")
            print("ğŸ’ ä¸ºç¬¬äºŒé˜¶æ®µæ€»æ”»åšå¥½äº†å®Œæ•´çš„æƒ…æŠ¥æ”¯æ’‘ï¼")
        else:
            print("\nâŒ æœªå‘ç°ä»»ä½•æœ‰æ•ˆè·¯å¾„ï¼Œè¯·æ£€æŸ¥ç›®æ ‡é…ç½®")
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸  æ‰«æè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æ‰«æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")

if __name__ == "__main__":
    main() 