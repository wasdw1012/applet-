import asyncio
import aiohttp
import re
import time
from datetime import datetime
from typing import Dict, List, Set, Optional, Tuple
from urllib.parse import urlparse
from dataclasses import dataclass, field
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class CloudAsset:
    """äº‘èµ„äº§ç»“æ„"""
    name: str
    platform: str  # aws, azure, gcp
    asset_type: str  # s3, blob, storage
    url: str
    status: str  # public, private, not_found, error
    size_estimate: Optional[str] = None
    last_modified: Optional[str] = None
    permissions: List[str] = field(default_factory=list)
    discovered_files: List[str] = field(default_factory=list)

@dataclass
class CloudScanConfig:
    """äº‘æ‰«æé…ç½®"""
    max_concurrent_requests: int = 200
    request_timeout: int = 10
    max_retries: int = 2
    enable_aws_s3: bool = True
    enable_azure_blob: bool = True
    enable_gcp_storage: bool = True
    deep_enumeration: bool = False  # æ˜¯å¦æ·±åº¦æšä¸¾æ–‡ä»¶
    verify_ssl: bool = False

class S3BucketScanner:
    """
    AWS S3 å­˜å‚¨æ¡¶æ‰«æå™¨
    """
    
    def __init__(self, config: CloudScanConfig):
        self.config = config
        self.session: Optional[aiohttp.ClientSession] = None
        self.semaphore = asyncio.Semaphore(config.max_concurrent_requests)
        
        # S3 åŒºåŸŸåˆ—è¡¨
        self.s3_regions = [
            'us-east-1', 'us-east-2', 'us-west-1', 'us-west-2',
            'eu-west-1', 'eu-west-2', 'eu-central-1', 'ap-southeast-1',
            'ap-northeast-1', 'ap-south-1', 'sa-east-1'
        ]
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        connector = aiohttp.TCPConnector(
            limit=self.config.max_concurrent_requests,
            ssl=False,
            ttl_dns_cache=300,
        )
        
        timeout = aiohttp.ClientTimeout(total=self.config.request_timeout)
        
        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout,
            headers={'User-Agent': 'CloudAssetScanner/1.0'}
        )
        
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€€å‡º"""
        if self.session:
            await self.session.close()
    
    def generate_bucket_names(self, target_domain: str) -> List[str]:
        """æ ¹æ®ç›®æ ‡åŸŸåç”Ÿæˆæ½œåœ¨çš„S3å­˜å‚¨æ¡¶åç§°"""
        # æå–åŸŸååŸºç¡€ä¿¡æ¯
        domain_clean = target_domain.replace('https://', '').replace('http://', '')
        domain_parts = domain_clean.split('.')
        company_name = domain_parts[0]
        
        # åŸºç¡€è¯æ±‡
        base_words = [
            company_name,
            domain_clean.replace('.', '-'),
            domain_clean.replace('.', ''),
            company_name.replace('-', ''),
            company_name.replace('_', ''),
        ]
        
        # å¸¸è§åç¼€
        suffixes = [
            '', '-backup', '-backups', '-data', '-assets', '-files', '-docs',
            '-images', '-media', '-uploads', '-download', '-downloads',
            '-static', '-public', '-private', '-staging', '-prod', '-production',
            '-dev', '-development', '-test', '-testing', '-logs', '-log',
            '-config', '-configs', '-database', '-db', '-dump', '-export',
            '-archive', '-archives', '-temp', '-tmp', '-cache',
            '-www', '-web', '-site', '-website', '-cdn', '-content',
            '-bucket', '-storage', '-store', '-repo', '-repository'
        ]
        
        # å¸¸è§å‰ç¼€
        prefixes = [
            '', 'www-', 'assets-', 'static-', 'media-', 'files-', 'data-',
            'backup-', 'prod-', 'dev-', 'test-', 'staging-', 'public-',
            'private-', 'internal-', 'external-', 'admin-', 'user-',
            'client-', 'customer-', 'api-', 'app-', 'mobile-'
        ]
        
        # ç”Ÿæˆç»„åˆ
        bucket_names = set()
        
        # åŸºç¡€åç§°
        for base in base_words:
            bucket_names.add(base)
            
            # æ·»åŠ åç¼€
            for suffix in suffixes:
                bucket_names.add(f"{base}{suffix}")
            
            # æ·»åŠ å‰ç¼€
            for prefix in prefixes:
                bucket_names.add(f"{prefix}{base}")
                
                # å‰ç¼€+åç¼€ç»„åˆ
                for suffix in suffixes[:10]:  # é™åˆ¶ç»„åˆæ•°é‡
                    bucket_names.add(f"{prefix}{base}{suffix}")
        
        # ç‰¹æ®Šç»„åˆ
        special_combinations = [
            f"{company_name}-com",
            f"{company_name}-net",
            f"{company_name}-org",
            f"{company_name}com",
            f"{company_name}backup",
            f"{company_name}data",
            f"backup{company_name}",
            f"data{company_name}",
            f"{company_name}2021",
            f"{company_name}2022",
            f"{company_name}2023",
            f"{company_name}2024",
            f"{company_name}2025",
        ]
        
        bucket_names.update(special_combinations)
        
        # è¿‡æ»¤æ— æ•ˆåç§°
        valid_buckets = []
        for name in bucket_names:
            if self._is_valid_bucket_name(name):
                valid_buckets.append(name.lower())
        
        return sorted(list(set(valid_buckets)))
    
    def _is_valid_bucket_name(self, name: str) -> bool:
        """éªŒè¯S3å­˜å‚¨æ¡¶åç§°æ˜¯å¦æœ‰æ•ˆ"""
        if not name or len(name) < 3 or len(name) > 63:
            return False
        
        # S3å‘½åè§„åˆ™
        if not re.match(r'^[a-z0-9.-]+$', name):
            return False
        
        if name.startswith('.') or name.endswith('.'):
            return False
        
        if '..' in name or '.-' in name or '-.' in name:
            return False
        
        return True
    
    async def check_bucket_exists(self, bucket_name: str) -> CloudAsset:
        """æ£€æŸ¥å•ä¸ªS3å­˜å‚¨æ¡¶æ˜¯å¦å­˜åœ¨åŠå…¶æƒé™"""
        async with self.semaphore:
            # å°è¯•å¤šä¸ªS3ç«¯ç‚¹
            endpoints = [
                f"https://{bucket_name}.s3.amazonaws.com",
                f"https://s3.amazonaws.com/{bucket_name}",
                f"https://{bucket_name}.s3-us-west-2.amazonaws.com",
                f"https://{bucket_name}.s3-eu-west-1.amazonaws.com",
            ]
            
            for endpoint in endpoints:
                try:
                    # å‘é€HEADè¯·æ±‚æ£€æŸ¥å­˜åœ¨æ€§
                    async with self.session.head(endpoint) as response:
                        status = self._analyze_s3_response(response.status, response.headers)
                        
                        asset = CloudAsset(
                            name=bucket_name,
                            platform='aws',
                            asset_type='s3',
                            url=endpoint,
                            status=status
                        )
                        
                        # å¦‚æœå­˜åœ¨ä¸”å¯è®¿é—®ï¼Œå°è¯•è·å–æ›´å¤šä¿¡æ¯
                        if status == 'public':
                            await self._enumerate_bucket_contents(asset)
                        
                        return asset
                
                except asyncio.TimeoutError:
                    continue
                except Exception as e:
                    continue
            
            # æ‰€æœ‰ç«¯ç‚¹éƒ½å¤±è´¥
            return CloudAsset(
                name=bucket_name,
                platform='aws',
                asset_type='s3',
                url=f"https://{bucket_name}.s3.amazonaws.com",
                status='not_found'
            )
    
    def _analyze_s3_response(self, status_code: int, headers: dict) -> str:
        """åˆ†æS3å“åº”çŠ¶æ€"""
        if status_code == 200:
            return 'public'  # å…¬å¼€å¯è¯»
        elif status_code == 403:
            return 'private'  # å­˜åœ¨ä½†ç§æœ‰
        elif status_code == 404:
            return 'not_found'  # ä¸å­˜åœ¨
        elif status_code == 301 or status_code == 302:
            return 'redirect'  # é‡å®šå‘
        else:
            return 'unknown'
    
    async def _enumerate_bucket_contents(self, asset: CloudAsset):
        """æšä¸¾å­˜å‚¨æ¡¶å†…å®¹ï¼ˆä»…å¯¹å…¬å¼€å¯è¯»çš„æ¡¶ï¼‰"""
        if not self.config.deep_enumeration:
            return
        
        try:
            async with self.session.get(asset.url) as response:
                if response.status == 200:
                    content = await response.text()
                    
                    # è§£æXMLå“åº”ï¼Œæå–æ–‡ä»¶åˆ—è¡¨
                    files = self._parse_s3_listing(content)
                    asset.discovered_files = files[:50]  # é™åˆ¶æ˜¾ç¤ºæ–‡ä»¶æ•°é‡
                    
                    # ä¼°ç®—å¤§å°
                    if len(files) > 0:
                        asset.size_estimate = f"~{len(files)} files"
        
        except Exception as e:
            logger.debug(f"Failed to enumerate bucket {asset.name}: {e}")
    
    def _parse_s3_listing(self, xml_content: str) -> List[str]:
        """è§£æS3åˆ—è¡¨XMLå“åº”"""
        files = []
        
        # ç®€å•çš„XMLè§£æï¼Œæå–Keyæ ‡ç­¾
        import re
        key_pattern = r'<Key>(.*?)</Key>'
        matches = re.findall(key_pattern, xml_content)
        
        for match in matches:
            if match and not match.endswith('/'):  # å¿½ç•¥ç›®å½•
                files.append(match)
        
        return files

class CloudAssetScanner:
    """
    äº‘èµ„äº§å‘ç°å¼•æ“ä¸»ç±»
    """
    
    def __init__(self, config: CloudScanConfig = None):
        self.config = config or CloudScanConfig()
        self.discovered_assets: List[CloudAsset] = []
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_checked': 0,
            'buckets_found': 0,
            'public_buckets': 0,
            'private_buckets': 0,
            'scan_duration': 0,
            'start_time': None
        }
    
    async def scan_target(self, target_domain: str) -> List[CloudAsset]:
        """æ‰«æç›®æ ‡çš„äº‘èµ„äº§"""
        self.stats['start_time'] = time.time()
        
        print(f"ğŸŒ©ï¸ å¼€å§‹äº‘èµ„äº§å‘ç°: {target_domain}")
        print(f"  ç›®æ ‡èŒƒå›´: AWS S3 å­˜å‚¨æ¡¶")
        
        results = []
        
        # AWS S3 æ‰«æ
        if self.config.enable_aws_s3:
            s3_results = await self._scan_aws_s3(target_domain)
            results.extend(s3_results)
        
        self.discovered_assets = results
        self.stats['scan_duration'] = time.time() - self.stats['start_time']
        
        self._print_summary()
        
        return results
    
    async def _scan_aws_s3(self, target_domain: str) -> List[CloudAsset]:
        """æ‰«æAWS S3å­˜å‚¨æ¡¶"""
        print(f"  ç”ŸæˆS3å­˜å‚¨æ¡¶å­—å…¸...")
        
        async with S3BucketScanner(self.config) as scanner:
            bucket_names = scanner.generate_bucket_names(target_domain)
            
            print(f"ğŸ“‹ ç”Ÿæˆå€™é€‰å­˜å‚¨æ¡¶: {len(bucket_names)} ä¸ª")
            print(f"  å¼€å§‹å¹¶å‘æ£€æµ‹...")
            
            # å¹¶å‘æ£€æµ‹æ‰€æœ‰å­˜å‚¨æ¡¶
            tasks = []
            for bucket_name in bucket_names:
                task = scanner.check_bucket_exists(bucket_name)
                tasks.append(task)
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # è¿‡æ»¤æœ‰æ•ˆç»“æœ
            valid_assets = []
            for result in results:
                if isinstance(result, CloudAsset) and result.status != 'not_found':
                    valid_assets.append(result)
                    
                    if result.status == 'public':
                        self.stats['public_buckets'] += 1
                        print(f"  å‘ç°å…¬å¼€å­˜å‚¨æ¡¶: {result.name}")
                    elif result.status == 'private':
                        self.stats['private_buckets'] += 1
                        print(f"  å‘ç°ç§æœ‰å­˜å‚¨æ¡¶: {result.name}")
            
            self.stats['total_checked'] = len(bucket_names)
            self.stats['buckets_found'] = len(valid_assets)
            
            return valid_assets
    
    def _print_summary(self):
        """æ‰“å°æ‰«ææ‘˜è¦"""
        print(f"\n{'='*60}")
        print(f"ğŸŒ©ï¸ äº‘èµ„äº§å‘ç°å®Œæˆ")
        print(f"{'='*60}")
        print(f"  æ‰«æç»Ÿè®¡:")
        print(f"    æ£€æµ‹æ€»æ•°: {self.stats['total_checked']}")
        print(f"  ğŸ“¦ å‘ç°å­˜å‚¨æ¡¶: {self.stats['buckets_found']}")
        print(f"    å…¬å¼€å¯è¯»: {self.stats['public_buckets']}")
        print(f"    ç§æœ‰å­˜å‚¨æ¡¶: {self.stats['private_buckets']}")
        print(f"     æ‰«æè€—æ—¶: {self.stats['scan_duration']:.2f}ç§’")
        
        if self.stats['public_buckets'] > 0:
            print(f"\n  é£é™©æé†’:")
            print(f"  å‘ç° {self.stats['public_buckets']} ä¸ªå…¬å¼€å¯è¯»çš„å­˜å‚¨æ¡¶")
            print(f"  å»ºè®®ç«‹å³æ£€æŸ¥æ˜¯å¦åŒ…å«æ•æ„Ÿæ•°æ®")
        
        print(f"\n  è¯¦ç»†ç»“æœ:")
        for asset in self.discovered_assets:
            status_emoji = {
                'public': ' ',
                'private': ' ',
                'redirect': 'ğŸ”„',
                'unknown': 'â“'
            }.get(asset.status, 'ğŸ“¦')
            
            print(f"  {status_emoji} {asset.name} - {asset.status.upper()}")
            print(f"     URL: {asset.url}")
            if asset.discovered_files:
                print(f"     æ–‡ä»¶: {len(asset.discovered_files)} ä¸ª")
            print()
    
    def save_report(self, filename: str):
        """ä¿å­˜æ‰«ææŠ¥å‘Š"""
        import json
        
        report_data = {
            'scan_info': {
                'scanner': 'CloudAssetScanner',
                'version': '1.0',
                'timestamp': datetime.now().isoformat(),
                'duration': self.stats['scan_duration'],
                'total_checked': self.stats['total_checked'],
                'buckets_found': self.stats['buckets_found'],
                'public_buckets': self.stats['public_buckets'],
                'private_buckets': self.stats['private_buckets']
            },
            'discovered_assets': [
                {
                    'name': asset.name,
                    'platform': asset.platform,
                    'type': asset.asset_type,
                    'url': asset.url,
                    'status': asset.status,
                    'size_estimate': asset.size_estimate,
                    'discovered_files': asset.discovered_files[:10]  # åªä¿å­˜å‰10ä¸ªæ–‡ä»¶
                }
                for asset in self.discovered_assets
            ]
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)
        
        print(f"  æŠ¥å‘Šå·²ä¿å­˜: {filename}")

# å‘½ä»¤è¡Œæ¥å£
async def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='äº‘èµ„äº§å‘ç°å¼•æ“')
    parser.add_argument('target', help='ç›®æ ‡åŸŸå')
    parser.add_argument('-o', '--output', help='è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶å')
    parser.add_argument('-c', '--concurrency', type=int, default=200, help='å¹¶å‘æ•°')
    parser.add_argument('--deep', action='store_true', help='æ·±åº¦æšä¸¾ï¼ˆæšä¸¾æ–‡ä»¶åˆ—è¡¨ï¼‰')
    parser.add_argument('--timeout', type=int, default=10, help='è¯·æ±‚è¶…æ—¶æ—¶é—´')
    
    args = parser.parse_args()
    
    # åˆ›å»ºé…ç½®
    config = CloudScanConfig(
        max_concurrent_requests=args.concurrency,
        request_timeout=args.timeout,
        deep_enumeration=args.deep
    )
    
    # åˆ›å»ºæ‰«æå™¨
    scanner = CloudAssetScanner(config)
    
    # æ‰§è¡Œæ‰«æ
    results = await scanner.scan_target(args.target)
    
    # ä¿å­˜æŠ¥å‘Š
    if args.output:
        scanner.save_report(args.output)
    else:
        # é»˜è®¤æ–‡ä»¶å
        domain_safe = args.target.replace('https://', '').replace('http://', '').replace('/', '_')
        filename = f"cloud_assets_{domain_safe}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        scanner.save_report(filename)

if __name__ == "__main__":
    asyncio.run(main()) 