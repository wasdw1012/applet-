
#èµ„äº§å®šä½å…¨é¢ä¾¦å¯Ÿ


import asyncio
import aiohttp
import json
import logging
import subprocess
import time
from datetime import datetime
from urllib.parse import urlparse, urljoin
from typing import Dict, List, Set, Optional, Any
import re
import ssl
import certifi
from dataclasses import dataclass, field
from collections import deque
import ipaddress

@dataclass
class DomainDiscoveryResult:
    """åŸŸåå‘ç°ç»“æœ"""
    domain: str
    source_domain: str = ""  # ä»å“ªä¸ªåŸŸåå‘ç°çš„
    discovery_method: str = ""  # å‘ç°æ–¹å¼
    discovery_depth: int = 0  # å‘ç°æ·±åº¦
    timestamp: datetime = field(default_factory=datetime.now)
    is_internal: bool = False  # æ˜¯å¦å†…éƒ¨åŸŸå
    risk_score: int = 0  # é£é™©è¯„åˆ†

@dataclass 
class ChainTrackingConfig:
    """é“¾å¼è¿½è¸ªé…ç½®"""
    max_scan_depth: int = 3  # æœ€å¤§æ‰«ææ·±åº¦
    max_domain_count: int = 10  # æœ€å¤§åŸŸåæ•°é‡
    scan_interval: float = 2.0  # æ‰«æé—´éš”ï¼ˆç§’ï¼‰
    enable_internal_scan: bool = True  # æ˜¯å¦æ‰«æå†…ç½‘åŸŸå
    enable_ip_scan: bool = False  # æ˜¯å¦æ‰«æIPåœ°å€
    scope_domains: List[str] = field(default_factory=list)  # æ‰«æèŒƒå›´åŸŸå

class ChainTrackingManager:
    """å²è¯—çº§é“¾å¼è¿½è¸ªç®¡ç†å™¨ - è‡ªåŠ¨å‘ç°æ•´ä¸ªèµ„äº§ç½‘ç»œ"""
    
    def __init__(self, initial_domain: str, config: ChainTrackingConfig = None):
        self.initial_domain = initial_domain
        self.config = config or ChainTrackingConfig()
        
        # æ ¸å¿ƒæ•°æ®ç»“æ„
        self.scanned_domains: Set[str] = set()  # å·²æ‰«æé›†åˆ
        self.scan_queue: deque = deque()  # å¾…æ‰«æé˜Ÿåˆ—
        self.global_results: Dict[str, 'ScanResult'] = {}  # å…¨å±€ç»“æœå­—å…¸
        self.discovery_chain: Dict[str, DomainDiscoveryResult] = {}  # å‘ç°é“¾è·¯
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.chain_stats = {
            'total_discovered': 0,
            'total_scanned': 0,
            'depth_distribution': {},
            'discovery_methods': {},
            'high_risk_domains': [],
            'scan_duration': 0,
            'circular_references_blocked': 0,  # å¾ªç¯å¼•ç”¨é˜»æ­¢æ¬¡æ•°
            'concurrent_batches': 0,           # å¹¶å‘æ‰¹æ¬¡æ•°
            'total_concurrent_scans': 0       # æ€»å¹¶å‘æ‰«ææ•°
        }
        
        # åˆå§‹åŒ–ï¼šå°†åˆå§‹åŸŸååŠ å…¥é˜Ÿåˆ—
        initial_discovery = DomainDiscoveryResult(
            domain=initial_domain,
            source_domain="manual_input",
            discovery_method="initial_target",
            discovery_depth=0
        )
        self.scan_queue.append(initial_discovery)
        self.discovery_chain[initial_domain] = initial_discovery
        
        print(f"[+] é“¾å¼è¿½è¸ªç®¡ç†å™¨å·²åˆå§‹åŒ–")
        print(f"    åˆå§‹ç›®æ ‡: {initial_domain}")
        print(f"    æœ€å¤§æ·±åº¦: {self.config.max_scan_depth}")
        print(f"    æœ€å¤§åŸŸåæ•°: {self.config.max_domain_count}")
    
    def add_discovered_domain(self, domain: str, source_domain: str = "", 
                            discovery_method: str = "", depth: int = 0) -> bool:
        """æ·»åŠ æ–°å‘ç°çš„åŸŸå"""
        # æ ‡å‡†åŒ–åŸŸå
        domain = self._normalize_domain(domain)
        if not domain:
            return False
        
        # ğŸ”„ å¾ªç¯å¼•ç”¨æ£€æµ‹ - é¿å…Aâ†’Bâ†’Açš„æƒ…å†µ
        if domain == source_domain:
            self.chain_stats['circular_references_blocked'] += 1
            print(f"[ğŸ”„] é˜»æ­¢å¾ªç¯å¼•ç”¨: {domain} â† {source_domain}")
            return False  # è·³è¿‡å¾ªç¯å¼•ç”¨
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if domain in self.scanned_domains or domain in [d.domain for d in self.scan_queue]:
            return False
        
        # æ™ºèƒ½è¿‡æ»¤
        if not self._should_scan_domain(domain, depth):
            return False
        
        # åˆ›å»ºå‘ç°ç»“æœ
        discovery_result = DomainDiscoveryResult(
            domain=domain,
            source_domain=source_domain,
            discovery_method=discovery_method,
            discovery_depth=depth,
            is_internal=self._is_internal_domain(domain),
            risk_score=self._calculate_risk_score(domain)
        )
        
        # åŠ å…¥é˜Ÿåˆ—å’Œè¿½è¸ªé“¾
        self.scan_queue.append(discovery_result)
        self.discovery_chain[domain] = discovery_result
        self.chain_stats['total_discovered'] += 1
        
        # æ›´æ–°ç»Ÿè®¡
        self.chain_stats['depth_distribution'][depth] = self.chain_stats['depth_distribution'].get(depth, 0) + 1
        self.chain_stats['discovery_methods'][discovery_method] = self.chain_stats['discovery_methods'].get(discovery_method, 0) + 1
        
        print(f"[+] å‘ç°æ–°åŸŸå: {domain} (æ¥æº: {source_domain}, æ–¹å¼: {discovery_method}, æ·±åº¦: {depth})")
        return True
    
    def get_next_scan_target(self) -> Optional[DomainDiscoveryResult]:
        """è·å–ä¸‹ä¸€ä¸ªæ‰«æç›®æ ‡"""
        if not self.scan_queue:
            return None
        
        # æ£€æŸ¥æ•°é‡é™åˆ¶
        if len(self.scanned_domains) >= self.config.max_domain_count:
            print(f"[!] è¾¾åˆ°æœ€å¤§æ‰«æåŸŸåæ•°é™åˆ¶: {self.config.max_domain_count}")
            return None
        
        return self.scan_queue.popleft()
    
    def mark_domain_scanned(self, domain: str, scan_result: 'ScanResult'):
        """æ ‡è®°åŸŸåå·²æ‰«æ"""
        self.scanned_domains.add(domain)
        self.global_results[domain] = scan_result
        self.chain_stats['total_scanned'] += 1
        
        # è¯„ä¼°é£é™©ç­‰çº§
        risk_score = self._evaluate_scan_risk(scan_result)
        if risk_score > 70:
            self.chain_stats['high_risk_domains'].append(domain)
        
        print(f"[âœ“] åŸŸåæ‰«æå®Œæˆ: {domain} (é£é™©è¯„åˆ†: {risk_score})")
    
    def _normalize_domain(self, domain: str) -> str:
        """æ ‡å‡†åŒ–åŸŸå"""
        if not domain:
            return ""
        
        # ç§»é™¤åè®®å‰ç¼€
        domain = re.sub(r'^https?://', '', domain)
        # ç§»é™¤è·¯å¾„
        domain = domain.split('/')[0]
        # ç§»é™¤ç«¯å£
        domain = domain.split(':')[0]
        # è½¬æ¢ä¸ºå°å†™
        domain = domain.lower().strip()
        
        # åŸºæœ¬éªŒè¯
        if not domain or '.' not in domain:
            return ""
        
        return domain
    
    def _should_scan_domain(self, domain: str, depth: int) -> bool:
        """æ™ºèƒ½è¿‡æ»¤ï¼šåˆ¤æ–­æ˜¯å¦åº”è¯¥æ‰«æè¯¥åŸŸå"""
        # æ·±åº¦é™åˆ¶
        if depth > self.config.max_scan_depth:
            return False
        
        # æ£€æŸ¥æ˜¯å¦åœ¨æ‰«æèŒƒå›´å†…
        if self.config.scope_domains:
            if not any(self._is_subdomain_of(domain, scope) for scope in self.config.scope_domains):
                return False
        else:
            # é»˜è®¤åªæ‰«æä¸»åŸŸåçš„å­åŸŸå
            main_domain = self._extract_main_domain(self.initial_domain)
            if not self._is_subdomain_of(domain, main_domain):
                return False
        
        # è·³è¿‡æ˜æ˜¾çš„ç¬¬ä¸‰æ–¹åŸŸå
        if self._is_obvious_third_party(domain):
            return False
        
        # å†…ç½‘åŸŸåæ£€æŸ¥
        if self._is_internal_domain(domain) and not self.config.enable_internal_scan:
            return False
        
        # IPåœ°å€æ£€æŸ¥
        if self._is_ip_address(domain) and not self.config.enable_ip_scan:
            return False
        
        return True
    
    def _is_subdomain_of(self, subdomain: str, main_domain: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºå­åŸŸå"""
        return subdomain == main_domain or subdomain.endswith('.' + main_domain)
    
    def _extract_main_domain(self, domain: str) -> str:
        """æå–ä¸»åŸŸå"""
        parts = domain.split('.')
        if len(parts) >= 2:
            return '.'.join(parts[-2:])
        return domain
    
    def _is_obvious_third_party(self, domain: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºæ˜æ˜¾çš„ç¬¬ä¸‰æ–¹åŸŸå"""
        third_party_patterns = [
            'googleapis.com', 'cloudflare.com', 'amazon.com', 'microsoft.com',
            'google.com', 'facebook.com', 'twitter.com', 'linkedin.com',
            'bootstrap', 'jquery', 'cdnjs', 'jsdelivr', 'unpkg.com'
        ]
        return any(pattern in domain for pattern in third_party_patterns)
    
    def _is_internal_domain(self, domain: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºå†…éƒ¨åŸŸå"""
        internal_patterns = [
            '.local', '.internal', '.intranet', '.corp', '.lan',
            'localhost', '127.0.0.1', '10.', '172.', '192.168.'
        ]
        return any(pattern in domain for pattern in internal_patterns)
    
    def _is_ip_address(self, domain: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºIPåœ°å€"""
        try:
            ipaddress.ip_address(domain)
            return True
        except ValueError:
            return False
    
    def _calculate_risk_score(self, domain: str) -> int:
        """è®¡ç®—åŸŸåé£é™©è¯„åˆ†"""
        score = 50  # åŸºç¡€åˆ†
        
        # å†…éƒ¨åŸŸååŠ åˆ†
        if self._is_internal_domain(domain):
            score += 30
        
        # ç®¡ç†ç›¸å…³åŸŸååŠ åˆ†
        admin_keywords = ['admin', 'manage', 'console', 'panel', 'backend']
        if any(keyword in domain for keyword in admin_keywords):
            score += 20
        
        # å¼€å‘æµ‹è¯•ç¯å¢ƒåŠ åˆ†
        dev_keywords = ['dev', 'test', 'staging', 'beta', 'debug']
        if any(keyword in domain for keyword in dev_keywords):
            score += 15
        
        return min(score, 100)
    
    def _evaluate_scan_risk(self, scan_result: 'ScanResult') -> int:
        """è¯„ä¼°æ‰«æç»“æœçš„é£é™©ç­‰çº§"""
        score = 0
        
        # æ•æ„Ÿæ–‡ä»¶å‘ç°
        score += len(scan_result.files) * 10
        
        # ç®¡ç†é¢æ¿å‘ç°
        score += len(scan_result.admin_panels) * 15
        
        # APIç«¯ç‚¹å‘ç°
        score += len(scan_result.api_routes) * 5
        
        # è¡¨å•å‘ç°
        score += len(scan_result.forms) * 8
        
        return min(score, 100)
    
    def has_more_targets(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¿˜æœ‰å¾…æ‰«æç›®æ ‡"""
        return len(self.scan_queue) > 0 and len(self.scanned_domains) < self.config.max_domain_count
    
    def get_chain_summary(self) -> Dict:
        """è·å–é“¾å¼è¿½è¸ªæ‘˜è¦"""
        return {
            'initial_domain': self.initial_domain,
            'total_discovered': self.chain_stats['total_discovered'],
            'total_scanned': self.chain_stats['total_scanned'],
            'pending_scan': len(self.scan_queue),
            'depth_distribution': self.chain_stats['depth_distribution'],
            'discovery_methods': self.chain_stats['discovery_methods'],
            'high_risk_domains': self.chain_stats['high_risk_domains'],
            'circular_references_blocked': self.chain_stats['circular_references_blocked'],
            'concurrent_batches': self.chain_stats['concurrent_batches'],
            'total_concurrent_scans': self.chain_stats['total_concurrent_scans'],
            'discovery_chain': {domain: {
                'source': result.source_domain,
                'method': result.discovery_method,
                'depth': result.discovery_depth,
                'risk_score': result.risk_score
            } for domain, result in self.discovery_chain.items()}
        }

# å¯¼å…¥å™ªéŸ³è¿‡æ»¤å™¨ - é˜²æ­¢"å‚»é€¼å…´å¥‹"
NOISE_FILTER_AVAILABLE = False
try:
    # å°è¯•ç›¸å¯¹å¯¼å…¥ï¼ˆä½œä¸ºæ¨¡å—è¿è¡Œæ—¶ï¼‰
    from .third_party_blacklist import (
        smart_filter, 
        filter_third_party_urls,
        analyze_noise_level,
        is_third_party,
        has_security_value
    )
    NOISE_FILTER_AVAILABLE = True
except ImportError:
    try:
        # å°è¯•ç»å¯¹å¯¼å…¥ï¼ˆç›´æ¥æ‰§è¡Œè„šæœ¬æ—¶ï¼‰
        from third_party_blacklist import (
            smart_filter,
            filter_third_party_urls,
            analyze_noise_level,
            is_third_party,
            has_security_value
        )
        NOISE_FILTER_AVAILABLE = True
    except ImportError:
        try:
            # å°è¯•ä»å½“å‰ç›®å½•å¯¼å…¥
            import sys
            import os
            sys.path.append(os.path.dirname(os.path.abspath(__file__)))
            from third_party_blacklist import (
                smart_filter,
                filter_third_party_urls,
                analyze_noise_level,
                is_third_party,
                has_security_value
            )
            NOISE_FILTER_AVAILABLE = True
        except ImportError:
            print("  è­¦å‘Š: å™ªéŸ³è¿‡æ»¤å™¨ä¸å¯ç”¨ï¼Œå¯èƒ½ä¼šæœ‰å¤§é‡ç¬¬ä¸‰æ–¹æœåŠ¡å™ªéŸ³")

# å¯¼å…¥ WAF Defender - é˜²æ­¢WAFæ¬ºéª—å“åº”
WAF_DEFENDER_AVAILABLE = False
try:
    # å°è¯•ç›¸å¯¹å¯¼å…¥ï¼ˆä½œä¸ºæ¨¡å—è¿è¡Œæ—¶ï¼‰
    from .waf_defender import create_waf_defender, WAFDefender
    WAF_DEFENDER_AVAILABLE = True
except ImportError:
    try:
        # å°è¯•ç»å¯¹å¯¼å…¥ï¼ˆç›´æ¥æ‰§è¡Œè„šæœ¬æ—¶ï¼‰
        from waf_defender import create_waf_defender, WAFDefender
        WAF_DEFENDER_AVAILABLE = True
    except ImportError:
        try:
            # å°è¯•ä»å½“å‰ç›®å½•å¯¼å…¥
            import sys
            import os
            sys.path.append(os.path.dirname(os.path.abspath(__file__)))
            from waf_defender import create_waf_defender, WAFDefender
            WAF_DEFENDER_AVAILABLE = True
        except ImportError:
            print("  è­¦å‘Š: WAF Defenderä¸å¯ç”¨ï¼Œå¯èƒ½ä¼šå—åˆ°WAFæ¬ºéª—")

# å¯¼å…¥åŠ¨æ€IPæ±  - 500ä¸ªIPè½®æ¢
DYNAMIC_IP_AVAILABLE = False
try:
    from .dynamic_ip_pool import init_ip_pool, get_proxy_session, get_ip_stats, force_switch_ip, _global_ip_pool
    DYNAMIC_IP_AVAILABLE = True
except ImportError:
    try:
        from dynamic_ip_pool import init_ip_pool, get_proxy_session, get_ip_stats, force_switch_ip, _global_ip_pool
        DYNAMIC_IP_AVAILABLE = True
    except ImportError:
        try:
            import sys
            import os
            sys.path.append(os.path.dirname(os.path.abspath(__file__)))
            from dynamic_ip_pool import init_ip_pool, get_proxy_session, get_ip_stats, force_switch_ip, _global_ip_pool
            DYNAMIC_IP_AVAILABLE = True
        except ImportError:
            print("  è­¦å‘Š: åŠ¨æ€IPæ± ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨å¸¸è§„è¯·æ±‚")

# å¯¼å…¥User-Agentç®¡ç†å™¨
USER_AGENT_AVAILABLE = False
try:
    from .user_agent_manager import UserAgentManager, get_user_agent_manager
    USER_AGENT_AVAILABLE = True
except ImportError:
    try:
        from user_agent_manager import UserAgentManager, get_user_agent_manager
        USER_AGENT_AVAILABLE = True
    except ImportError:
        try:
            import sys
            import os
            sys.path.append(os.path.dirname(os.path.abspath(__file__)))
            from user_agent_manager import UserAgentManager, get_user_agent_manager
            USER_AGENT_AVAILABLE = True
        except ImportError:
            print("  è­¦å‘Š: User-Agentç®¡ç†å™¨ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨åŸºç¡€è¯·æ±‚å¤´")

# å¯¼å…¥è®¤è¯ç®¡ç†å™¨ - è®¿é—®è®¤è¯åçš„å†…éƒ¨èµ„äº§
try:
    from .auth_manager import AuthenticationManager, AuthConfig, create_auth_manager
    AUTH_MANAGER_AVAILABLE = True
    print("è®¤è¯ è®¤è¯ç®¡ç†å™¨å·²åŠ è½½ - å¯è®¿é—®è®¤è¯åå†…éƒ¨èµ„äº§")
except ImportError:
    AUTH_MANAGER_AVAILABLE = False
    print("   è­¦å‘Š: è®¤è¯ç®¡ç†å™¨ä¸å¯ç”¨ - æ— æ³•è®¿é—®è®¤è¯åèµ„äº§")

# é…ç½®æ—¥å¿—ç³»ç»Ÿ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)

class RequestBypassEnhancer:
    """è¯·æ±‚ç»•è¿‡å¢å¼ºå™¨ - ä¸“é—¨ç”¨äºç»•è¿‡WAFå’Œæ£€æµ‹ç³»ç»Ÿ"""
    
    def __init__(self, target_url: str):
        self.target_url = target_url
        self.current_ua = None
        self.current_headers = {}
        self.request_count = 0
        self.bypass_stats = {
            'ua_rotations': 0,
            'header_variations': 0,
            'requests_made': 0,
            'detected_blocks': 0
        }
        
        # åˆå§‹åŒ–User-Agentç®¡ç†å™¨
        if USER_AGENT_AVAILABLE:
            self.ua_manager = get_user_agent_manager()
        else:
            self.ua_manager = None
    
    def rotate_user_agent(self):
        """è½®æ¢User-Agent"""
        if self.ua_manager:
            self.current_ua = self.ua_manager.rotate_user_agent()
        else:
            self.current_ua = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'
        self.bypass_stats['ua_rotations'] += 1
        return self.current_ua
    
    def generate_realistic_headers(self):
        """ç”Ÿæˆé€¼çœŸçš„HTTPè¯·æ±‚å¤´"""
        if self.ua_manager:
            headers = self.ua_manager.generate_realistic_headers(self.target_url, force_rotate=True)
            self.current_ua = headers.get('User-Agent')
        else:
            # åŸºç¡€å›é€€å¤´
            headers = {
                'User-Agent': self.current_ua or self.rotate_user_agent(),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Cache-Control': 'no-cache',
            }
        
        self.current_headers = headers
        self.bypass_stats['header_variations'] += 1
        return headers
    
    async def create_enhanced_session(self):
        """åˆ›å»ºå¢å¼ºä¼šè¯ï¼ˆåŸºç¡€User-Agentæ¨¡å¼ï¼‰"""
        headers = self.generate_realistic_headers()
        
        connector = aiohttp.TCPConnector(
            ssl=False,
            limit=100,
            ttl_dns_cache=300,
            use_dns_cache=True,
        )
        
        timeout = aiohttp.ClientTimeout(total=30, connect=10)
        
        return aiohttp.ClientSession(
            connector=connector,
            timeout=timeout,
            headers=headers
        )

@dataclass
class ScanResult:
    """æ ‘çŠ¶å…³è”æ¨¡å‹çš„æ‰«æç»“æœæ•°æ®ç»“æ„"""
    # æ ¸å¿ƒï¼šä»¥èµ„äº§ä¸ºä¸­å¿ƒçš„æ ‘çŠ¶å­—å…¸
    assets: Dict[str, Dict] = field(default_factory=dict)
    
    # ä¿ç•™å°‘é‡æ— æ³•å½’å±åˆ°ç‰¹å®šèµ„äº§çš„å‘ç°
    orphaned_findings: Dict = field(default_factory=lambda: {
        "global_technologies": [],  # å…¨å±€æŠ€æœ¯æ ˆä¿¡æ¯
        "cdn_services": [],         # CDN æœåŠ¡
        "external_apis": []         # å¤–éƒ¨APIè°ƒç”¨
    })
    
    def add_asset(self, domain: str, asset_type: str = "subdomain", 
                  protocol: str = "", status: int = 0, title: str = "") -> None:
        """æ·»åŠ æ–°èµ„äº§æˆ–æ›´æ–°ç°æœ‰èµ„äº§"""
        if domain not in self.assets:
            self.assets[domain] = {
                "type": asset_type,
                "protocol": protocol,
                "status": status,
                "title": title,
                "discovery_timestamp": datetime.now().isoformat(),
                "technologies": [],
                "endpoints": {},
                "forms": [],
                "files": [],
                "risk_score": 0,
                "waf_detected": False,
                "cms_info": {},
                "database_info": {},
                "server_info": {}
            }
        else:
            # æ›´æ–°å·²å­˜åœ¨çš„èµ„äº§ä¿¡æ¯
            if protocol: self.assets[domain]["protocol"] = protocol
            if status: self.assets[domain]["status"] = status  
            if title: self.assets[domain]["title"] = title
    
    def add_endpoint(self, domain: str, path: str, endpoint_data: Dict) -> None:
        """ä¸ºæŒ‡å®šèµ„äº§æ·»åŠ ç«¯ç‚¹"""
        self.add_asset(domain)  # ç¡®ä¿èµ„äº§å­˜åœ¨
        self.assets[domain]["endpoints"][path] = endpoint_data
    
    def add_technology(self, domain: str, tech_data: Dict) -> None:
        """ä¸ºæŒ‡å®šèµ„äº§æ·»åŠ æŠ€æœ¯æ ˆä¿¡æ¯"""
        self.add_asset(domain)  # ç¡®ä¿èµ„äº§å­˜åœ¨
        self.assets[domain]["technologies"].append(tech_data)
    
    def add_form(self, domain: str, form_data: Dict) -> None:
        """ä¸ºæŒ‡å®šèµ„äº§æ·»åŠ è¡¨å•"""
        self.add_asset(domain)  # ç¡®ä¿èµ„äº§å­˜åœ¨
        self.assets[domain]["forms"].append(form_data)
    
    def add_file(self, domain: str, file_data: Dict) -> None:
        """ä¸ºæŒ‡å®šèµ„äº§æ·»åŠ æ•æ„Ÿæ–‡ä»¶"""
        self.add_asset(domain)  # ç¡®ä¿èµ„äº§å­˜åœ¨
        self.assets[domain]["files"].append(file_data)
    
    def calculate_risk_scores(self) -> None:
        """è®¡ç®—æ¯ä¸ªèµ„äº§çš„é£é™©è¯„åˆ†"""
        for domain, asset in self.assets.items():
            score = 0
            
            # åŸºç¡€åˆ†æ•°
            if asset["type"] == "main_domain":
                score += 20
            elif asset["type"] == "subdomain":
                score += 10
                
            # ç«¯ç‚¹æ•°é‡å½±å“
            endpoint_count = len(asset["endpoints"])
            score += min(endpoint_count * 5, 30)  # æœ€å¤š30åˆ†
            
            # é«˜é£é™©ç«¯ç‚¹åŠ åˆ†
            for path, endpoint in asset["endpoints"].items():
                if any(keyword in path.lower() for keyword in ['admin', 'login', 'api', 'graphql', 'upload']):
                    score += 15
                if endpoint.get("risk_level") == "high":
                    score += 20
                elif endpoint.get("risk_level") == "medium":
                    score += 10
            
            # æŠ€æœ¯æ ˆé£é™©
            for tech in asset["technologies"]:
                if tech.get("category") == "framework":
                    score += 5
                if tech.get("has_vulnerabilities"):
                    score += 25
            
            # è¡¨å•æ•°é‡
            score += len(asset["forms"]) * 3
            
            # æ•æ„Ÿæ–‡ä»¶
            score += len(asset["files"]) * 8
            
            # WAFæ£€æµ‹å½±å“ï¼ˆè¢«ä¿æŠ¤çš„èµ„äº§é£é™©ç›¸å¯¹è¾ƒä½ï¼‰
            if asset["waf_detected"]:
                score = int(score * 0.8)
            
            asset["risk_score"] = min(score, 100)  # æœ€é«˜100åˆ†
    
    def get_attack_surface_map(self) -> Dict:
        """ç”Ÿæˆæ”»å‡»é¢åœ°å›¾"""
        attack_map = {}
        
        for domain, asset in self.assets.items():
            # è¯†åˆ«æ”»å‡»è·¯å¾„
            attack_paths = []
            
            # ç®¡ç†åå°è·¯å¾„
            admin_endpoints = [path for path in asset["endpoints"].keys() 
                             if any(keyword in path.lower() for keyword in ['admin', 'login', 'dashboard'])]
            if admin_endpoints:
                attack_paths.append({
                    "type": "admin_access",
                    "endpoints": admin_endpoints,
                    "risk": "high"
                })
            
            # APIç«¯ç‚¹
            api_endpoints = [path for path in asset["endpoints"].keys()
                           if any(keyword in path.lower() for keyword in ['api', 'graphql', 'rest'])]
            if api_endpoints:
                attack_paths.append({
                    "type": "api_access", 
                    "endpoints": api_endpoints,
                    "risk": "medium"
                })
            
            # ä¸Šä¼ åŠŸèƒ½
            upload_endpoints = [path for path in asset["endpoints"].keys()
                              if 'upload' in path.lower()]
            if upload_endpoints:
                attack_paths.append({
                    "type": "file_upload",
                    "endpoints": upload_endpoints, 
                    "risk": "high"
                })
            
            if attack_paths:
                attack_map[domain] = {
                    "asset_info": {
                        "risk_score": asset["risk_score"],
                        "technologies": [t.get("name", "") for t in asset["technologies"]],
                        "waf_protected": asset["waf_detected"]
                    },
                    "attack_paths": attack_paths
                }
        
        return attack_map
    
    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ - æ–°çš„æ ‘çŠ¶ç»“æ„"""
        return {
            "assets": self.assets,
            "orphaned_findings": self.orphaned_findings,
            "asset_count": len(self.assets),
            "total_endpoints": sum(len(asset["endpoints"]) for asset in self.assets.values()),
            "attack_surface_map": self.get_attack_surface_map()
        }
    
    # å…¼å®¹æ€§æ–¹æ³•ï¼šä¸ºäº†ä¸ç ´åç°æœ‰ä»£ç ï¼Œæš‚æ—¶ä¿ç•™æ—§çš„å±æ€§è®¿é—®æ–¹å¼
    @property
    def subdomains(self) -> List[Dict]:
        """å…¼å®¹æ€§ï¼šè¿”å›å­åŸŸååˆ—è¡¨"""
        return [{"domain": domain, **asset} for domain, asset in self.assets.items() 
                if asset["type"] in ["subdomain", "main_domain"]]
    
    @property  
    def admin_panels(self) -> List[Dict]:
        """å…¼å®¹æ€§ï¼šè¿”å›ç®¡ç†é¢æ¿åˆ—è¡¨"""
        panels = []
        for domain, asset in self.assets.items():
            for path, endpoint in asset["endpoints"].items():
                if any(keyword in path.lower() for keyword in ['admin', 'login', 'dashboard']):
                    panels.append({
                        "domain": domain,
                        "url": f"{asset['protocol']}://{domain}{path}",
                        "path": path,
                        **endpoint
                    })
        return panels
    
    @property
    def technologies(self) -> List[Dict]:
        """å…¼å®¹æ€§ï¼šè¿”å›æŠ€æœ¯æ ˆåˆ—è¡¨"""  
        techs = []
        for domain, asset in self.assets.items():
            for tech in asset["technologies"]:
                techs.append({"domain": domain, **tech})
        return techs + self.orphaned_findings["global_technologies"]

class SimpleProxyPool:
    """è½»é‡çº§ä»£ç†æ±  - ç›´æ¥è¯»å–500ä¸ªIPå¹¶è½®æ¢"""
    
    def __init__(self, proxy_file: str = "æ–°å»ºæ–‡æœ¬æ–‡æ¡£.txt"):
        self.proxies = []
        self.index = 0
        self.proxy_file = proxy_file
        
        try:
            # è¯»å–ä»£ç†æ–‡ä»¶
            with open(proxy_file, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if ':' in line:
                        proxy_url = f"socks5://{line}"
                        self.proxies.append(proxy_url)
                    elif line:  # éç©ºè¡Œä½†æ ¼å¼é”™è¯¯
                        logger.warning(f"PROXY_POOL_ERROR: Line {line_num} invalid format: {line}")
            
            if not self.proxies:
                raise ValueError(f"PROXY_POOL_ERROR: No valid proxies found in {proxy_file}")
            
            logger.info(f"PROXY_POOL_SUCCESS: Loaded {len(self.proxies)} proxies from {proxy_file}")
            
        except FileNotFoundError:
            logger.error(f"PROXY_POOL_ERROR: File not found: {proxy_file}")
            raise
        except Exception as e:
            logger.error(f"PROXY_POOL_ERROR: Failed to load proxies: {e}")
            raise
    
    async def get_proxy(self) -> str:
        """è·å–ä¸‹ä¸€ä¸ªä»£ç†åœ°å€"""
        if not self.proxies:
            raise RuntimeError("PROXY_POOL_ERROR: No proxies available")
        
        proxy = self.proxies[self.index % len(self.proxies)]
        self.index += 1
        return proxy
    
    def get_stats(self) -> dict:
        """è·å–ä»£ç†æ± ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_proxies': len(self.proxies),
            'current_index': self.index,
            'proxy_file': self.proxy_file
        }

@dataclass 
class AssetMapperConfig:
    """æ‰«æé…ç½®"""
    max_crawl_pages: int = 100
    max_js_files: int = 50
    request_timeout: int = 10
    subdomain_timeout: int = 5
    concurrent_limit: int = 20
    enable_zone_transfer: bool = True
    enable_crt_check: bool = True
    max_retries: int = 3
    retry_delay: float = 0.5
    
    # ç»•è¿‡å¢å¼ºé…ç½®
    use_dynamic_ip: bool = True  # å¯ç”¨åŠ¨æ€IPæ± 
    use_user_agent: bool = True  # å¯ç”¨User-Agentè½®æ¢
    
    # è®¤è¯é…ç½®
    enable_authentication: bool = False
    auth_config: Dict[str, Any] = field(default_factory=dict)

class AssetMapper:
    def __init__(self, target_domain: str, config: AssetMapperConfig = None, auth_config=None,
                 enable_chain_tracking: bool = True, chain_config: ChainTrackingConfig = None):
        self.target = target_domain
        self.config = config or AssetMapperConfig()
        self.results = ScanResult()
        self.session: Optional[aiohttp.ClientSession] = None
        self.domain_protocols: Dict[str, str] = {}  # ç¼“å­˜åŸŸååè®®ä¿¡æ¯
        self.start_time = time.time()
        
        # ğŸš€ å²è¯—çº§é“¾å¼è¿½è¸ªç®¡ç†å™¨
        self.enable_chain_tracking = enable_chain_tracking
        self.chain_tracking_manager = None
        if enable_chain_tracking:
            self.chain_tracking_manager = ChainTrackingManager(target_domain, chain_config)
            print(f"[+] å²è¯—çº§é“¾å¼è¿½è¸ªå·²å¯ç”¨ï¼")
        
        # ç»•è¿‡å¢å¼ºå™¨é…ç½®
        target_url = f"https://{target_domain}"  # é»˜è®¤ä½¿ç”¨HTTPS
        self.bypass_enhancer = RequestBypassEnhancer(target_url)
        self.use_dynamic_ip = self.config.use_dynamic_ip and DYNAMIC_IP_AVAILABLE
        self.use_user_agent = self.config.use_user_agent and USER_AGENT_AVAILABLE
        self.dynamic_ip_initialized = False
        
        # è®¤è¯ç®¡ç†å™¨é…ç½®
        self.auth_manager = None
        self.auth_config = auth_config or self.config.auth_config if self.config.enable_authentication else None
        if AUTH_MANAGER_AVAILABLE and self.auth_config:
            try:
                if isinstance(self.auth_config, dict):
                    auth_config_obj = AuthConfig(**self.auth_config)
                else:
                    auth_config_obj = self.auth_config
                self.auth_manager = AuthenticationManager(auth_config_obj)
                print("è®¤è¯ è®¤è¯ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ - å‡†å¤‡è®¿é—®è®¤è¯åå†…éƒ¨èµ„äº§")
            except Exception as e:
                print(f"   è®¤è¯ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        
        # æ™ºèƒ½å†³ç­–çŠ¶æ€
        self.is_wordpress: bool = False
        self.is_medical: bool = False
        self.detected_cloud_services: List[str] = []
        self.detected_waf: List[str] = []
        
        # ç¼“å­˜æœºåˆ¶
        self.cache: Dict[str, Dict] = {}
        self.cache_ttl: int = 300  # 5åˆ†é’Ÿç¼“å­˜
        
        # å¹¶å‘æ§åˆ¶ä¿¡å·é‡
        self.path_semaphore = asyncio.Semaphore(20)
        self.js_semaphore = asyncio.Semaphore(10)
        
        # å¸¸è§è·¯å¾„
        self.jp_paths = [
            "/admin", "/wp-admin", "/login", "/member", "/mypage",
            "/reserve", "/reservation", "/yoyaku", "/contact",
            "/patient", "/kanja", "/user",
            "/api", "/ajax", "/json",
            "/export", "/download", "/csv",
            "/backup", "/bak", "/old",
            "/.git", "/.env", "/config",
            "/phpmyadmin", "/pma", "/mysql"
        ]
        
        # ç°ä»£APIè·¯å¾„
        self.api_paths = [
            "/graphql", "/graphiql", "/playground",
            "/__graphql", "/query", "/gql",
            "/api/v1", "/api/v2", "/api/v3",
            "/v1", "/v2", "/v3",
            "/swagger", "/swagger-ui", "/api-docs",
            "/openapi.json", "/swagger.json"
        ]
        
        # ç¬¬ä¸‰æ–¹é›†æˆè·¯å¾„
        self.integration_paths = [
            "/oauth", "/auth", "/callback",
            "/login/line", "/login/google", 
            "/saml", "/sso",
            "/payment", "/stripe", "/paypal",
            "/callback/payment",
            "/line-notify", "/linepay"
        ]
        
        # åŒ»ç–—ç›¸å…³è·¯å¾„
        self.medical_paths = [
            "/patient", "/kanja", "/æ‚£è€…",
            "/doctor", "/åŒ»å¸«", "/ishi", 
            "/appointment", "/äºˆç´„", "/yoyaku",
            "/medical", "/è¨ºç™‚", "/shinryo",
            "/pharmacy", "/è–¬å±€", "/yakkyoku",
            "/insurance", "/ä¿é™º", "/hoken",
            "/dicom", "/pacs", "/ris",
            "/hl7", "/fhir"
        ]
        
        # åˆå¹¶æ‰€æœ‰è·¯å¾„
        self.jp_paths.extend(self.api_paths)
        self.jp_paths.extend(self.integration_paths)
        self.jp_paths.extend(self.medical_paths)
        
        # å…³é”®æ–‡ä»¶å®šä¹‰
        self.key_files = [
            "robots.txt", "sitemap.xml", "crossdomain.xml",
            ".htaccess", "web.config", 
            "package.json", "package-lock.json",  # npmä¾èµ–
            "composer.json", "composer.lock",      # PHPä¾èµ–
            "wp-config.php.bak", "wp-config.php~",
            ".env", ".env.production", ".env.local",  # ç¯å¢ƒå˜é‡
            ".git/config", ".git/HEAD",               # Gitä¿¡æ¯
            ".DS_Store", "Thumbs.db",                 # ç³»ç»Ÿæ–‡ä»¶
            "webpack.config.js", "webpack.mix.js",    # Webpacké…ç½®
            ".map", "app.js.map", "main.js.map"      # Source Maps
        ]
        
        # å†…éƒ¨å­åŸŸåå‰ç¼€
        self.internal_prefixes = [
            "internal", "staff", "admin", "dev", "test", "staging",
            "api", "backend", "dashboard", "manage", "console",
            "vpn", "mail", "ftp", "db", "mysql", "postgres",
            "jenkins", "gitlab", "jira", "wiki", "doc"
        ]
        
        # æ€§èƒ½ç›‘æ§ç»Ÿè®¡
        self.stats = {
            'requests_made': 0,
            'requests_failed': 0,
            'bytes_downloaded': 0,
            'cache_hits': 0,
            'proxy_switches': 0,
            'noise_filtered': 0,        # è¿‡æ»¤çš„å™ªéŸ³æ•°é‡
            'valuable_findings': 0      # æœ‰ä»·å€¼çš„å‘ç°
        }
        
        # åˆå§‹åŒ–å™ªéŸ³è¿‡æ»¤ç»Ÿè®¡ï¼ˆä¸gitå·¥å…·ä¸€è‡´ï¼‰
        self.noise_stats = {
            'total_checked': 0,
            'noise_filtered': 0,
            'valuable_kept': 0,
            'filter_enabled': NOISE_FILTER_AVAILABLE
        }
        
        # ä»£ç†æ± æ”¯æŒ - è‡ªåŠ¨åˆå§‹åŒ–500ä¸ªIP
        try:
            self.proxy_pool = SimpleProxyPool()
            logger.info(f"PROXY_POOL_INIT: Successfully initialized with {self.proxy_pool.get_stats()['total_proxies']} proxies")
        except Exception as e:
            logger.error(f"PROXY_POOL_INIT_FAILED: {e}")
            self.proxy_pool = None
        
        self.current_proxy = None
        
        # WAF Defender çŠ¶æ€
        self.waf_defender = None
        self.waf_defender_initialized = False
        
        # åˆå§‹åŒ–ä¼šè¯ï¼ˆå¦‚æœæœ‰ä»£ç†æ± åˆ™ä½¿ç”¨ç¬¬ä¸€ä¸ªä»£ç†ï¼‰
        self._init_session()
    
    def _init_session(self):
        """åˆå§‹åŒ–HTTPä¼šè¯ï¼ˆä½¿ç”¨ç³»ç»Ÿä»£ç†ï¼‰"""
        timeout = aiohttp.ClientTimeout(total=self.config.request_timeout)
        
        self.session = aiohttp.ClientSession(
            timeout=timeout,
            headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
            }
        )
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        if self.session:
            await self.session.close()
    
    async def check_protocol(self, domain: str) -> Optional[str]:
        """æ™ºèƒ½æ£€æµ‹åŸŸåæ”¯æŒçš„åè®® - æ”¯æŒè®¤è¯"""
        if domain in self.domain_protocols:
            return self.domain_protocols[domain]
        
        # ç¡®ä¿sessionå·²åˆ›å»º
        if not self.session:
            await self._create_session()
        
        for protocol in ['https', 'http']:
            try:
                url = f"{protocol}://{domain}"
                resp = await self.safe_request(url, 'GET', timeout=3, allow_redirects=True)
                if resp and resp.status < 500:
                    self.domain_protocols[domain] = protocol
                    logger.debug(f"åŸŸå {domain} æ”¯æŒåè®®: {protocol}")
                    await resp.release()  # é‡Šæ”¾è¿æ¥
                    return protocol
                if resp:
                    await resp.release()
            except Exception as e:
                logger.debug(f"åè®®æ£€æµ‹å¤±è´¥ {protocol}://{domain}: {type(e).__name__}")
                continue
        
        logger.warning(f"æ— æ³•è¿æ¥åˆ°åŸŸå: {domain}")
        return None
    
    async def _initialize_waf_defender(self, protocol: str):
        """åˆå§‹åŒ– WAF Defender"""
        if not WAF_DEFENDER_AVAILABLE or self.waf_defender_initialized:
            return
        
        try:
            target_url = f"{protocol}://{self.target}"
            logger.info("[+] åˆå§‹åŒ–WAF Defender...")
            
            # ç¡®ä¿sessionå·²åˆ›å»º
            if not self.session:
                await self._create_session()
            
            self.waf_defender = await create_waf_defender(target_url, self.session)
            self.waf_defender_initialized = True
            
            logger.info(f"[+] WAF Defenderåˆå§‹åŒ–æˆåŠŸ (ç›®æ ‡: {target_url})")
            
            # æ˜¾ç¤ºWAF DefenderåŸºçº¿ä¿¡æ¯
            if hasattr(self.waf_defender, 'get_stats'):
                stats = self.waf_defender.get_stats()
                logger.info(f"[+] WAF Defender åŸºçº¿æ•°é‡: {stats.get('baseline_count', 'unknown')}")
                logger.info(f"[+] WAF Defender æ£€æµ‹èƒ½åŠ›: {', '.join(stats.get('detection_capabilities', []))}")
            
        except Exception as e:
            logger.warning(f"[-] WAF Defenderåˆå§‹åŒ–å¤±è´¥: {e}")
            self.waf_defender = None
            self.waf_defender_initialized = False
    
    async def safe_request(self, url: str, method: str = 'GET', **kwargs) -> Optional[aiohttp.ClientResponse]:
        """å®‰å…¨çš„HTTPè¯·æ±‚ï¼ˆæ”¯æŒç»•è¿‡å¢å¼º + è®¤è¯ç®¡ç†å™¨ + é‡è¯•æœºåˆ¶ + æ€§èƒ½ç›‘æ§ï¼‰"""
        start_time = time.time()
        
        for attempt in range(self.config.max_retries):
            try:
                # è®¤è¯ç®¡ç†å™¨ï¼šä¸ºè¯·æ±‚æ·»åŠ è®¤è¯ä¿¡æ¯
                if self.auth_manager:
                    try:
                        kwargs = await self.auth_manager.prepare_request(url, **kwargs)
                    except Exception as e:
                        logger.debug(f"è®¤è¯å‡†å¤‡å¤±è´¥: {e}")
                
                # ä½¿ç”¨å¢å¼ºä¼šè¯ï¼ˆåŠ¨æ€IP + User-Agentï¼‰
                if self.use_dynamic_ip or self.use_user_agent:
                    async with await self.get_enhanced_session() as enhanced_session:
                        if method.upper() == 'GET':
                            response = await enhanced_session.get(url, **kwargs)
                        elif method.upper() == 'POST':
                            response = await enhanced_session.post(url, **kwargs)
                        else:
                            response = await enhanced_session.request(method, url, **kwargs)
                        
                        # æ›´æ–°ç»•è¿‡ç»Ÿè®¡
                        self.bypass_enhancer.bypass_stats['requests_made'] += 1
                else:
                    # ä½¿ç”¨æ ‡å‡†session
                    if method.upper() == 'GET':
                        response = await self.session.get(url, **kwargs)
                    elif method.upper() == 'POST':
                        response = await self.session.post(url, **kwargs)
                    else:
                        response = await self.session.request(method, url, **kwargs)
                
                # è®¤è¯ç®¡ç†å™¨ï¼šæ£€æŸ¥å“åº”è®¤è¯çŠ¶æ€
                auth_ok = True
                if self.auth_manager:
                    auth_ok = await self.auth_manager.handle_response(response, url)
                    if not auth_ok and self.auth_manager.should_retry(response):
                        # è®¤è¯å¤±æ•ˆï¼Œå°è¯•æ¢å¤
                        logger.debug("æ£€æµ‹åˆ°è®¤è¯å¤±æ•ˆï¼Œå°è¯•æ¢å¤...")
                        await response.release()  # é‡Šæ”¾å½“å‰è¿æ¥
                        
                        recovery_success = await self.auth_manager._recover_authentication()
                        if recovery_success:
                            # é‡æ–°å‡†å¤‡è¯·æ±‚å¹¶é‡è¯•
                            kwargs = await self.auth_manager.prepare_request(url, **kwargs)
                            if method.upper() == 'GET':
                                response = await self.session.get(url, **kwargs)
                            elif method.upper() == 'POST':
                                response = await self.session.post(url, **kwargs)
                            else:
                                response = await self.session.request(method, url, **kwargs)
                
                # æ›´æ–°æˆåŠŸç»Ÿè®¡
                self.stats['requests_made'] += 1
                request_time = time.time() - start_time
                
                if hasattr(self, '_request_times'):
                    self._request_times.append(request_time)
                else:
                    self._request_times = [request_time]
                
                # é™åˆ¶è¯·æ±‚æ—¶é—´åˆ—è¡¨å¤§å°ï¼ˆå†…å­˜ä¼˜åŒ–ï¼‰
                if len(self._request_times) > 1000:
                    self._request_times = self._request_times[-500:]
                
                # WAFæ¬ºéª—æ£€æµ‹
                if (self.waf_defender and self.waf_defender_initialized and 
                    response.status == 200):
                    try:
                        # æ ¹æ®å“åº”å†…å®¹çŒœæµ‹é¢„æœŸç±»å‹
                        content_type = response.headers.get('Content-Type', '')
                        if 'json' in content_type:
                            expected_type = 'json'
                        elif 'html' in content_type:
                            expected_type = 'html'
                        else:
                            expected_type = 'unknown'
                        
                        is_real = await self.waf_defender.validate(url, response, expected_type=expected_type)
                        if not is_real:
                            logger.warning(f"WAFæ¬ºéª—æ£€æµ‹: {url} - è·³è¿‡ä¼ªé€ å“åº”")
                            await response.release()  # é‡Šæ”¾è¿æ¥
                            continue  # é‡è¯•æˆ–è¿”å›None
                    except Exception as e:
                        logger.debug(f"WAFæ£€æµ‹å¼‚å¸¸: {e}")
                        # WAFæ£€æµ‹å¤±è´¥æ—¶ä¸å½±å“æ­£å¸¸æµç¨‹
                
                return response
            
            except asyncio.TimeoutError:
                logger.debug(f"è¯·æ±‚è¶…æ—¶ {url} (å°è¯• {attempt + 1}/{self.config.max_retries})")
                self.stats['requests_failed'] += 1
            except aiohttp.ClientError as e:
                logger.debug(f"å®¢æˆ·ç«¯é”™è¯¯ {url}: {type(e).__name__} (å°è¯• {attempt + 1}/{self.config.max_retries})")
                self.stats['requests_failed'] += 1
            except Exception as e:
                logger.debug(f"è¯·æ±‚å¼‚å¸¸ {url}: {type(e).__name__} (å°è¯• {attempt + 1}/{self.config.max_retries})")
                self.stats['requests_failed'] += 1
            
            if attempt < self.config.max_retries - 1:
                await asyncio.sleep(self.config.retry_delay * (attempt + 1))
        
        logger.warning(f"è¯·æ±‚æœ€ç»ˆå¤±è´¥: {url}")
        self.stats['requests_failed'] += 1
        return None
    
    async def _maybe_switch_proxy(self):
        """æ ¹æ®éœ€è¦åˆ‡æ¢ä»£ç†ï¼ˆä»£ç†æ± é›†æˆæ”¯æŒï¼‰"""
        if self.proxy_pool:
            try:
                new_proxy = await self.proxy_pool.get_proxy()
                if new_proxy != self.current_proxy:
                    await self.setup_proxy_session(new_proxy)
                    self.stats['proxy_switches'] += 1
                    logger.debug(f"PROXY_SWITCH_SUCCESS: Changed to {new_proxy}")
            except Exception as e:
                logger.warning(f"PROXY_SWITCH_FAILED: {e}")
    
    def _optimize_memory(self):
        """å†…å­˜ä¼˜åŒ– - æ™ºèƒ½æ¸…ç†å’Œå™ªéŸ³è¿‡æ»¤"""
        original_count = len(self.results.api_routes)
        
        # ç¬¬ä¸€æ­¥ï¼šå™ªéŸ³æ¸…ç†ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if NOISE_FILTER_AVAILABLE and original_count > 0:
            logger.info("æ‰§è¡Œæ™ºèƒ½å™ªéŸ³æ¸…ç†...")
            
            # åˆ†æå½“å‰å™ªéŸ³æ°´å¹³
            noise_analysis = analyze_noise_level([route.get('route', '') for route in self.results.api_routes])
            
            if noise_analysis['noise_ratio'] > 0.3:  # è¶…è¿‡30%æ˜¯å™ªéŸ³
                logger.warning(f"æ£€æµ‹åˆ°é«˜å™ªéŸ³ç¯å¢ƒ: {noise_analysis['noise_ratio']:.1%} çš„å‘ç°æ˜¯ç¬¬ä¸‰æ–¹å™ªéŸ³")
                
                # æ‰§è¡Œæ·±åº¦æ¸…ç†
                cleaned_routes = []
                for route in self.results.api_routes:
                    route_url = route.get('route', '')
                    route_type = route.get('type', 'unknown')
                    
                    # é«˜ä»·å€¼ç±»å‹æ€»æ˜¯ä¿ç•™
                    if route_type in ['credential', 'internal_host']:
                        cleaned_routes.append(route)
                        continue
                    
                    # å…¶ä»–ç±»å‹ä½¿ç”¨æ™ºèƒ½è¿‡æ»¤
                    if smart_filter(route_url, 'api_endpoint'):
                        cleaned_routes.append(route)
                    else:
                        self.stats['noise_filtered'] += 1
                
                noise_removed = len(self.results.api_routes) - len(cleaned_routes)
                self.results.api_routes = cleaned_routes
                logger.info(f"å™ªéŸ³æ¸…ç†å®Œæˆ: ç§»é™¤ {noise_removed} ä¸ªå™ªéŸ³ï¼Œä¿ç•™ {len(cleaned_routes)} ä¸ªæœ‰ä»·å€¼å‘ç°")
        
        # ç¬¬äºŒæ­¥ï¼šå»é‡å’Œæ•°é‡é™åˆ¶
        if len(self.results.api_routes) > 1000:
            logger.warning(f"APIè·¯ç”±ä»ç„¶è¿‡å¤š ({len(self.results.api_routes)}), æ‰§è¡Œå»é‡...")
            
            # æ™ºèƒ½å»é‡ï¼šæŒ‰ä»·å€¼ä¼˜å…ˆçº§æ’åº
            prioritized_routes = sorted(
                self.results.api_routes,
                key=lambda x: self._get_route_priority(x),
                reverse=True
            )
            
            # å»é‡é€»è¾‘
            seen_routes = set()
            deduplicated_routes = []
            
            for route in prioritized_routes:
                route_key = route.get('route', '')
                if route_key not in seen_routes:
                    seen_routes.add(route_key)
                    deduplicated_routes.append(route)
                    
                    # é™åˆ¶æœ€å¤§æ•°é‡
                    if len(deduplicated_routes) >= 500:
                        break
            
            self.results.api_routes = deduplicated_routes
            logger.info(f"æ™ºèƒ½å»é‡å®Œæˆï¼Œä¿ç•™ {len(deduplicated_routes)} ä¸ªæœ€æœ‰ä»·å€¼çš„å‘ç°")
        
        # ç¬¬ä¸‰æ­¥ï¼šæ¸…ç†è¿‡å¤§çš„ç¼“å­˜
        if len(self.cache) > 500:
            # ä¿ç•™æœ€è¿‘çš„250ä¸ªç¼“å­˜é¡¹
            cache_items = list(self.cache.items())
            cache_items.sort(key=lambda x: x[1].get('time', 0), reverse=True)
            self.cache = dict(cache_items[:250])
            logger.debug("ç¼“å­˜ä¼˜åŒ–å®Œæˆ")
        
        # ä¼˜åŒ–ç»Ÿè®¡
        final_count = len(self.results.api_routes)
        if original_count != final_count:
            reduction = (original_count - final_count) / original_count
            logger.info(f"å†…å­˜ä¼˜åŒ–å®Œæˆ: {original_count} â†’ {final_count} (-{reduction:.1%})")
    
    def _get_route_priority(self, route: dict) -> int:
        """è®¡ç®—è·¯ç”±ä¼˜å…ˆçº§åˆ†æ•°ï¼ˆç”¨äºæ’åºï¼‰"""
        score = 0
        route_type = route.get('type', '')
        route_content = route.get('route', '').lower()
        risk_level = route.get('risk_level', 'low')
        
        # ç±»å‹ä¼˜å…ˆçº§
        type_scores = {
            'credential': 100,      # å‡­æ®æœ€é«˜ä¼˜å…ˆçº§
            'internal_host': 90,    # å†…éƒ¨ä¸»æœº
            'graphql': 80,          # GraphQL
            'api_endpoint': 70,     # APIç«¯ç‚¹
            'unknown': 10
        }
        score += type_scores.get(route_type, 10)
        
        # é£é™©ç­‰çº§åŠ åˆ†
        risk_scores = {
            'critical': 50,
            'high': 30,
            'medium': 20,
            'low': 10
        }
        score += risk_scores.get(risk_level, 10)
        
        # å†…å®¹ä»·å€¼åŠ åˆ†
        if any(keyword in route_content for keyword in ['admin', 'api', 'secret', 'token', 'internal']):
            score += 20
        
        return score
    
    def get_performance_stats(self) -> Dict:
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.stats.copy()
        
        if hasattr(self, '_request_times') and self._request_times:
            stats['avg_request_time'] = sum(self._request_times) / len(self._request_times)
            stats['max_request_time'] = max(self._request_times)
            stats['min_request_time'] = min(self._request_times)
        
        # æˆåŠŸç‡
        total_requests = stats['requests_made'] + stats['requests_failed']
        if total_requests > 0:
            stats['success_rate'] = stats['requests_made'] / total_requests
        else:
            stats['success_rate'] = 0.0
        
        # ç¼“å­˜å‘½ä¸­ç‡
        total_cache_requests = stats.get('cache_hits', 0) + stats['requests_made']
        if total_cache_requests > 0:
            stats['cache_hit_rate'] = stats.get('cache_hits', 0) / total_cache_requests
        else:
            stats['cache_hit_rate'] = 0.0
        
        return stats
    
    async def _create_session(self):
        """åˆ›å»ºç»Ÿä¸€çš„HTTP session - æ”¯æŒè®¤è¯å’Œä»£ç†"""
        if self.session and not self.session.closed:
            return self.session
        
        # å¦‚æœæœ‰ä»£ç†æ± ä¸”æ²¡æœ‰è®¾ç½®å½“å‰ä»£ç†ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªä»£ç†
        if self.proxy_pool and not self.current_proxy:
            try:
                first_proxy = await self.proxy_pool.get_proxy()
                await self.setup_proxy_session(first_proxy)
                logger.info(f"PROXY_INIT_SUCCESS: Using first proxy {first_proxy}")
                return self.session
            except Exception as e:
                logger.error(f"PROXY_INIT_FAILED: {e}, falling back to no proxy")
        
        # å¦‚æœå·²æœ‰ä»£ç†ä¼šè¯ï¼Œç›´æ¥è¿”å›
        if self.current_proxy:
            return self.session
        
        # åˆ›å»ºsessionï¼ˆä½¿ç”¨ç³»ç»Ÿä»£ç†è®¾ç½®ï¼‰
        timeout = aiohttp.ClientTimeout(total=30, connect=10)
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        
        self.session = aiohttp.ClientSession(
            timeout=timeout,
            headers=headers
        )
        
        logger.info("SESSION_INIT: Created session without proxy")
        return self.session



    async def get_enhanced_session(self):
        """è·å–ç»ˆæå¢å¼ºä¼šè¯ - åŠ¨æ€IP + User-Agent ç»„åˆæ‹³"""
        if self.use_dynamic_ip and self.dynamic_ip_initialized:
            # === ç»ˆæç»„åˆæ¨¡å¼ï¼šåŠ¨æ€IP + User-Agent ===
            ip = self.get_current_ip()
            if ip:
                # åˆ›å»ºå¸¦ä»£ç†çš„å¢å¼ºä¼šè¯
                headers = self.bypass_enhancer.generate_realistic_headers()
                
                connector = aiohttp.TCPConnector(
                    ssl=False,
                    limit=100,
                    ttl_dns_cache=300,
                    use_dns_cache=True,
                )
                
                timeout = aiohttp.ClientTimeout(total=30, connect=10)
                proxy_url = f'http://{ip}'
                
                session = aiohttp.ClientSession(
                    connector=connector,
                    timeout=timeout,
                    headers=headers
                )
                
                # è®¾ç½®ä»£ç†
                session._proxy = proxy_url
                return session
            else:
                logger.warning("[!] åŠ¨æ€IPè·å–å¤±è´¥ï¼Œå›é€€åˆ°User-Agentç»•è¿‡æ¨¡å¼")
                if DYNAMIC_IP_AVAILABLE:
                    force_switch_ip()  # å¼ºåˆ¶åˆ‡æ¢IP
        
        # ä½¿ç”¨User-Agentç»•è¿‡æ¨¡å¼
        if self.use_user_agent:
            return await self.bypass_enhancer.create_enhanced_session()
        else:
            # åŸºç¡€ä¼šè¯
            timeout = aiohttp.ClientTimeout(total=self.config.request_timeout)
            return aiohttp.ClientSession(timeout=timeout)
    
    def get_current_ip(self):
        """è·å–å½“å‰åŠ¨æ€IP"""
        try:
            # ç›´æ¥è°ƒç”¨å…¨å±€IPæ± 
            if DYNAMIC_IP_AVAILABLE and _global_ip_pool:
                if hasattr(_global_ip_pool, 'working_ips') and _global_ip_pool.working_ips:
                    return _global_ip_pool.get_random_ip()
        except:
            pass
        return None
    
    def _print_bypass_stats(self):
        """è¾“å‡ºç»•è¿‡å¢å¼ºç»Ÿè®¡"""
        logger.info("=== ç»•è¿‡å¢å¼ºç»Ÿè®¡ ===")
        
        if self.use_dynamic_ip and self.dynamic_ip_initialized:
            try:
                if DYNAMIC_IP_AVAILABLE:
                    ip_stats = get_ip_stats()
                    logger.info(f"åŠ¨æ€IPæ± : {ip_stats.get('working_count', 0)} ä¸ªæœ‰æ•ˆIP")
                    logger.info(f"IPåˆ‡æ¢æ¬¡æ•°: {ip_stats.get('switch_count', 0)}")
            except:
                logger.info("åŠ¨æ€IPæ± : ç»Ÿè®¡è·å–å¤±è´¥")
        elif self.use_dynamic_ip:
            logger.info("åŠ¨æ€IPæ± : åˆå§‹åŒ–å¤±è´¥ï¼Œå·²ç¦ç”¨")
        
        if self.use_user_agent:
            stats = self.bypass_enhancer.bypass_stats
            logger.info(f"User-Agentè½®æ¢: {stats['ua_rotations']} æ¬¡")
            logger.info(f"è¯·æ±‚å¤´å˜ä½“: {stats['header_variations']} æ¬¡")
            
            # æ˜¾ç¤ºå½“å‰User-Agentä¿¡æ¯
            if self.bypass_enhancer.ua_manager:
                try:
                    ua_info = self.bypass_enhancer.ua_manager.get_user_agent_info()
                    if ua_info:
                        logger.info(f"å½“å‰UA: {ua_info.get('browser', 'Unknown')} {ua_info.get('version', '')} "
                                   f"({ua_info.get('os', 'Unknown')} {ua_info.get('device', 'Desktop')})")
                except:
                    logger.info("å½“å‰UA: ä¿¡æ¯è·å–å¤±è´¥")
        else:
            logger.info("User-Agentè½®æ¢: æœªå¯ç”¨")

    async def _cleanup_session(self):
        """æ¸…ç†sessionèµ„æº"""
        if self.session and not self.session.closed:
            await self.session.close()
        
        if self.auth_manager:
            try:
                await self.auth_manager.cleanup()
            except Exception as e:
                logger.debug(f"è®¤è¯ç®¡ç†å™¨æ¸…ç†å¼‚å¸¸: {e}")
           
    async def run(self) -> ScanResult:
        """ğŸš€ å²è¯—çº§ä¸»æ‰§è¡Œå‡½æ•° - é“¾å¼è¿½è¸ªèµ„äº§æ˜ å°„ + è®¤è¯æ”¯æŒ"""
        
        # å¦‚æœå¯ç”¨é“¾å¼è¿½è¸ªï¼Œä½¿ç”¨å²è¯—çº§å¾ªç¯æ¨¡å¼
        if self.enable_chain_tracking and self.chain_tracking_manager:
            return await self._run_chain_tracking_mode()
        else:
            # ä¼ ç»Ÿå•ç‚¹æ‰«ææ¨¡å¼
            return await self._run_single_target_mode()
    
    async def _run_chain_tracking_mode(self) -> ScanResult:
        """ğŸš€ å²è¯—çº§é“¾å¼è¿½è¸ªæ‰«ææ¨¡å¼"""
        logger.info("="*80)
        logger.info("ğŸš€ å²è¯—çº§é“¾å¼è¿½è¸ªèµ„äº§æ˜ å°„å¯åŠ¨ï¼")
        logger.info("ğŸ“¡ è‡ªåŠ¨å‘ç°å’Œæ‰«ææ•´ä¸ªèµ„äº§ç½‘ç»œ")
        logger.info("="*80)
        
        chain_start_time = time.time()
        
        # æ˜¾ç¤ºç»„ä»¶çŠ¶æ€
        self._display_component_status()
        
        # é“¾å¼æ‰«æä¸»å¾ªç¯ - æ‰¹é‡å¹¶å‘ä¼˜åŒ–
        scan_round = 0
        total_scan_result = ScanResult()
        concurrent_limit = 3  # æœ€å¤§å¹¶å‘æ‰«ææ•°
        
        try:
            while self.chain_tracking_manager.has_more_targets():
                scan_round += 1
                
                # ğŸš€ æ‰¹é‡è·å–æ‰«æç›®æ ‡ï¼ˆ2-3ä¸ªå¹¶å‘ï¼‰
                batch_targets = []
                for _ in range(concurrent_limit):
                    next_target = self.chain_tracking_manager.get_next_scan_target()
                    if next_target:
                        batch_targets.append(next_target)
                    else:
                        break
                
                if not batch_targets:
                    break
                
                # ç»Ÿè®¡å¹¶å‘æ‰¹æ¬¡
                if len(batch_targets) > 1:
                    self.chain_tracking_manager.chain_stats['concurrent_batches'] += 1
                    self.chain_tracking_manager.chain_stats['total_concurrent_scans'] += len(batch_targets)
                
                # æ˜¾ç¤ºæ‰¹æ¬¡ä¿¡æ¯
                if len(batch_targets) == 1:
                    logger.info(f"\nğŸ¯ [ç¬¬{scan_round}è½®] å•ç›®æ ‡æ‰«æ: {batch_targets[0].domain}")
                else:
                    logger.info(f"\nğŸš€ [ç¬¬{scan_round}è½®] å¹¶å‘æ‰«æ {len(batch_targets)} ä¸ªç›®æ ‡:")
                    for target in batch_targets:
                        logger.info(f"   ğŸ¯ {target.domain} (æ¥æº: {target.source_domain}, æ–¹å¼: {target.discovery_method}, æ·±åº¦: {target.discovery_depth})")
                
                # ğŸš€ å¹¶å‘æ‰§è¡Œæ‰«æä»»åŠ¡
                scan_tasks = []
                for target in batch_targets:
                    async def scan_single_target(target_info):
                        # ä¸´æ—¶è®¾ç½®ç›®æ ‡åŸŸå
                        original_target = self.target
                        self.target = target_info.domain
                        try:
                            result = await self._run_single_target_mode(suppress_logs=True)
                            return target_info, result
                        finally:
                            self.target = original_target
                    
                    scan_tasks.append(scan_single_target(target))
                
                # ç­‰å¾…æ‰€æœ‰å¹¶å‘æ‰«æå®Œæˆ
                batch_results = await asyncio.gather(*scan_tasks, return_exceptions=True)
                
                # å¤„ç†æ‰¹æ¬¡æ‰«æç»“æœ
                for i, result in enumerate(batch_results):
                    if isinstance(result, Exception):
                        logger.error(f"   âŒ {batch_targets[i].domain} æ‰«æå¤±è´¥: {result}")
                        continue
                    
                    target_info, scan_result = result
                    
                    # ğŸ” ä»æ‰«æç»“æœä¸­æå–æ–°å‘ç°çš„åŸŸå
                    await self._extract_domains_from_scan_result(scan_result, target_info)
                    
                    # æ ‡è®°å½“å‰åŸŸåå·²æ‰«æ
                    self.chain_tracking_manager.mark_domain_scanned(target_info.domain, scan_result)
                    
                    # åˆå¹¶ç»“æœåˆ°æ€»ç»“æœ
                    self._merge_scan_results(total_scan_result, scan_result)
                    
                    logger.info(f"   âœ… {target_info.domain} æ‰«æå®Œæˆ")
                
                # æ‰¹æ¬¡æ‰«æé—´éš”æ§åˆ¶
                if self.chain_tracking_manager.config.scan_interval > 0:
                    await asyncio.sleep(self.chain_tracking_manager.config.scan_interval)
            
            # é“¾å¼è¿½è¸ªå®Œæˆï¼Œç”Ÿæˆå²è¯—çº§æ€»æŠ¥å‘Š
            await self._generate_chain_tracking_report(total_scan_result, chain_start_time)
            
            return total_scan_result
            
        except Exception as e:
            logger.error(f"é“¾å¼è¿½è¸ªæ‰«æå¼‚å¸¸: {e}")
            return total_scan_result
        finally:
            await self._cleanup_session()
    
    async def _run_single_target_mode(self, suppress_logs: bool = False) -> ScanResult:
        """ä¼ ç»Ÿå•ç›®æ ‡æ‰«ææ¨¡å¼"""
        if not suppress_logs:
            logger.info(f"å¼€å§‹æ‰«æç›®æ ‡: {self.target}")
            logger.info(f"æ‰«ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            self._display_component_status()
        
        try:
            # åˆå§‹åŒ–åŠ¨æ€IPæ± 
            if self.use_dynamic_ip:
                logger.info("[+] åˆå§‹åŒ–åŠ¨æ€IPæ± ...")
                try:
                    if await init_ip_pool():
                        self.dynamic_ip_initialized = True
                        logger.info("[+] åŠ¨æ€IPæ± åˆå§‹åŒ–æˆåŠŸ!")
                    else:
                        logger.warning("[!] åŠ¨æ€IPæ± åˆå§‹åŒ–å¤±è´¥ï¼Œå›é€€åˆ°User-Agentæ¨¡å¼")
                        self.use_dynamic_ip = False
                except Exception as e:
                    logger.warning(f"[!] åŠ¨æ€IPæ± åˆå§‹åŒ–å¼‚å¸¸: {e}")
                    self.use_dynamic_ip = False
            
            # åˆå§‹åŒ–ç»„ä»¶
            await self._create_session()
            
            # åˆå§‹åŒ–è®¤è¯ç®¡ç†å™¨
            if self.auth_manager:
                await self.auth_manager.initialize()
                logger.info("è®¤è¯ è®¤è¯ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ - å‡†å¤‡è®¿é—®è®¤è¯åå†…éƒ¨èµ„äº§")
                
                # æ˜¾ç¤ºè®¤è¯ç»Ÿè®¡
                auth_stats = self.auth_manager.get_auth_stats()
                auth_type = auth_stats.get('current_auth_type', 'unknown')
                logger.info(f"è®¤è¯ è®¤è¯ç±»å‹: {auth_type}")
            
            # é¦–å…ˆæ£€æµ‹ä¸»åŸŸååè®®
            main_protocol = await self.check_protocol(self.target)
            if not main_protocol:
                logger.error(f"æ— æ³•è¿æ¥åˆ°ä¸»åŸŸå: {self.target}")
                return self.results
            
            logger.info(f"ä¸»åŸŸååè®®: {main_protocol}://{self.target}")
            
            # åˆå§‹åŒ– WAF Defender
            await self._initialize_waf_defender(main_protocol)
            
            # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰æ‰«æä»»åŠ¡
            logger.info("å¯åŠ¨å¹¶è¡Œæ‰«æä»»åŠ¡...")
            
            # ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€æ‰«æ
            basic_tasks = [
                self.subdomain_enum(),
                self.crawl_site(),
                self.tech_fingerprint()
            ]
            
            basic_results = await asyncio.gather(*basic_tasks, return_exceptions=True)
            
            # æ™ºèƒ½è·¯å¾„å‘ç°ï¼ˆåŸºäºå·²å‘ç°çš„æŠ€æœ¯æ ˆï¼‰
            await self.smart_path_discovery()
            
            # ç¬¬äºŒé˜¶æ®µï¼šæ·±åº¦æ‰«æ
            deep_tasks = [
                self.find_admin_panels(),
                self.scan_key_files(),
                self.extract_js_data_enhanced(),
                self.deep_tech_fingerprint(),
                self.medical_system_detection()
            ]
            
            tasks = basic_tasks + deep_tasks
            
            # ä½¿ç”¨ gather å¹¶å¤„ç†å¼‚å¸¸
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # æ£€æŸ¥ä»»åŠ¡æ‰§è¡Œç»“æœ
            for i, result in enumerate(results):
                task_names = ['å­åŸŸåæšä¸¾', 'ç½‘ç«™çˆ¬å–', 'åå°æ‰«æ', 'æ–‡ä»¶æ‰«æ', 'JSåˆ†æ', 'æŠ€æœ¯è¯†åˆ«']
                if isinstance(result, Exception):
                    logger.error(f"{task_names[i]}ä»»åŠ¡å¤±è´¥: {type(result).__name__}: {result}")
                else:
                    logger.info(f"{task_names[i]}ä»»åŠ¡å®Œæˆ")
            
            # å†…å­˜ä¼˜åŒ–
            logger.info("æ‰§è¡Œå†…å­˜ä¼˜åŒ–...")
            self._optimize_memory()
            
            # ç”ŸæˆæŠ¥å‘Šå’Œæ”»å‡»é¢åˆ†æ
            attack_surface = self.analyze_attack_surface()
            self.generate_report()
            
            scan_time = time.time() - self.start_time
            
            # è¾“å‡ºæ€§èƒ½ç»Ÿè®¡
            perf_stats = self.get_performance_stats()
            logger.info(f"æ‰«æå®Œæˆï¼Œè€—æ—¶: {scan_time:.2f}ç§’")
            logger.info(f"æ€§èƒ½ç»Ÿè®¡: è¯·æ±‚ {perf_stats['requests_made']}, æˆåŠŸç‡ {perf_stats['success_rate']:.2%}")
            
            # å™ªéŸ³è¿‡æ»¤ç»Ÿè®¡ï¼ˆä¸gitå·¥å…·ä¸€è‡´ï¼‰
            if NOISE_FILTER_AVAILABLE and self.noise_stats['total_checked'] > 0:
                noise_ratio = self.noise_stats['noise_filtered'] / max(self.noise_stats['total_checked'], 1)
                logger.info(f"å™ªéŸ³è¿‡æ»¤ç»Ÿè®¡:")
                logger.info(f"    - æ€»æ£€æŸ¥: {self.noise_stats['total_checked']}")
                logger.info(f"    - å™ªéŸ³è¿‡æ»¤: {self.noise_stats['noise_filtered']}")
                logger.info(f"    - æœ‰ä»·å€¼ä¿ç•™: {self.noise_stats['valuable_kept']}")
                logger.info(f"    - å™ªéŸ³ç‡: {noise_ratio:.1%}")
                
                if noise_ratio > 0.5:
                    logger.info("    - æ•ˆæœ: æˆåŠŸé¿å…äº†ä¸¥é‡çš„'å‚»é€¼å…´å¥‹' - å¤§é‡ç¬¬ä¸‰æ–¹å™ªéŸ³è¢«è¿‡æ»¤")
                elif self.noise_stats['noise_filtered'] > 0:
                    logger.info("    - æ•ˆæœ: æ™ºèƒ½è¿‡æ»¤ç”Ÿæ•ˆï¼Œä¿æŒé«˜è´¨é‡ç»“æœ")
                else:
                    logger.info("    - æ•ˆæœ: ç›®æ ‡è´¨é‡è‰¯å¥½ï¼Œæ— éœ€è¿‡æ»¤")
            elif NOISE_FILTER_AVAILABLE:
                logger.info("å™ªéŸ³è¿‡æ»¤: å·²å¯ç”¨ï¼Œä½†æœªæ£€æµ‹åˆ°éœ€è¦è¿‡æ»¤çš„å†…å®¹")
            else:
                logger.info("å™ªéŸ³è¿‡æ»¤: æœªå¯ç”¨ - å»ºè®®å¯ç”¨ä»¥æé«˜æ‰«æè´¨é‡")
            
            if perf_stats.get('avg_request_time'):
                logger.info(f"å¹³å‡è¯·æ±‚æ—¶é—´: {perf_stats['avg_request_time']:.3f}ç§’")
            if perf_stats.get('proxy_switches', 0) > 0:
                logger.info(f"ä»£ç†åˆ‡æ¢: {perf_stats['proxy_switches']} æ¬¡")
            
            # è¾“å‡ºç»•è¿‡å¢å¼ºç»Ÿè®¡
            if self.use_dynamic_ip or self.use_user_agent:
                self._print_bypass_stats()
            
            # è¾“å‡ºæ”»å‡»é¢åˆ†æå»ºè®®
            self._print_attack_recommendations(attack_surface)
            
            return self.results
            
        except Exception as e:
            logger.error(f"æ‰«æè¿‡ç¨‹ä¸­å‘ç”Ÿè‡´å‘½é”™è¯¯: {type(e).__name__}: {e}")
            return self.results
        finally:
            # æ¸…ç†èµ„æº
            await self._cleanup_session()
    
    def _display_component_status(self):
        """æ˜¾ç¤ºç»„ä»¶çŠ¶æ€"""
        waf_status = "OK" if WAF_DEFENDER_AVAILABLE else "é”™è¯¯"
        noise_status = "OK" if NOISE_FILTER_AVAILABLE else "é”™è¯¯"
        auth_status = "è®¤è¯OK" if self.auth_manager else "  "
        
        logger.info(f"[*] WAFé˜²æŠ¤: {waf_status}")
        logger.info(f"[*] å™ªéŸ³è¿‡æ»¤: {noise_status}")
        logger.info(f"[*] è®¤è¯ç®¡ç†: {auth_status}")
        
        # æ˜¾ç¤ºç»•è¿‡æ¨¡å¼
        if self.use_dynamic_ip:
            logger.info(f"[*] ç»•è¿‡æ¨¡å¼: ç»ˆæç»„åˆæ‹³ (åŠ¨æ€IP + User-Agent + æ™ºèƒ½è¯·æ±‚å¤´)")
        elif self.use_user_agent:
            logger.info(f"[*] ç»•è¿‡æ¨¡å¼: User-Agentè½®æ¢ + æ™ºèƒ½è¯·æ±‚å¤´")
        else:
            logger.info(f"[*] ç»•è¿‡æ¨¡å¼: åŸºç¡€æ¨¡å¼")
    
    async def _extract_domains_from_scan_result(self, scan_result: ScanResult, current_target: DomainDiscoveryResult):
        """ğŸ” ä»æ‰«æç»“æœä¸­æå–æ–°å‘ç°çš„åŸŸå"""
        discovered_count = 0
        
        # 1. ä»å­åŸŸåæšä¸¾ç»“æœä¸­æå–
        for subdomain in scan_result.subdomains:
            domain = subdomain.get('subdomain', '')
            if self.chain_tracking_manager.add_discovered_domain(
                domain=domain,
                source_domain=current_target.domain,
                discovery_method="subdomain_enum",
                depth=current_target.discovery_depth + 1
            ):
                discovered_count += 1
        
        # 2. ä»JSæ–‡ä»¶ä¸­æå–åŸŸå
        for endpoint in scan_result.endpoints:
            if endpoint.get('source') == 'js_analysis':
                # æå–URLä¸­çš„åŸŸå
                url = endpoint.get('url', '')
                if url:
                    domain = self._extract_domain_from_url(url)
                    if domain and self.chain_tracking_manager.add_discovered_domain(
                        domain=domain,
                        source_domain=current_target.domain,
                        discovery_method="js_analysis",
                        depth=current_target.discovery_depth + 1
                    ):
                        discovered_count += 1
        
        # 3. ä»APIå“åº”ä¸­æå–å†…éƒ¨åŸŸå
        for api_route in scan_result.api_routes:
            # è§£æAPIå“åº”ä¸­å¯èƒ½åŒ…å«çš„å†…éƒ¨åŸŸå
            description = api_route.get('description', '')
            potential_domains = self._extract_domains_from_text(description)
            for domain in potential_domains:
                if self.chain_tracking_manager.add_discovered_domain(
                    domain=domain,
                    source_domain=current_target.domain,
                    discovery_method="api_response",
                    depth=current_target.discovery_depth + 1
                ):
                    discovered_count += 1
        
        # 4. ä»SSLè¯ä¹¦ä¸­æå–åŸŸåï¼ˆå¦‚æœæœ‰WAF Defenderä¿¡æ¯ï¼‰
        if hasattr(self, 'waf_defender') and self.waf_defender:
            # è¿™é‡Œå¯ä»¥æ·»åŠ SSLè¯ä¹¦åŸŸåæå–é€»è¾‘
            pass
        
        # 5. ä»æŠ€æœ¯æ ˆä¿¡æ¯ä¸­æå–ç›¸å…³åŸŸå
        for tech in scan_result.technologies:
            tech_info = tech.get('details', '')
            potential_domains = self._extract_domains_from_text(tech_info)
            for domain in potential_domains:
                if self.chain_tracking_manager.add_discovered_domain(
                    domain=domain,
                    source_domain=current_target.domain,
                    discovery_method="tech_fingerprint",
                    depth=current_target.discovery_depth + 1
                ):
                    discovered_count += 1
        
        if discovered_count > 0:
            logger.info(f"    ğŸ” æ–°å‘ç° {discovered_count} ä¸ªåŸŸååŠ å…¥æ‰«æé˜Ÿåˆ—")
        else:
            logger.info(f"    ğŸ” æœªå‘ç°æ–°çš„ç›®æ ‡åŸŸå")
    
    def _extract_domain_from_url(self, url: str) -> str:
        """ä»URLä¸­æå–åŸŸå"""
        try:
            parsed = urlparse(url if url.startswith(('http://', 'https://')) else f'http://{url}')
            domain = parsed.netloc
            # ç§»é™¤ç«¯å£å·
            if ':' in domain:
                domain = domain.split(':')[0]
            return domain.lower()
        except:
            return ""
    
    def _extract_domains_from_text(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–æ½œåœ¨çš„åŸŸå"""
        if not text:
            return []
        
        # åŸŸåæ­£åˆ™è¡¨è¾¾å¼
        domain_pattern = r'\b(?:[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?\.)+[a-zA-Z]{2,}\b'
        potential_domains = re.findall(domain_pattern, text)
        
        # è¿‡æ»¤æ˜æ˜¾çš„åƒåœ¾åŸŸå
        valid_domains = []
        for domain in potential_domains:
            domain = domain.lower()
            # æ’é™¤æ˜æ˜¾çš„æ–‡ä»¶æ‰©å±•åç­‰
            if not domain.endswith(('.jpg', '.png', '.gif', '.css', '.js', '.pdf', '.zip')):
                valid_domains.append(domain)
        
        return valid_domains
    
    def _merge_scan_results(self, total_result: ScanResult, current_result: ScanResult):
        """åˆå¹¶æ‰«æç»“æœåˆ°æ€»ç»“æœ"""
        total_result.endpoints.extend(current_result.endpoints)
        total_result.forms.extend(current_result.forms)
        total_result.api_routes.extend(current_result.api_routes)
        total_result.admin_panels.extend(current_result.admin_panels)
        total_result.files.extend(current_result.files)
        total_result.subdomains.extend(current_result.subdomains)
        total_result.technologies.extend(current_result.technologies)
    
    async def _generate_chain_tracking_report(self, total_result: ScanResult, chain_start_time: float):
        """ğŸš€ ç”Ÿæˆå²è¯—çº§é“¾å¼è¿½è¸ªæ€»æŠ¥å‘Š"""
        chain_duration = time.time() - chain_start_time
        chain_summary = self.chain_tracking_manager.get_chain_summary()
        
        logger.info("\n" + "="*100)
        logger.info("ğŸš€ å²è¯—çº§é“¾å¼è¿½è¸ªå®Œæˆï¼èµ„äº§ç½‘ç»œå…¨æ™¯æŠ¥å‘Š")
        logger.info("="*100)
        
        # åŸºç¡€ç»Ÿè®¡
        logger.info(f"ğŸ“Š æ€»ä½“ç»Ÿè®¡:")
        logger.info(f"   ğŸ¯ åˆå§‹ç›®æ ‡: {chain_summary['initial_domain']}")
        logger.info(f"   ğŸ” å‘ç°åŸŸå: {chain_summary['total_discovered']} ä¸ª")
        logger.info(f"   âœ… æ‰«æå®Œæˆ: {chain_summary['total_scanned']} ä¸ª")
        logger.info(f"   â±ï¸  æ€»è€—æ—¶: {chain_duration:.2f} ç§’")
        logger.info(f"   âš¡ å¹³å‡æ‰«ææ—¶é—´: {chain_duration/max(chain_summary['total_scanned'], 1):.2f} ç§’/åŸŸå")
        
        # æ·±åº¦åˆ†å¸ƒ
        if chain_summary['depth_distribution']:
            logger.info(f"\nğŸ“Š å‘ç°æ·±åº¦åˆ†å¸ƒ:")
            for depth, count in sorted(chain_summary['depth_distribution'].items()):
                logger.info(f"   ğŸ“Š æ·±åº¦ {depth}: {count} ä¸ªåŸŸå")
        
        # å‘ç°æ–¹å¼ç»Ÿè®¡
        if chain_summary['discovery_methods']:
            logger.info(f"\nğŸ” å‘ç°æ–¹å¼ç»Ÿè®¡:")
            for method, count in sorted(chain_summary['discovery_methods'].items(), key=lambda x: x[1], reverse=True):
                logger.info(f"   ğŸ” {method}: {count} ä¸ªåŸŸå")
        
        # ğŸš€ ä¼˜åŒ–æ•ˆæœç»Ÿè®¡
        logger.info(f"\nğŸš€ ä¼˜åŒ–æ•ˆæœç»Ÿè®¡:")
        logger.info(f"   ğŸ”„ å¾ªç¯å¼•ç”¨é˜»æ­¢: {chain_summary.get('circular_references_blocked', 0)} æ¬¡")
        logger.info(f"   âš¡ å¹¶å‘æ‰¹æ¬¡: {chain_summary.get('concurrent_batches', 0)} æ¬¡")
        logger.info(f"   ğŸš€ å¹¶å‘æ‰«ææ€»æ•°: {chain_summary.get('total_concurrent_scans', 0)} ä¸ª")
        if chain_summary.get('concurrent_batches', 0) > 0:
            avg_concurrent = chain_summary.get('total_concurrent_scans', 0) / chain_summary.get('concurrent_batches', 1)
            logger.info(f"   ğŸ“Š å¹³å‡å¹¶å‘æ•°: {avg_concurrent:.1f} ä¸ª/æ‰¹æ¬¡")
            efficiency_gain = (chain_summary.get('total_concurrent_scans', 0) - chain_summary.get('concurrent_batches', 0)) / max(chain_summary['total_scanned'], 1) * 100
            logger.info(f"   âš¡ æ•ˆç‡æå‡: çº¦ {efficiency_gain:.1f}%")
        
        # é«˜é£é™©åŸŸå
        if chain_summary['high_risk_domains']:
            logger.info(f"\nâš ï¸  é«˜é£é™©åŸŸå ({len(chain_summary['high_risk_domains'])} ä¸ª):")
            for domain in chain_summary['high_risk_domains'][:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                logger.info(f"   âš ï¸  {domain}")
        
        # å‘ç°é“¾è·¯å›¾
        logger.info(f"\nğŸ”— å‘ç°é“¾è·¯å›¾:")
        discovery_chain = chain_summary['discovery_chain']
        for domain, info in list(discovery_chain.items())[:15]:  # æ˜¾ç¤ºå‰15ä¸ª
            if info['source'] == 'manual_input':
                logger.info(f"   ğŸ¯ {domain} (åˆå§‹ç›®æ ‡)")
            else:
                logger.info(f"   ğŸ”— {domain} â† {info['source']} (é€šè¿‡{info['method']}, æ·±åº¦{info['depth']}, é£é™©{info['risk_score']})")
        
        if len(discovery_chain) > 15:
            logger.info(f"   ... è¿˜æœ‰ {len(discovery_chain) - 15} ä¸ªåŸŸå")
        
        # æ±‡æ€»é«˜å±å‘ç°
        logger.info(f"\nğŸ¯ å…¨ç½‘ç»œé«˜å±å‘ç°æ±‡æ€»:")
        logger.info(f"   ğŸ¯ APIç«¯ç‚¹: {len(total_result.api_routes)} ä¸ª")
        logger.info(f"   ğŸ¯ ç®¡ç†é¢æ¿: {len(total_result.admin_panels)} ä¸ª")
        logger.info(f"   ğŸ¯ æ•æ„Ÿæ–‡ä»¶: {len(total_result.files)} ä¸ª")
        logger.info(f"   ğŸ¯ è¡¨å•æ¥å£: {len(total_result.forms)} ä¸ª")
        logger.info(f"   ğŸ¯ å­åŸŸå: {len(total_result.subdomains)} ä¸ª")
        logger.info(f"   ğŸ¯ æŠ€æœ¯æ ˆ: {len(total_result.technologies)} ä¸ª")
        
        # ç»•è¿‡å¢å¼ºç»Ÿè®¡
        if self.use_dynamic_ip or self.use_user_agent:
            self._print_bypass_stats()
        
        logger.info("="*100)
        logger.info("ğŸ‰ å²è¯—çº§é“¾å¼è¿½è¸ªä»»åŠ¡å®Œæˆï¼å®Œæ•´çš„èµ„äº§ç½‘ç»œæƒ…æŠ¥å·²è·å–ï¼")
        logger.info("="*100)

    async def subdomain_enum(self):
        """å­åŸŸåæšä¸¾ - å¢å¼ºç‰ˆ"""
        logger.info("å¼€å§‹å­åŸŸåæšä¸¾...")
        
        subdomains = set()
        
        # 1. ä½¿ç”¨å¤–éƒ¨å·¥å…·
        tools = [
            ("subfinder", f"subfinder -d {self.target} -silent"),
            ("amass", f"amass enum -passive -d {self.target} -silent")
        ]
        
        for tool_name, cmd in tools:
            try:
                logger.debug(f"è¿è¡Œå·¥å…·: {tool_name}")
                result = subprocess.run(
                    cmd.split(), 
                    capture_output=True, 
                    text=True, 
                    timeout=60
                )
                if result.returncode == 0:
                    found_domains = [d.strip() for d in result.stdout.strip().split('\n') if d.strip()]
                    subdomains.update(found_domains)
                    logger.info(f"{tool_name} å‘ç° {len(found_domains)} ä¸ªå­åŸŸå")
                else:
                    logger.warning(f"{tool_name} æ‰§è¡Œå¤±è´¥: {result.stderr}")
            except subprocess.TimeoutExpired:
                logger.warning(f"{tool_name} æ‰§è¡Œè¶…æ—¶")
            except FileNotFoundError:
                logger.debug(f"{tool_name} å·¥å…·æœªå®‰è£…")
            except Exception as e:
                logger.debug(f"{tool_name} æ‰§è¡Œå¼‚å¸¸: {type(e).__name__}: {e}")
        
        # 2. è¯ä¹¦é€æ˜åº¦æ—¥å¿—æŸ¥è¯¢
        if self.config.enable_crt_check:
            await self.query_crtsh(subdomains)
        
        # 3. DNS Zone Transferå°è¯•
        if self.config.enable_zone_transfer:
            await self.try_zone_transfer(subdomains)
        
        # 4. å†…éƒ¨åŸŸåçŒœæµ‹
        logger.debug("æ·»åŠ çŒœæµ‹çš„å†…éƒ¨åŸŸå...")
        for prefix in self.internal_prefixes:
            subdomains.add(f"{prefix}.{self.target}")
        
        # 5. éªŒè¯å­˜æ´»çš„å­åŸŸå
        await self._verify_subdomains(subdomains)
        
        logger.info(f"å­åŸŸåæšä¸¾å®Œæˆï¼Œå‘ç° {len(self.results.subdomains)} ä¸ªå­˜æ´»åŸŸå")
    
    async def _verify_subdomains(self, subdomains: Set[str]):
        """éªŒè¯å­åŸŸåå­˜æ´»çŠ¶æ€"""
        logger.info(f"éªŒè¯ {len(subdomains)} ä¸ªå­åŸŸå...")
        
        # åˆ›å»ºéªŒè¯ä»»åŠ¡
        tasks = []
        semaphore = asyncio.Semaphore(10)  # é™åˆ¶å¹¶å‘æ•°
        
        async def verify_single_subdomain(subdomain: str):
            async with semaphore:
                if not subdomain or subdomain == self.target:
                    return
                
                protocol = await self.check_protocol(subdomain)
                if protocol:
                    try:
                        url = f"{protocol}://{subdomain}"
                        response = await self.safe_request(url, timeout=self.config.subdomain_timeout)
                        if response and response.status < 500:
                            # ğŸŒŸ æ–°çš„æ ‘çŠ¶å­˜å‚¨æ–¹å¼
                            self.results.add_asset(
                                domain=subdomain,
                                asset_type="subdomain",
                                protocol=protocol,
                                status=response.status,
                                title=await self._extract_title(response)
                            )
                            logger.debug(f"å­˜æ´»å­åŸŸå: {protocol}://{subdomain} ({response.status})")
                    except Exception as e:
                        logger.debug(f"å­åŸŸåéªŒè¯å¤±è´¥ {subdomain}: {type(e).__name__}")
        
        # å¯åŠ¨æ‰€æœ‰éªŒè¯ä»»åŠ¡
        for subdomain in subdomains:
            tasks.append(verify_single_subdomain(subdomain.strip()))
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        await asyncio.gather(*tasks, return_exceptions=True)
    
    async def _extract_title(self, response: aiohttp.ClientResponse) -> str:
        """æå–é¡µé¢æ ‡é¢˜"""
        try:
            content = await response.text()
            title_match = re.search(r'<title[^>]*>([^<]+)</title>', content, re.I)
            if title_match:
                return title_match.group(1).strip()[:100]  # é™åˆ¶é•¿åº¦
        except Exception:
            pass
        return ""
    
    async def smart_path_discovery(self):
        """æ™ºèƒ½è·¯å¾„æ¢æµ‹ - æ ¹æ®å·²å‘ç°å†…å®¹åŠ¨æ€ç”Ÿæˆè·¯å¾„"""
        logger.info("æ™ºèƒ½è·¯å¾„å‘ç°...")
        
        additional_paths = []
        
        # WordPressç›¸å…³è·¯å¾„
        if self.is_wordpress:
            logger.info("æ£€æµ‹åˆ°WordPressï¼Œæ·»åŠ ä¸“é¡¹è·¯å¾„")
            wp_paths = [
                '/wp-json/wp/v2/users',
                '/wp-json/wp/v2/posts', 
                '/?rest_route=/wp/v2/users',
                '/wp-content/debug.log',
                '/wp-config.php.bak',
                '/wp-admin/admin-ajax.php',
                '/xmlrpc.php'
            ]
            additional_paths.extend(wp_paths)
        
        # åŒ»ç–—ç³»ç»Ÿç›¸å…³è·¯å¾„
        if self.is_medical:
            logger.info("æ£€æµ‹åˆ°åŒ»ç–—ç³»ç»Ÿï¼Œæ·»åŠ ä¸“é¡¹è·¯å¾„")
            medical_paths = [
                '/api/patients/export',
                '/api/appointments/list',
                '/medical/records/download',
                '/ris/studies',
                '/pacs/wado',
                '/hie/patient/search'
            ]
            additional_paths.extend(medical_paths)
        
        # æ ¹æ®æ£€æµ‹åˆ°çš„æŠ€æœ¯æ ˆæ·»åŠ è·¯å¾„
        for tech in self.results.technologies:
            tech_name = tech.get('name', '').lower()
            if 'react' in tech_name:
                additional_paths.extend(['/static/js/main.*.js.map', '/manifest.json'])
            elif 'vue' in tech_name:
                additional_paths.extend(['/js/app.*.js.map', '/js/chunk-vendors.*.js.map'])
            elif 'laravel' in tech_name:
                additional_paths.extend(['/api/user', '/telescope', '/horizon'])
        
        if additional_paths:
            self.jp_paths.extend(additional_paths)
            logger.info(f"æ·»åŠ äº† {len(additional_paths)} ä¸ªæ™ºèƒ½è·¯å¾„")
    
    async def check_path_cached(self, url: str) -> Optional[Dict]:
        """å¸¦ç¼“å­˜çš„è·¯å¾„æ£€æŸ¥ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        # æ£€æŸ¥ç¼“å­˜
        if url in self.cache:
            cache_entry = self.cache[url]
            if time.time() - cache_entry['time'] < self.cache_ttl:
                self.stats['cache_hits'] += 1
                logger.debug(f"ä½¿ç”¨ç¼“å­˜: {url}")
                return cache_entry['data']
        
        # ä½¿ç”¨å¹¶å‘æ§åˆ¶
        async with self.path_semaphore:
            result = await self._check_admin_path(url)
            
            # å­˜å…¥ç¼“å­˜
            self.cache[url] = {
                'data': result,
                'time': time.time()
            }
            
            return result
    
    def set_proxy_pool(self, proxy_pool):
        """è®¾ç½®ä»£ç†æ± ï¼ˆä¾›å¤–éƒ¨è°ƒç”¨ï¼‰"""
        self.proxy_pool = proxy_pool
        logger.info("ä»£ç†æ± å·²è®¾ç½®ï¼Œæ”¯æŒè‡ªåŠ¨åˆ‡æ¢ä»£ç†")
    
    async def setup_proxy_session(self, proxy_endpoint: str):
        """ä½¿ç”¨æŒ‡å®šä»£ç†è®¾ç½®ä¼šè¯ - æ”¯æŒSOCKS5"""
        if self.session:
            await self.session.close()
        
        ssl_context = ssl.create_default_context(cafile=certifi.where())
        ssl_context.check_hostname = False
        ssl_context.verify_mode = ssl.CERT_NONE
        
        try:
            if proxy_endpoint.startswith('socks5://'):
                # ä½¿ç”¨SOCKS5ä»£ç†
                import aiohttp_socks
                connector = aiohttp_socks.ProxyConnector.from_url(proxy_endpoint, ssl=ssl_context)
                logger.debug(f"PROXY_SESSION: Using SOCKS5 connector for {proxy_endpoint}")
            else:
                # ä½¿ç”¨HTTPä»£ç†
                connector = aiohttp.TCPConnector(
                    ssl=ssl_context,
                    limit=self.config.concurrent_limit,
                    limit_per_host=10,
                    ttl_dns_cache=300,
                    use_dns_cache=True
                )
                logger.debug(f"PROXY_SESSION: Using HTTP connector for {proxy_endpoint}")
            
            timeout = aiohttp.ClientTimeout(total=self.config.request_timeout)
            
            session_kwargs = {
                'connector': connector,
                'timeout': timeout,
                'headers': {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                    'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
                    'Accept-Encoding': 'gzip, deflate',
                    'Connection': 'keep-alive',
                }
            }
            
            # åªæœ‰HTTPä»£ç†æ‰éœ€è¦proxyå‚æ•°
            if not proxy_endpoint.startswith('socks5://'):
                session_kwargs['proxy'] = proxy_endpoint
            
            self.session = aiohttp.ClientSession(**session_kwargs)
            self.current_proxy = proxy_endpoint
            logger.info(f"PROXY_SESSION_SUCCESS: Set proxy {proxy_endpoint}")
            
        except ImportError:
            logger.error("PROXY_SESSION_ERROR: aiohttp_socks not available for SOCKS5 proxy")
            raise
        except Exception as e:
            logger.error(f"PROXY_SESSION_ERROR: Failed to setup proxy session: {e}")
            raise
    
    async def extract_js_data_enhanced(self):
        """å¢å¼ºçš„JSåˆ†æ - æå–æ›´å¤šæ•æ„Ÿä¿¡æ¯"""
        logger.info("å¼€å§‹å¢å¼ºJSåˆ†æ...")
        
        main_protocol = await self.check_protocol(self.target)
        if not main_protocol:
            return
        
        base_url = f"{main_protocol}://{self.target}"
        
        try:
            response = await self.safe_request(base_url)
            if not response:
                return
            
            html = await response.text()
            
            # æ‰¾å‡ºæ‰€æœ‰JSæ–‡ä»¶
            js_files = re.findall(r'<script[^>]*src=["\']([^"\']+\.js[^"\']*)', html)
            
            # é™åˆ¶åˆ†ææ–‡ä»¶æ•°é‡
            js_files = js_files[:self.config.max_js_files]
            
            # å¹¶å‘åˆ†æJSæ–‡ä»¶
            tasks = []
            for js_file in js_files:
                task = self._analyze_single_js_file(urljoin(base_url, js_file))
                tasks.append(task)
            
            await asyncio.gather(*tasks, return_exceptions=True)
            
            logger.info(f"JSåˆ†æå®Œæˆï¼Œåˆ†æäº† {len(js_files)} ä¸ªæ–‡ä»¶")
            
        except Exception as e:
            logger.debug(f"JSåˆ†æå¤±è´¥: {type(e).__name__}: {e}")
    
    async def _analyze_single_js_file(self, js_url: str):
        """åˆ†æå•ä¸ªJSæ–‡ä»¶"""
        async with self.js_semaphore:
            try:
                response = await self.safe_request(js_url, timeout=5)
                if not response:
                    return
                
                js_content = await response.text()
                
                # 1. æå–ç¡¬ç¼–ç å‡­æ®
                await self._extract_credentials(js_content, js_url)
                
                # 2. æå–å†…éƒ¨åŸŸå/IP
                await self._extract_internal_hosts(js_content, js_url)
                
                # 3. æå–APIç«¯ç‚¹
                await self._extract_api_endpoints(js_content, js_url)
                
                # 4. GraphQL schemaæå–
                await self._extract_graphql_info(js_content, js_url)
                
                # 5. æ£€æŸ¥Source Map
                await self._check_source_map(js_content, js_url)
                
            except Exception as e:
                logger.debug(f"JSæ–‡ä»¶åˆ†æå¤±è´¥ {js_url}: {type(e).__name__}")
    
    async def _extract_credentials(self, js_content: str, source: str):
        """æå–ç¡¬ç¼–ç å‡­æ®"""
        credential_patterns = [
            (r'["\'](?:api[_-]?key|apikey)["\']:\s*["\']([a-zA-Z0-9\-_]{20,})["\']', 'API Key'),
            (r'["\'](?:secret|token)["\']:\s*["\']([a-zA-Z0-9\-_]{20,})["\']', 'Secret/Token'),
            (r'(?:Bearer|Token)\s+([a-zA-Z0-9\-_\.]{20,})', 'Bearer Token'),
            (r'["\'](?:password|pwd)["\']:\s*["\']([^"\']{8,})["\']', 'Password'),
            (r'(?:access_token|accessToken)["\']:\s*["\']([^"\']{20,})["\']', 'Access Token')
        ]
        
        for pattern, cred_type in credential_patterns:
            matches = re.findall(pattern, js_content, re.I)
            for match in matches:
                if len(match) >= 20:  # è¿‡æ»¤çŸ­å€¼
                    # ğŸŒŸ æ–°çš„æ ‘çŠ¶å­˜å‚¨æ–¹å¼ï¼šä»æºURLæå–åŸŸå
                    domain = self._extract_domain_from_url(source)
                    if domain:
                        self.results.add_endpoint(
                            domain=domain,
                            path=f"/js_analysis/credential/{cred_type.lower().replace(' ', '_')}",
                            endpoint_data={
                                "credential_type": cred_type,
                                "credential_preview": f"{match[:30]}...",
                        "source": source,
                                "endpoint_type": "credential_leak",
                                "risk_level": "critical",
                                "discovery_method": "js_analysis"
                            }
                        )
    
    async def _extract_internal_hosts(self, js_content: str, source: str):
        """æå–å†…éƒ¨åŸŸåå’ŒIP"""
        internal_patterns = [
            (r'https?://([0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3})', 'Internal IP'),
            (r'https?://([a-zA-Z0-9\-]+\.internal)', 'Internal Domain'),
            (r'https?://([a-zA-Z0-9\-]+\.local)', 'Local Domain'),
            (r'https?://([a-zA-Z0-9\-]+\.intranet)', 'Intranet Domain'),
            (r'https?://([a-zA-Z0-9\-]+\.corp)', 'Corp Domain')
        ]
        
        for pattern, host_type in internal_patterns:
            matches = re.findall(pattern, js_content, re.I)
            for match in matches:
                # ğŸŒŸ æ–°çš„æ ‘çŠ¶å­˜å‚¨æ–¹å¼ï¼šä»æºURLæå–åŸŸå
                domain = self._extract_domain_from_url(source)
                if domain:
                    self.results.add_endpoint(
                        domain=domain,
                        path=f"/js_analysis/internal_host/{match}",
                        endpoint_data={
                            "host_type": host_type,
                            "internal_host": match,
                    "source": source,
                            "endpoint_type": "internal_host_leak",
                            "risk_level": "high",
                            "discovery_method": "js_analysis"
                        }
                    )
    
    async def _extract_api_endpoints(self, js_content: str, source: str):
        """æå–APIç«¯ç‚¹ - æ™ºèƒ½å™ªéŸ³è¿‡æ»¤ç‰ˆ"""
        api_patterns = [
            r'["\']/(api/[^"\']+)',
            r'["\']/(graphql[^"\']*)',
            r'["\']/(rest/[^"\']+)',
            r'endpoint["\']:\s*["\']([^"\']+)',
            r'baseURL["\']:\s*["\']([^"\']+)',
            r'apiUrl["\']:\s*["\']([^"\']+)'
        ]
        
        total_found = 0
        noise_filtered = 0
        
        for pattern in api_patterns:
            matches = re.findall(pattern, js_content)
            for match in matches:
                total_found += 1
                self.noise_stats['total_checked'] += 1
                
                if len(match) <= 3:  # è¿‡æ»¤å¤ªçŸ­çš„åŒ¹é…
                    continue
                
                # æ™ºèƒ½å™ªéŸ³è¿‡æ»¤ - é˜²æ­¢"å‚»é€¼å…´å¥‹"
                if NOISE_FILTER_AVAILABLE:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ç¬¬ä¸‰æ–¹å™ªéŸ³
                    if is_third_party(match):
                        # ä½†å¦‚æœæœ‰å®‰å…¨ä»·å€¼ï¼Œä»ç„¶ä¿ç•™
                        if not has_security_value(match):
                            noise_filtered += 1
                            self.stats['noise_filtered'] += 1
                            self.noise_stats['noise_filtered'] += 1
                            logger.debug(f"è¿‡æ»¤ç¬¬ä¸‰æ–¹å™ªéŸ³: {match}")
                            continue
                
                # è¿™æ˜¯æœ‰ä»·å€¼çš„å‘ç°
                self.stats['valuable_findings'] += 1
                self.noise_stats['valuable_kept'] += 1
                
                # ğŸŒŸ æ–°çš„æ ‘çŠ¶å­˜å‚¨æ–¹å¼ï¼šä»æºURLæå–åŸŸå
                domain = self._extract_domain_from_url(source)
                if domain:
                    self.results.add_endpoint(
                        domain=domain,
                        path=match,
                        endpoint_data={
                    "source": source,
                            "endpoint_type": "api_endpoint",
                            "risk_level": "medium",
                            "filtered": False,
                            "discovery_method": "js_analysis"
                        }
                    )
        
        # æ—¥å¿—ç»Ÿè®¡
        if total_found > 0:
            noise_ratio = noise_filtered / total_found
            if noise_ratio > 0.5:
                logger.info(f"APIç«¯ç‚¹å™ªéŸ³è¿‡æ»¤: {noise_filtered}/{total_found} ({noise_ratio:.1%}) - é¿å…äº†å‚»é€¼å…´å¥‹")
            elif noise_filtered > 0:
                logger.debug(f"è¿‡æ»¤äº† {noise_filtered} ä¸ªç¬¬ä¸‰æ–¹APIç«¯ç‚¹")
    
    async def _extract_graphql_info(self, js_content: str, source: str):
        """æå–GraphQLä¿¡æ¯ - æ™ºèƒ½è¿‡æ»¤ç‰ˆ"""
        graphql_patterns = [
            (r'type\s+(\w+)\s*{[^}]+}', 'GraphQL Type'),
            (r'__schema', 'GraphQL Introspection'),
            (r'IntrospectionQuery', 'GraphQL Introspection Query'),
            (r'query\s+(\w+)\s*{', 'GraphQL Query'),
            (r'mutation\s+(\w+)\s*{', 'GraphQL Mutation')
        ]
        
        for pattern, gql_type in graphql_patterns:
            matches = re.findall(pattern, js_content)
            for match in matches:
                route_content = f"[GRAPHQL: {gql_type}] {match if isinstance(match, str) else 'found'}"
                
                # GraphQLå‘ç°é€šå¸¸å¾ˆæœ‰ä»·å€¼ï¼Œä½†ä¹Ÿè¦æ£€æŸ¥æ˜¯å¦æ¥è‡ªç¬¬ä¸‰æ–¹
                self.noise_stats['total_checked'] += 1
                
                if NOISE_FILTER_AVAILABLE and is_third_party(source):
                    # å¦‚æœæ¥æºæ˜¯ç¬¬ä¸‰æ–¹JSï¼Œä½†GraphQL schemaæ³„éœ²ä»æœ‰ä»·å€¼
                    if gql_type not in ['GraphQL Introspection', 'IntrospectionQuery']:
                        self.stats['noise_filtered'] += 1
                        self.noise_stats['noise_filtered'] += 1
                        logger.debug(f"è¿‡æ»¤ç¬¬ä¸‰æ–¹GraphQLå™ªéŸ³: {route_content}")
                        continue
                
                self.stats['valuable_findings'] += 1
                self.noise_stats['valuable_kept'] += 1
                
                # ğŸŒŸ æ–°çš„æ ‘çŠ¶å­˜å‚¨æ–¹å¼ï¼šä»æºURLæå–åŸŸå
                domain = self._extract_domain_from_url(source)
                if domain:
                    self.results.add_endpoint(
                        domain=domain,
                        path=f"/graphql/{gql_type.lower().replace(' ', '_')}",
                        endpoint_data={
                            "graphql_type": gql_type,
                            "content": route_content,
                    "source": source,
                            "endpoint_type": "graphql",
                            "risk_level": "high" if gql_type in ['GraphQL Introspection', 'IntrospectionQuery'] else "medium",
                            "discovery_method": "js_analysis"
                        }
                    )
    
    async def _check_source_map(self, js_content: str, js_url: str):
        """æ£€æŸ¥å¹¶ä¸‹è½½Source Map"""
        if '//# sourceMappingURL=' in js_content:
            map_match = re.search(r'//# sourceMappingURL=([^\s]+)', js_content)
            if map_match:
                map_url = urljoin(js_url, map_match.group(1))
                map_response = await self.safe_request(map_url, timeout=3)
                if map_response and map_response.status == 200:
                    map_content = await map_response.read()
                    self.results.files.append({
                        "file": "source_map",
                        "url": map_url,
                        "size": len(map_content),
                        "preview": "Source map - åŒ…å«åŸå§‹æºä»£ç !",
                        "risk_level": "high"
                    })
    
    async def deep_tech_fingerprint(self):
        """æ·±åº¦æŠ€æœ¯æ ˆè¯†åˆ«"""
        logger.info("æ·±åº¦æŠ€æœ¯æ ˆè¯†åˆ«...")
        
        main_protocol = await self.check_protocol(self.target)
        if not main_protocol:
            return
        
        response = await self.safe_request(f"{main_protocol}://{self.target}")
        if not response:
            return
        
        headers = dict(response.headers)
        html = await response.text()
        
        # 1. æ£€æµ‹å…·ä½“æ¡†æ¶ç‰ˆæœ¬
        await self._detect_framework_versions(html)
        
        # 2. æ£€æµ‹äº‘æœåŠ¡
        await self._detect_cloud_services(headers, html)
        
        # 3. æ£€æµ‹å®‰å…¨è®¾å¤‡/WAF
        await self._detect_waf_signatures(headers, html)
        
        logger.info(f"æŠ€æœ¯æ ˆè¯†åˆ«å®Œæˆ: {len(self.results.technologies)} é¡¹æŠ€æœ¯")
    
    async def _detect_framework_versions(self, html: str):
        """æ£€æµ‹æ¡†æ¶ç‰ˆæœ¬"""
        version_patterns = {
            'wordpress': r'wp-includes/js/wp-embed\.min\.js\?ver=([\d\.]+)',
            'jquery': r'jquery[/-]([\d\.]+)',
            'react': r'react@([\d\.]+)',
            'vue': r'vue@([\d\.]+)',
            'bootstrap': r'bootstrap[/-]([\d\.]+)'
        }
        
        for framework, pattern in version_patterns.items():
            matches = re.findall(pattern, html, re.I)
            for version in matches:
                # ğŸŒŸ æ–°çš„æ ‘çŠ¶å­˜å‚¨æ–¹å¼ï¼šæŠ€æœ¯æ ˆå…³è”åˆ°ä¸»åŸŸå
                self.results.add_technology(
                    domain=self.target,
                    tech_data={
                        "category": "framework_version",
                    "name": f"{framework.title()} v{version}",
                        "version": version,
                        "framework": framework.title(),
                        "has_vulnerabilities": self._check_framework_vulnerabilities(framework, version),
                        "discovery_method": "html_analysis"
                    }
                )
                
                # è®¾ç½®WordPressæ ‡å¿—
                if framework == 'wordpress':
                    self.is_wordpress = True
    
    def _check_framework_vulnerabilities(self, framework: str, version: str) -> bool:
        """æ£€æŸ¥æ¡†æ¶ç‰ˆæœ¬æ˜¯å¦å­˜åœ¨å·²çŸ¥æ¼æ´ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # è¿™é‡Œå¯ä»¥é›†æˆCVEæ•°æ®åº“ï¼Œç›®å‰ä½¿ç”¨ç®€å•çš„ç‰ˆæœ¬åˆ¤æ–­
        vulnerable_versions = {
            'wordpress': ['5.7', '5.6', '5.5'],  # ç¤ºä¾‹
            'drupal': ['8.9', '9.0', '9.1'],
            'joomla': ['3.9', '4.0']
        }
        
        if framework.lower() in vulnerable_versions:
            return version in vulnerable_versions[framework.lower()]
        return False
    
    async def _detect_cloud_services(self, headers: Dict, html: str):
        """æ£€æµ‹äº‘æœåŠ¡"""
        cloud_indicators = {
            'aws': {
                'headers': ['x-amz-cf-id', 'x-amz-request-id'],
                'content': ['amazonaws.com', 'cloudfront.net', 's3.amazonaws.com']
            },
            'gcp': {
                'headers': ['x-goog-trace'],
                'content': ['googleapis.com', 'googleusercontent.com', 'storage.googleapis.com']
            },
            'azure': {
                'headers': ['x-azure-ref'],
                'content': ['azurewebsites.net', 'blob.core.windows.net', 'azureedge.net']
            },
            'cloudflare': {
                'headers': ['cf-ray', 'cf-cache-status'],
                'content': ['cloudflare.com', 'cdnjs.cloudflare.com']
            }
        }
        
        for cloud, indicators in cloud_indicators.items():
            detected = False
            
            # æ£€æŸ¥headers
            for header in indicators['headers']:
                if header in headers:
                    detected = True
                    break
            
            # æ£€æŸ¥å†…å®¹
            if not detected:
                html_lower = html.lower()
                for content_indicator in indicators['content']:
                    if content_indicator in html_lower:
                        detected = True
                        break
            
            if detected:
                self.detected_cloud_services.append(cloud)
                # ğŸŒŸ æ–°çš„æ ‘çŠ¶å­˜å‚¨æ–¹å¼ï¼šäº‘æœåŠ¡æŠ€æœ¯æ ˆå…³è”åˆ°ä¸»åŸŸå
                self.results.add_technology(
                    domain=self.target,
                    tech_data={
                        "category": "cloud_service",
                    "name": cloud.upper(),
                        "detail": f"æ£€æµ‹åˆ°{cloud}äº‘æœåŠ¡ç‰¹å¾",
                        "service_type": "cloud_infrastructure",
                        "discovery_method": "header_content_analysis"
                    }
                )
    
    async def _detect_waf_signatures(self, headers: Dict, html: str):
        """æ£€æµ‹WAF/å®‰å…¨è®¾å¤‡"""
        waf_signatures = {
            'cloudflare': {
                'headers': ['cf-ray', '__cfduid', 'cf-cache-status'],
                'content': ['cloudflare', 'attention required']
            },
            'akamai': {
                'headers': ['akamai-cache-status'],
                'content': ['akamai', 'akamaihd.net']
            },
            'incapsula': {
                'headers': ['x-iinfo'],
                'content': ['incap_ses', 'visid_incap']
            },
            'sucuri': {
                'headers': ['x-sucuri-cache'],
                'content': ['sucuri', 'cloudproxy']
            }
        }
        
        for waf, signatures in waf_signatures.items():
            detected = False
            
            # æ£€æŸ¥headers
            for header in signatures['headers']:
                if header in headers:
                    detected = True
                    break
            
            # æ£€æŸ¥å†…å®¹
            if not detected:
                html_lower = html.lower()
                for content_sig in signatures['content']:
                    if content_sig in html_lower:
                        detected = True
                        break
            
            if detected:
                self.detected_waf.append(waf)
                # ğŸŒŸ æ–°çš„æ ‘çŠ¶å­˜å‚¨æ–¹å¼ï¼šWAFæŠ€æœ¯æ ˆå…³è”åˆ°ä¸»åŸŸå
                self.results.add_technology(
                    domain=self.target,
                    tech_data={
                        "category": "security_device",
                    "name": f"{waf.upper()} WAF",
                        "detail": f"æ£€æµ‹åˆ°{waf} Webåº”ç”¨é˜²ç«å¢™",
                        "security_type": "web_application_firewall",
                        "discovery_method": "waf_signature_analysis"
                    }
                )
                # æ ‡è®°èµ„äº§å—WAFä¿æŠ¤
                self.results.add_asset(self.target)
                self.results.assets[self.target]["waf_detected"] = True
    
    async def medical_system_detection(self):
        """åŒ»ç–—ç³»ç»Ÿä¸“é¡¹æ£€æµ‹"""
        logger.info("åŒ»ç–—ç³»ç»Ÿä¸“é¡¹æ£€æµ‹...")
        
        main_protocol = await self.check_protocol(self.target)
        if not main_protocol:
            return
        
        base_url = f"{main_protocol}://{self.target}"
        
        # FHIR APIç«¯ç‚¹æ£€æµ‹
        fhir_endpoints = [
            '/fhir/Patient',
            '/fhir/Appointment', 
            '/fhir/Medication',
            '/fhir/metadata',
            '/fhir/Observation'
        ]
        
        # DICOM/PACSç«¯ç‚¹æ£€æµ‹
        dicom_endpoints = [
            '/dicom-web/studies',
            '/pacs/studies',
            '/wado',
            '/dcm4chee',
            '/orthanc'
        ]
        
        # HL7/åŒ»ç–—é›†æˆç«¯ç‚¹
        hl7_endpoints = [
            '/hl7/messages',
            '/hie/patient/search',
            '/emr/api',
            '/his/api'
        ]
        
        all_medical_endpoints = fhir_endpoints + dicom_endpoints + hl7_endpoints
        
        medical_found = 0
        for endpoint in all_medical_endpoints:
            url = urljoin(base_url, endpoint)
            response = await self.safe_request(url, timeout=5)
            
            if response and response.status in [200, 401, 403]:
                medical_found += 1
                # ğŸŒŸ æ–°çš„æ ‘çŠ¶å­˜å‚¨æ–¹å¼ï¼šåŒ»ç–—ç«¯ç‚¹å…³è”åˆ°ä¸»åŸŸå
                path = f"/medical/{endpoint.split('/')[-1]}"
                self.results.add_endpoint(
                    domain=self.target,
                    path=path,
                    endpoint_data={
                    "url": url,
                    "status": response.status,
                    "risk_level": "critical" if response.status == 200 else "high",
                        "endpoint_type": "medical_system",
                        "description": "åŒ»ç–—ç³»ç»ŸAPIç«¯ç‚¹",
                        "discovery_method": "medical_system_scan"
                    }
                )
        
        if medical_found > 0:
            self.is_medical = True
            logger.info(f"æ£€æµ‹åˆ°åŒ»ç–—ç³»ç»Ÿï¼Œå‘ç° {medical_found} ä¸ªåŒ»ç–—ç›¸å…³ç«¯ç‚¹")
            
            # ğŸŒŸ æ–°çš„æ ‘çŠ¶å­˜å‚¨æ–¹å¼ï¼šåŒ»ç–—ç³»ç»ŸæŠ€æœ¯æ ˆå…³è”åˆ°ä¸»åŸŸå  
            self.results.add_technology(
                domain=self.target,
                tech_data={
                    "category": "medical_system",
                "name": "åŒ»ç–—ä¿¡æ¯ç³»ç»Ÿ",
                    "detail": f"å‘ç° {medical_found} ä¸ªåŒ»ç–—ç›¸å…³ç«¯ç‚¹",
                    "endpoint_count": medical_found,
                    "compliance_required": True,
                    "data_sensitivity": "high",
                    "discovery_method": "medical_system_scan"
                }
            )
    
    def prioritize_findings(self):
        """å¯¹å‘ç°è¿›è¡Œä¼˜å…ˆçº§æ’åº"""
        priority_keywords = {
            'critical': ['admin', 'config', 'backup', 'sql', 'database', 'patient', 'medical'],
            'high': ['api', 'auth', 'login', 'payment', 'fhir', 'dicom'],
            'medium': ['user', 'profile', 'search', 'appointment'],
            'low': ['static', 'assets', 'images', 'css', 'js']
        }
        
        # ä¸ºç®¡ç†é¢æ¿è®¾ç½®ä¼˜å…ˆçº§
        for finding in self.results.admin_panels:
            if 'priority' not in finding:  # é¿å…é‡å¤è®¾ç½®
                url_lower = finding['url'].lower()
                for level, keywords in priority_keywords.items():
                    if any(kw in url_lower for kw in keywords):
                        finding['priority'] = level
                        break
                else:
                    finding['priority'] = 'low'
        
        # ä¸ºAPIè·¯ç”±è®¾ç½®ä¼˜å…ˆçº§
        for api in self.results.api_routes:
            if 'priority' not in api:
                route_lower = api.get('route', '').lower()
                api_type = api.get('type', '')
                
                if api_type in ['credential', 'internal_host']:
                    api['priority'] = 'critical'
                elif api_type == 'graphql':
                    api['priority'] = 'high'
                else:
                    for level, keywords in priority_keywords.items():
                        if any(kw in route_lower for kw in keywords):
                            api['priority'] = level
                            break
                    else:
                        api['priority'] = 'medium'
    
    def analyze_attack_surface(self) -> Dict:
        """åˆ†ææ”»å‡»é¢ï¼Œä¸ºåç»­å·¥å…·æä¾›å»ºè®®"""
        logger.info("åˆ†ææ”»å‡»é¢...")
        
        # é¦–å…ˆå¯¹å‘ç°è¿›è¡Œä¼˜å…ˆçº§æ’åº
        self.prioritize_findings()
        
        recommendations = {
            'next_tools': [],
            'priority_targets': [],
            'techniques': [],
            'risk_assessment': 'low'
        }
        
        # æ ¹æ®å‘ç°æ¨èä¸‹ä¸€æ­¥å·¥å…·
        if self.results.files:
            sensitive_files = [f for f in self.results.files if 'map' in f.get('file', '') or '.env' in f.get('file', '')]
            if sensitive_files:
                recommendations['next_tools'].extend(['git_leak_extractor', 'backup_miner'])
        
        # GraphQLæ£€æµ‹
        graphql_apis = [api for api in self.results.api_routes if 'graphql' in api.get('route', '').lower()]
        if graphql_apis:
            recommendations['next_tools'].append('graphql_bomber')
            recommendations['techniques'].append('GraphQL introspection attack')
        
        # åŒ»ç–—ç³»ç»Ÿæ£€æµ‹
        if self.is_medical:
            recommendations['next_tools'].extend(['data_hunter', 'incremental_id_hunter'])
            recommendations['techniques'].extend(['FHIR API enumeration', 'Patient data mining'])
            recommendations['risk_assessment'] = 'critical'  # åŒ»ç–—æ•°æ®æœ€é«˜ä¼˜å…ˆçº§
        
        # WordPressæ£€æµ‹
        if self.is_wordpress:
            recommendations['next_tools'].extend(['backup_miner', 'data_hunter'])
            recommendations['techniques'].append('WordPress REST API exploitation')
        
        # ç¼“å­˜ç³»ç»Ÿæ£€æµ‹
        if any('redis' in tech.get('name', '').lower() for tech in self.results.technologies):
            recommendations['next_tools'].append('cache_layer_extractor')
        
        # äº‘æœåŠ¡æ£€æµ‹
        if self.detected_cloud_services:
            recommendations['next_tools'].append('cloud_native_attacker')
            recommendations['techniques'].append('Cloud metadata service attack')
        
        # WAFæ£€æµ‹
        if self.detected_waf:
            recommendations['techniques'].extend(['WAF bypass techniques', 'Rate limiting evasion'])
        
        # é«˜ä¼˜å…ˆçº§ç›®æ ‡è¯†åˆ«
        critical_findings = [
            f for f in self.results.admin_panels 
            if f.get('priority') == 'critical' or f.get('risk_level') == 'critical'
        ]
        
        high_priority_apis = [
            api for api in self.results.api_routes 
            if api.get('priority') in ['critical', 'high']
        ]
        
        recommendations['priority_targets'] = critical_findings + high_priority_apis
        
        # é£é™©è¯„ä¼°
        if len(critical_findings) > 3 or self.is_medical:
            recommendations['risk_assessment'] = 'critical'
        elif len(critical_findings) > 1 or len(high_priority_apis) > 5:
            recommendations['risk_assessment'] = 'high'
        elif len(self.results.admin_panels) > 5:
            recommendations['risk_assessment'] = 'medium'
        
        return recommendations
    
    def _print_attack_recommendations(self, attack_surface: Dict):
        """æ‰“å°æ”»å‡»é¢åˆ†æå»ºè®®"""
        logger.info("=== æ”»å‡»é¢åˆ†æå»ºè®® ===")
        
        risk = attack_surface['risk_assessment']
        risk_icons = {
            'critical': 'ğŸš¨',
            'high': ' ',
            'medium': ' ',
            'low': 'OK'
        }
        
        logger.info(f"{risk_icons.get(risk, ' ')} é£é™©è¯„ä¼°: {risk.upper()}")
        
        if attack_surface['next_tools']:
            logger.info(f"  æ¨èå·¥å…· ({len(attack_surface['next_tools'])}ä¸ª):")
            for tool in attack_surface['next_tools'][:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                logger.info(f"   - {tool}")
        
        if attack_surface['techniques']:
            logger.info(f"ç›®æ ‡ æ¨èæŠ€æœ¯ ({len(attack_surface['techniques'])}ä¸ª):")
            for technique in attack_surface['techniques'][:5]:
                logger.info(f"   - {technique}")
        
        if attack_surface['priority_targets']:
            logger.info(f"  ä¼˜å…ˆç›®æ ‡ ({len(attack_surface['priority_targets'])}ä¸ª):")
            for target in attack_surface['priority_targets'][:3]:
                if 'url' in target:
                    logger.info(f"   - {target['url']} [{target.get('priority', 'unknown')}]")
                else:
                    logger.info(f"   - {target.get('route', 'Unknown')} [{target.get('priority', 'unknown')}]")
        
        if self.detected_waf:
            logger.info(f"  æ£€æµ‹åˆ°WAF: {', '.join(self.detected_waf)} - å»ºè®®ä½¿ç”¨ç»•è¿‡æŠ€æœ¯")
    async def crawl_site(self):
        """ç½‘ç«™å†…å®¹çˆ¬å– - ä¼˜åŒ–ç‰ˆ"""
        logger.info("å¼€å§‹ç½‘ç«™ç»“æ„çˆ¬å–...")
        
        visited = set()
        main_protocol = await self.check_protocol(self.target) 
        if not main_protocol:
            logger.warning("æ— æ³•ç¡®å®šä¸»åŸŸååè®®ï¼Œè·³è¿‡çˆ¬å–")
            return
            
        base_url = f"{main_protocol}://{self.target}"
        to_visit = [base_url]
        
        crawl_count = 0
        max_pages = self.config.max_crawl_pages
        
        while to_visit and crawl_count < max_pages:
            url = to_visit.pop(0)
            if url in visited:
                continue
            
            visited.add(url)
            crawl_count += 1
            
            response = await self.safe_request(url)
            if response and response.status == 200:
                try:
                    text = await response.text()
                    logger.debug(f"çˆ¬å–é¡µé¢ {crawl_count}/{max_pages}: {url}")
                    
                    # æå–è¡¨å•ä¿¡æ¯
                    await self._extract_forms(url, text)
                    
                    # æå–æ–°é“¾æ¥
                    new_links = self._extract_links(url, text)
                    for link in new_links:
                        if link not in visited and len(to_visit) < 50:  # é™åˆ¶å¾…è®¿é—®é˜Ÿåˆ—
                            to_visit.append(link)
                    
                    # æ£€æµ‹ç‰¹æ®ŠåŠŸèƒ½é¡µé¢
                    await self._detect_special_pages(url, text)
                    
                except Exception as e:
                    logger.debug(f"é¡µé¢å†…å®¹è§£æå¤±è´¥ {url}: {type(e).__name__}: {e}")
            
            # é¿å…è¿‡å¿«è¯·æ±‚
            await asyncio.sleep(0.1)
        
        logger.info(f"ç½‘ç«™çˆ¬å–å®Œæˆï¼Œè®¿é—®äº† {crawl_count} ä¸ªé¡µé¢")
    
    async def _extract_forms(self, url: str, html: str):
        """æå–è¡¨å•ä¿¡æ¯"""
        forms = re.findall(r'<form[^>]*>(.*?)</form>', html, re.DOTALL | re.IGNORECASE)
        for form_content in forms:
            action_match = re.search(r'action=["\']([^"\']+)', form_content, re.IGNORECASE)
            method_match = re.search(r'method=["\']([^"\']+)', form_content, re.IGNORECASE)
            
            action = action_match.group(1) if action_match else url
            method = method_match.group(1).upper() if method_match else "GET"
            form_url = urljoin(url, action)
            
            # åˆ†æè¡¨å•è¾“å…¥
            inputs = re.findall(r'<input[^>]*>', form_content, re.IGNORECASE)
            textareas = re.findall(r'<textarea[^>]*>', form_content, re.IGNORECASE)
            selects = re.findall(r'<select[^>]*>', form_content, re.IGNORECASE)
            
            # æ£€æµ‹è¡¨å•ç±»å‹
            form_type = self._detect_form_type(form_content.lower())
            
            # ğŸŒŸ æ–°çš„æ ‘çŠ¶å­˜å‚¨æ–¹å¼ï¼šä»é¡µé¢URLæå–åŸŸå
            domain = self._extract_domain_from_url(url)
            if domain:
                self.results.add_form(
                    domain=domain,
                    form_data={
                "url": url,
                "action": form_url,
                "method": method,
                "inputs": len(inputs),
                "textareas": len(textareas),
                "selects": len(selects),
                "form_type": form_type,
                        "has_file_upload": 'type="file"' in form_content.lower(),
                        "risk_level": "high" if form_type in ["login", "admin", "upload"] else "medium",
                        "discovery_method": "form_analysis"
                    }
                )
    
    def _detect_form_type(self, form_content: str) -> str:
        """æ£€æµ‹è¡¨å•ç±»å‹"""
        type_keywords = {
            "login": ["login", "signin", "ãƒ­ã‚°ã‚¤ãƒ³", "password", "username"],
            "contact": ["contact", "ãŠå•ã„åˆã‚ã›", "message", "inquiry"],
            "reservation": ["reserve", "yoyaku", "äºˆç´„", "appointment", "booking"],
            "registration": ["register", "signup", "ä¼šå“¡ç™»éŒ²", "registration"],
            "search": ["search", "æ¤œç´¢", "query"],
            "payment": ["payment", "pay", "credit", "æ”¯æ‰•ã„", "æ±ºæ¸ˆ"]
        }
        
        for form_type, keywords in type_keywords.items():
            if any(keyword in form_content for keyword in keywords):
                return form_type
        
        return "unknown"
    
    def _extract_links(self, base_url: str, html: str) -> List[str]:
        """æå–é¡µé¢é“¾æ¥"""
        links = set()
        link_patterns = [
            r'href=["\']([^"\']+)',
            r'src=["\']([^"\']+\.(?:js|css))',
        ]
        
        for pattern in link_patterns:
            matches = re.findall(pattern, html, re.IGNORECASE)
            for link in matches:
                if link.startswith(('javascript:', 'mailto:', 'tel:', '#')):
                    continue
                
                full_url = urljoin(base_url, link)
                # åªæ”¶é›†åŒåŸŸåé“¾æ¥
                if self.target in full_url and not full_url.endswith(('.jpg', '.png', '.gif', '.pdf')):
                    links.add(full_url)
        
        return list(links)
    
    async def _detect_special_pages(self, url: str, html: str):
        """æ£€æµ‹ç‰¹æ®ŠåŠŸèƒ½é¡µé¢"""
        content_lower = html.lower()
        
        # ğŸŒŸ æ–°çš„æ ‘çŠ¶å­˜å‚¨æ–¹å¼ï¼šä»é¡µé¢URLæå–åŸŸå
        domain = self._extract_domain_from_url(url)
        if not domain:
            return
        
        # é¢„çº¦ç›¸å…³é¡µé¢
        reservation_keywords = ['äºˆç´„', 'yoyaku', 'reserve', 'booking', 'appointment']
        if any(keyword in content_lower for keyword in reservation_keywords):
            path = urlparse(url).path or "/"
            self.results.add_endpoint(
                domain=domain,
                path=path,
                endpoint_data={
                "url": url,
                    "endpoint_type": "reservation_related",
                    "keywords_found": [kw for kw in reservation_keywords if kw in content_lower],
                    "risk_level": "low",
                    "discovery_method": "content_analysis"
                }
            )
        
        # ç®¡ç†åå°ç›¸å…³
        admin_keywords = ['admin', 'dashboard', 'management', 'ç®¡ç†', 'control']
        if any(keyword in content_lower for keyword in admin_keywords):
            path = urlparse(url).path or "/"
            self.results.add_endpoint(
                domain=domain,
                path=path,
                endpoint_data={
                "url": url,
                    "endpoint_type": "admin_related",
                    "keywords_found": [kw for kw in admin_keywords if kw in content_lower],
                    "risk_level": "high",
                    "discovery_method": "content_analysis"
                }
            )
        
        # APIç›¸å…³é¡µé¢
        api_keywords = ['api', 'graphql', 'swagger', 'openapi']
        if any(keyword in content_lower for keyword in api_keywords):
            path = urlparse(url).path or "/"
            self.results.add_endpoint(
                domain=domain,
                path=path,
                endpoint_data={
                "url": url,
                    "endpoint_type": "api_related",
                    "keywords_found": [kw for kw in api_keywords if kw in content_lower],
                    "risk_level": "medium",
                    "discovery_method": "content_analysis"
                }
            )

    async def find_admin_panels(self):
        """æŸ¥æ‰¾ç®¡ç†åå° - ä¼˜åŒ–ç‰ˆ"""
        logger.info("å¼€å§‹åå°è·¯å¾„æ‰«æ...")
        
        main_protocol = await self.check_protocol(self.target)
        if not main_protocol:
            logger.warning("æ— æ³•ç¡®å®šåè®®ï¼Œè·³è¿‡åå°æ‰«æ")
            return
        
        base_url = f"{main_protocol}://{self.target}"
        
        # åˆ›å»ºæ‰«æä»»åŠ¡
        tasks = []
        semaphore = asyncio.Semaphore(15)  # é™åˆ¶å¹¶å‘æ•°
        
        async def check_single_path(path: str):
            url = urljoin(base_url, path)
            result = await self.check_path_cached(url)
            if result:
                # ğŸŒŸ æ–°çš„æ ‘çŠ¶å­˜å‚¨æ–¹å¼ï¼šå°†ç®¡ç†é¢æ¿å…³è”åˆ°ç‰¹å®šåŸŸå
                self.results.add_endpoint(
                    domain=self.target,
                    path=path,
                    endpoint_data={
                        **result,
                        "endpoint_type": "admin_panel",
                        "discovery_method": "path_scan"
                    }
                )
        
        # å¯åŠ¨æ‰€æœ‰æ‰«æä»»åŠ¡
        for path in self.jp_paths:
            tasks.append(check_single_path(path))
        
        # æ‰§è¡Œæ‰«æ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ç»Ÿè®¡ç»“æœ
        successful_scans = sum(1 for r in results if not isinstance(r, Exception))
        logger.info(f"åå°æ‰«æå®Œæˆ: å‘ç° {len(self.results.admin_panels)} ä¸ªæœ‰æ•ˆè·¯å¾„ (æ‰«æäº† {successful_scans} ä¸ªè·¯å¾„)")

    async def _check_admin_path(self, url: str) -> Optional[Dict]:
        """æ£€æŸ¥å•ä¸ªç®¡ç†è·¯å¾„"""
        response = await self.safe_request(url, allow_redirects=False, timeout=5)
        if not response:
            return None
        
        # æœ‰ä»·å€¼çš„çŠ¶æ€ç 
        if response.status in [200, 301, 302, 401, 403, 405]:
            try:
                content = await response.read()
                headers = dict(response.headers)
                
                # åˆ†æå“åº”ç‰¹å¾
                risk_level = self._assess_path_risk(response.status, headers, len(content))
                
                return {
                    "url": url,
                    "status": response.status,
                    "size": len(content),
                    "risk_level": risk_level,
                    "server": headers.get('Server', ''),
                    "content_type": headers.get('Content-Type', ''),
                    "location": headers.get('Location', '') if response.status in [301, 302] else ''
                }
            except Exception as e:
                logger.debug(f"è·¯å¾„æ£€æŸ¥å¤±è´¥ {url}: {type(e).__name__}")
        
        return None
    
    def _assess_path_risk(self, status: int, headers: Dict, content_size: int) -> str:
        """è¯„ä¼°è·¯å¾„é£é™©ç­‰çº§"""
        # 200 çŠ¶æ€çš„å¤§å“åº”é€šå¸¸æ˜¯çœŸå®é¡µé¢
        if status == 200 and content_size > 1000:
            return "high"
        
        # 401/403 è¡¨ç¤ºå­˜åœ¨ä½†éœ€è¦è®¤è¯
        if status in [401, 403]:
            return "medium"
        
        # é‡å®šå‘ä¹Ÿå€¼å¾—å…³æ³¨
        if status in [301, 302]:
            return "medium"
        
        # 405 Method Not Allowed è¡¨ç¤ºç«¯ç‚¹å­˜åœ¨
        if status == 405:
            return "low"
        
        return "low"

    async def scan_key_files(self):
        """æ‰«æå…³é”®æ–‡ä»¶ - ä¼˜åŒ–ç‰ˆ"""
        logger.info("å¼€å§‹æ•æ„Ÿæ–‡ä»¶æ‰«æ...")
        
        main_protocol = await self.check_protocol(self.target)
        if not main_protocol:
            logger.warning("æ— æ³•ç¡®å®šåè®®ï¼Œè·³è¿‡æ–‡ä»¶æ‰«æ")
            return
        
        base_url = f"{main_protocol}://{self.target}"
        
        # å¹¶å‘æ‰«ææ–‡ä»¶
        tasks = []
        semaphore = asyncio.Semaphore(10)  # é™åˆ¶å¹¶å‘æ•°
        
        async def scan_single_file(file_name: str):
            async with semaphore:
                url = urljoin(base_url, file_name)
                response = await self.safe_request(url, timeout=5)
                
                # æ›´æ–°ç»Ÿè®¡
                self.stats['requests_made'] += 1
                
                if response and response.status == 200:
                    try:
                        content = await response.text()
                        file_size = len(content)
                        
                        # æ›´æ–°ä¸‹è½½ç»Ÿè®¡
                        self.stats['bytes_downloaded'] += file_size
                        
                        # è¯„ä¼°æ–‡ä»¶é£é™©ç­‰çº§
                        risk_level = self._assess_file_risk(file_name, file_size, content)
                        
                        self.results.files.append({
                            "file": file_name,
                            "url": url,
                            "size": file_size,
                            "preview": content[:100],
                            "risk_level": risk_level,
                            "content_type": response.headers.get('Content-Type', ''),
                            "last_modified": response.headers.get('Last-Modified', '')
                        })
                        
                        logger.debug(f"å‘ç°æ•æ„Ÿæ–‡ä»¶: {file_name} ({file_size} bytes, {risk_level})")
                        
                    except Exception as e:
                        logger.debug(f"æ–‡ä»¶å†…å®¹è¯»å–å¤±è´¥ {url}: {type(e).__name__}")
                        self.stats['requests_failed'] += 1
                elif response:
                    # è®°å½•å…¶ä»–çŠ¶æ€ç çš„å“åº”
                    if response.status in [403, 401]:
                        self.results.files.append({
                            "file": file_name,
                            "url": url,
                            "size": 0,
                            "preview": f"Access denied (HTTP {response.status})",
                            "risk_level": "medium",
                            "status": response.status
                        })
                else:
                    self.stats['requests_failed'] += 1
        
        # å¯åŠ¨æ‰€æœ‰ä»»åŠ¡
        for file_name in self.key_files:
            tasks.append(scan_single_file(file_name))
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        await asyncio.gather(*tasks, return_exceptions=True)
        
        logger.info(f"æ•æ„Ÿæ–‡ä»¶æ‰«æå®Œæˆï¼Œå‘ç° {len(self.results.files)} ä¸ªæ–‡ä»¶")
    
    def _assess_file_risk(self, file_name: str, file_size: int, content: str) -> str:
        """è¯„ä¼°æ–‡ä»¶é£é™©ç­‰çº§"""
        file_lower = file_name.lower()
        content_lower = content.lower()
        
        # é«˜é£é™©æ–‡ä»¶
        high_risk_files = ['.env', 'wp-config', '.git', 'backup', 'database', 'config.php']
        if any(risk_file in file_lower for risk_file in high_risk_files):
            return "critical"
        
        # æ£€æŸ¥å†…å®¹ä¸­çš„æ•æ„Ÿä¿¡æ¯
        sensitive_patterns = ['password', 'secret', 'key', 'token', 'database', 'mysql', 'postgres']
        if any(pattern in content_lower for pattern in sensitive_patterns):
            return "high"
        
        # Source maps
        if '.map' in file_lower:
            return "high"
        
        # é…ç½®æ–‡ä»¶
        if any(ext in file_lower for ext in ['.json', '.xml', '.config']):
            return "medium"
        
        return "low"
    async def extract_js_data(self):
        #æå–JSä¸­çš„APIç«¯ç‚¹ å¢å¼ºç‰ˆ
        print("[+] åˆ†æJavaScriptæ–‡ä»¶...")
        
        # ä½¿ç”¨ç³»ç»Ÿä»£ç†è®¾ç½®
        async with aiohttp.ClientSession() as session:
            try:
                async with session.get(self.base_url, timeout=10) as resp:
                    html = await resp.text()
                    
                    # æ‰¾å‡ºæ‰€æœ‰JSæ–‡ä»¶
                    js_files = re.findall(r'<script[^>]*src=["\']([^"\']+\.js[^"\']*)', html)
                    
                    # æ£€æŸ¥React DevTools Hook
                    if '__REACT_DEVTOOLS_GLOBAL_HOOK__' in html:
                        self.results["technologies"].append({
                            "type": "frontend",
                            "name": "React (DevTools Exposed)",
                            "detail": "React DevTools Hook detected"
                        })
                    
                    # æ£€æŸ¥webpack
                    if 'webpackJsonp' in html or 'webpackChunk' in html:
                        self.results["technologies"].append({
                            "type": "bundler",
                            "name": "Webpack",
                            "detail": "Webpack chunks detected"
                        })
                    
                    for js_file in js_files[:50]:  # ç›´æ¥ä½ å¦ˆ50ï¼
                        js_url = urljoin(self.base_url, js_file)
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„source map
                        map_url = js_url + '.map'
                        
                        try:
                            # ä¸‹è½½JSæ–‡ä»¶
                            async with session.get(js_url, timeout=5) as js_resp:
                                js_content = await js_resp.text()
                                
                                # æ£€æŸ¥source mapæ³¨é‡Š
                                if '//# sourceMappingURL=' in js_content:
                                    map_ref = re.search(r'//# sourceMappingURL=([^\s]+)', js_content)
                                    if map_ref:
                                        actual_map_url = urljoin(js_url, map_ref.group(1))
                                        # å°è¯•ä¸‹è½½source map
                                        try:
                                            async with session.get(actual_map_url, timeout=5) as map_resp:
                                                if map_resp.status == 200:
                                                    self.results["files"].append({
                                                        "file": "source_map",
                                                        "url": actual_map_url,
                                                        "size": len(await map_resp.read()),
                                                        "preview": "Source map found - contains original source code!"
                                                    })
                                        except Exception as e:  # æ³¨æ„ï¼šéœ€è¦ import logging
                                            logging.warning(f"å¼‚å¸¸è¢«å¿½ç•¥: {type(e).__name__}: {str(e)}")
                                # å¢å¼ºçš„APIæå–
                                api_patterns = [
                                    r'["\']/(api/[^"\']+)',
                                    r'["\']/(ajax/[^"\']+)',
                                    r'["\']/(graphql[^"\']*)',
                                    r'["\']/(rest/[^"\']+)',
                                    r'endpoint["\']:\s*["\']([^"\']+)',
                                    r'url["\']:\s*["\']([^"\']+)',
                                    r'baseURL["\']:\s*["\']([^"\']+)',
                                    r'apiUrl["\']:\s*["\']([^"\']+)',
                                    # GraphQLç‰¹å¾
                                    r'query\s+\w+\s*{[^}]+}',
                                    r'mutation\s+\w+\s*{[^}]+}',
                                    r'subscription\s+\w+\s*\{[^}]+\}',  #æ–°å¢
                                    r'["\']swagger["\']:\s*["\']([^"\']+)', #æ–°å¢
                                    r'["\']openapi["\']:\s*["\']([^"\']+)'  #æ–°å¢
                                ]
                                
                                for pattern in api_patterns:
                                    matches = re.findall(pattern, js_content)
                                    for match in matches:
                                        self.results["api_routes"].append({
                                            "route": match[:100],  # é™åˆ¶é•¿åº¦
                                            "source": js_file
                                        })
                                
                                # æå–å¯èƒ½çš„å¯†é’¥ä»¤ç‰Œ
                                secret_patterns = [
                                    r'["\'](?:api[_-]?key|apikey)["\']:\s*["\']([^"\']+)',
                                    r'["\'](?:secret|token)["\']:\s*["\']([^"\']+)',
                                    r'["\'](?:auth|authorization)["\']:\s*["\']([^"\']+)',
                                    r'["\'](?:firebase|aws|azure)[^"\']*["\']:\s*["\']([^"\']+)'
                                ]
                                
                                for pattern in secret_patterns:
                                    matches = re.findall(pattern, js_content, re.I)
                                    for match in matches:
                                        if len(match) > 10:  # è¿‡æ»¤æ˜æ˜¾çš„å ä½ç¬¦
                                            self.results["api_routes"].append({
                                                "route": f"[POTENTIAL SECRET: {match[:20]}...]",
                                                "source": js_file
                                            })
                                            
                        except Exception as e:  # æ³¨æ„ï¼šéœ€è¦ import logging
                                            
                            logging.warning(f"å¼‚å¸¸è¢«å¿½ç•¥: {type(e).__name__}: {str(e)}")
                        # å°è¯•ç›´æ¥è®¿é—®.map
                        try:
                            async with session.get(map_url, timeout=3) as map_resp:
                                if map_resp.status == 200:
                                    self.results["files"].append({
                                        "file": "source_map",
                                        "url": map_url,
                                        "size": len(await map_resp.read()),
                                        "preview": "Direct source map access!"
                                    })
                        except Exception as e:  # æ³¨æ„ï¼šéœ€è¦ import logging
                            logging.warning(f"å¼‚å¸¸è¢«å¿½ç•¥: {type(e).__name__}: {str(e)}")
            except Exception as e:  # æ³¨æ„ï¼šéœ€è¦ import logging
                logging.warning(f"å¼‚å¸¸è¢«å¿½ç•¥: {type(e).__name__}: {str(e)}")
    async def tech_fingerprint(self):
        #æŠ€æœ¯æ ˆè¯†åˆ«
        print("[+] è¯†åˆ«æŠ€æœ¯æ ˆ...")
        
        # ä½¿ç”¨ç³»ç»Ÿä»£ç†è®¾ç½®
        async with aiohttp.ClientSession() as session:
            try:
                async with session.get(self.base_url, timeout=10) as resp:
                    headers = dict(resp.headers)
                    html = await resp.text()
                    
                    # ä¿®æ­£ï¼šæ·»åŠ å®‰å…¨é…ç½®æ£€æµ‹
                    # å®‰å…¨å¤´æ£€æµ‹
                    security_headers = {
                        'X-Frame-Options': 'é˜²ç‚¹å‡»åŠ«æŒ',
                        'Content-Security-Policy': 'CSPç­–ç•¥',
                        'Strict-Transport-Security': 'HSTS',
                        'X-Content-Type-Options': 'MIMEç±»å‹',
                        'X-XSS-Protection': 'XSSé˜²æŠ¤'
                    }
                    
                    missing_headers = []
                    for header, desc in security_headers.items():
                        if header not in headers:
                            missing_headers.append(f"{header} ({desc})")
                    
                    if missing_headers:
                        self.results["technologies"].append({
                            "type": "security_config", 
                            "name": "Missing Security Headers",
                            "detail": missing_headers
                        })
                    
                    # CORSé…ç½®æ£€æµ‹
                    if 'Access-Control-Allow-Origin' in headers:
                        cors_value = headers['Access-Control-Allow-Origin']
                        if cors_value == '*':
                            self.results["technologies"].append({
                                "type": "security_config",
                                "name": "CORS Misconfiguration", 
                                "detail": f"Wildcard CORS: {cors_value}"
                            })
                    
                    # CDNæ£€æµ‹
                    cdn_headers = {
                        'cf-ray': 'Cloudflare',
                        'x-amz-cf-id': 'AWS CloudFront', 
                        'x-azure-ref': 'Azure CDN',
                        'ali-swift-global-savetime': 'Alibaba CDN'
                    }
                    
                    for header, cdn_name in cdn_headers.items():
                        if header in headers:
                            self.results["technologies"].append({
                                "type": "cdn",
                                "name": cdn_name,
                                "detail": f"{header}: {headers[header]}"
                            })
                    
                    
                    # æœåŠ¡å™¨è¯†åˆ«
                    if 'Server' in headers:
                        self.results["technologies"].append({
                            "type": "server",
                            "name": headers['Server']
                        })
                    
                    # CMSè¯†åˆ«
                    cms_signatures = {
                        "WordPress": ["wp-content", "wp-includes", "wp-json"],
                        "Drupal": ["sites/default", "drupal.js"],
                        "Joomla": ["option=com_", "joomla"],
                        "Laravel": ["laravel_session"],
                        "Django": ["csrfmiddlewaretoken"]
                    }
                    
                    for cms, signatures in cms_signatures.items():
                        if any(sig in html for sig in signatures):
                            self.results["technologies"].append({
                                "type": "cms",
                                "name": cms
                            })
                    
                    # æ¡†æ¶è¯†åˆ«
                    if "react" in html.lower():
                        self.results["technologies"].append({"type": "frontend", "name": "React"})
                    if "vue" in html.lower():
                        self.results["technologies"].append({"type": "frontend", "name": "Vue"})
                    if "angular" in html.lower():
                        self.results["technologies"].append({"type": "frontend", "name": "Angular"})
                        
            except Exception as e:  # æ³¨æ„ï¼šéœ€è¦ import logging
                        
                logging.warning(f"å¼‚å¸¸è¢«å¿½ç•¥: {type(e).__name__}: {str(e)}")
    def generate_report(self):
        """ç”Ÿæˆæ‰«ææŠ¥å‘Š - æ ‘çŠ¶å…³è”æ¨¡å‹ç‰ˆ"""
        # ğŸŒŸ é¦–å…ˆè®¡ç®—æ‰€æœ‰èµ„äº§çš„é£é™©è¯„åˆ†
        self.results.calculate_risk_scores()
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_file = f"asset_report_{self.target}_{timestamp}.json"
        
        # å‡†å¤‡æŠ¥å‘Šæ•°æ®
        report_data = {
            "scan_info": {
                "target": self.target,
                "timestamp": datetime.now().isoformat(),
                "scan_duration": f"{time.time() - self.start_time:.2f}s",
                "config": {
                    "max_crawl_pages": self.config.max_crawl_pages,
                    "max_js_files": self.config.max_js_files,
                    "concurrent_limit": self.config.concurrent_limit
                }
            },
            "results": self.results.to_dict(),
            "summary": self._generate_summary(),
            "high_priority_targets": self._identify_high_priority_targets(),
            "waf_protection_stats": self._get_waf_stats()
        }
        
        # ä¿å­˜JSONæŠ¥å‘Š
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, ensure_ascii=False, indent=2, default=str)
            
            logger.info(f"æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
            
            # è¾“å‡ºé«˜ä¼˜å…ˆçº§å‘ç°
            self._print_priority_findings()
            
        except Exception as e:
            logger.error(f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {type(e).__name__}: {e}")
    
    def _generate_summary(self) -> Dict:
        """ç”Ÿæˆæ‰«ææ‘˜è¦ - æ ‘çŠ¶å…³è”æ¨¡å‹ç‰ˆ"""
        total_endpoints = sum(len(asset["endpoints"]) for asset in self.results.assets.values())
        total_forms = sum(len(asset["forms"]) for asset in self.results.assets.values())
        total_technologies = sum(len(asset["technologies"]) for asset in self.results.assets.values())
        
        # ç»Ÿè®¡é«˜é£é™©èµ„äº§
        high_risk_assets = [domain for domain, asset in self.results.assets.items() 
                          if asset["risk_score"] >= 70]
        
        # ç»Ÿè®¡ç«¯ç‚¹ç±»å‹
        endpoint_types = {}
        admin_endpoints = 0
        api_endpoints = 0
        
        for asset in self.results.assets.values():
            for endpoint in asset["endpoints"].values():
                endpoint_type = endpoint.get("endpoint_type", "unknown")
                endpoint_types[endpoint_type] = endpoint_types.get(endpoint_type, 0) + 1
                
                if endpoint_type == "admin_panel":
                    admin_endpoints += 1
                elif endpoint_type in ["api_endpoint", "graphql"]:
                    api_endpoints += 1
        
        return {
            "total_assets": len(self.results.assets),
            "total_subdomains": len([a for a in self.results.assets.values() if a["type"] == "subdomain"]),
            "total_endpoints": total_endpoints,
            "total_forms": total_forms,
            "total_technologies": total_technologies,
            "admin_endpoints": admin_endpoints,
            "api_endpoints": api_endpoints,
            "endpoint_types": endpoint_types,
            "form_types": self._count_form_types(),
            "high_risk_assets": len(high_risk_assets),
            "high_risk_asset_list": high_risk_assets[:5],  # æ˜¾ç¤ºå‰5ä¸ª
            "avg_risk_score": round(sum(asset["risk_score"] for asset in self.results.assets.values()) / max(len(self.results.assets), 1), 2)
        }
    
    def _count_form_types(self) -> Dict[str, int]:
        """ç»Ÿè®¡è¡¨å•ç±»å‹ - æ ‘çŠ¶å…³è”æ¨¡å‹ç‰ˆ"""
        form_types = {}
        for asset in self.results.assets.values():
            for form in asset["forms"]:
                form_type = form.get('form_type', 'unknown')
                form_types[form_type] = form_types.get(form_type, 0) + 1
        return form_types
    
    def _identify_high_priority_targets(self) -> Dict:
        """è¯†åˆ«é«˜ä¼˜å…ˆçº§ç›®æ ‡"""
        high_priority = {
            "reservation_forms": [],
            "login_forms": [],
            "admin_panels": [],
            "sensitive_files": [],
            "api_endpoints": []
        }
        
        # é¢„çº¦è¡¨å•ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
        for form in self.results.forms:
            if form.get('form_type') == 'reservation':
                high_priority["reservation_forms"].append({
                    "url": form["url"],
                    "action": form["action"],
                    "method": form["method"]
                })
        
        # ç™»å½•è¡¨å•
        for form in self.results.forms:
            if form.get('form_type') == 'login':
                high_priority["login_forms"].append({
                    "url": form["url"],
                    "action": form["action"]
                })
        
        # é«˜é£é™©ç®¡ç†é¢æ¿
        for panel in self.results.admin_panels:
            if panel.get('risk_level') in ['high', 'medium']:
                high_priority["admin_panels"].append({
                    "url": panel["url"],
                    "status": panel["status"],
                    "risk_level": panel["risk_level"]
                })
        
        # æ•æ„Ÿæ–‡ä»¶
        sensitive_extensions = ['.env', '.git', '.config', '.bak', '.map']
        for file_info in self.results.files:
            file_name = file_info.get('file', '')
            if any(ext in file_name for ext in sensitive_extensions):
                high_priority["sensitive_files"].append({
                    "file": file_name,
                    "url": file_info["url"],
                    "size": file_info["size"]
                })
        
        # APIç«¯ç‚¹
        for api in self.results.api_routes:
            route = api.get('route', '')
            if any(keyword in route.lower() for keyword in ['graphql', 'api', 'rest']):
                high_priority["api_endpoints"].append({
                    "route": route,
                    "source": api.get("source", "")
                })
        
        return high_priority
    
    def _get_waf_stats(self) -> Dict:
        """è·å–WAFé˜²æŠ¤ç»Ÿè®¡ä¿¡æ¯"""
        waf_stats = {
            'waf_defender_enabled': WAF_DEFENDER_AVAILABLE,
            'waf_defender_initialized': self.waf_defender_initialized,
            'target_url': f"https://{self.target}" if hasattr(self, 'target') else 'unknown',
            'protection_status': 'å·²å¯ç”¨WAFæ¬ºéª—æ£€æµ‹' if self.waf_defender_initialized else 
                               ('WAF Defenderä¸å¯ç”¨' if not WAF_DEFENDER_AVAILABLE else 'æœªåˆå§‹åŒ–'),
            'baseline_info': self.waf_defender.get_stats() if self.waf_defender else None
        }
        
        # æ·»åŠ å™ªéŸ³è¿‡æ»¤ç»Ÿè®¡ï¼ˆä¸gitå·¥å…·ä¸€è‡´ï¼‰
        waf_stats['noise_filtering_stats'] = {
            'filter_enabled': NOISE_FILTER_AVAILABLE,
            'total_checked': self.noise_stats['total_checked'],
            'noise_filtered': self.noise_stats['noise_filtered'],
            'valuable_kept': self.noise_stats['valuable_kept'],
            'noise_ratio': (self.noise_stats['noise_filtered'] / 
                           max(self.noise_stats['total_checked'], 1)) if self.noise_stats['total_checked'] > 0 else 0,
            'effectiveness': 'æœ‰æ•ˆè¿‡æ»¤å™ªéŸ³' if self.noise_stats['noise_filtered'] > 0 else 'æ— å™ªéŸ³å‘ç°'
        }
        
        return waf_stats
    
    def _print_priority_findings(self):
        """æ‰“å°é«˜ä¼˜å…ˆçº§å‘ç°"""
        priority = self._identify_high_priority_targets()
        
        logger.info("=== é«˜ä¼˜å…ˆçº§ç›®æ ‡ ===")
        
        if priority["reservation_forms"]:
            logger.info(f" é¢„çº¦è¡¨å• ({len(priority['reservation_forms'])}ä¸ª):")
            for form in priority["reservation_forms"][:10]:  
                logger.info(f"   - {form['action']} ({form['method']})")
        
        if priority["admin_panels"]:
            logger.info(f" ç®¡ç†é¢æ¿ ({len(priority['admin_panels'])}ä¸ª):")
            for panel in priority["admin_panels"][:10]:
                logger.info(f"   - {panel['url']} [{panel['status']}] ({panel['risk_level']})")
        
        if priority["sensitive_files"]:
            logger.info(f" æ•æ„Ÿæ–‡ä»¶ ({len(priority['sensitive_files'])}ä¸ª):")
            for file_info in priority["sensitive_files"][:50]:
                logger.info(f"   - {file_info['file']} ({file_info['size']} bytes)")
        
        if priority["api_endpoints"]:
            logger.info(f" APIç«¯ç‚¹ ({len(priority['api_endpoints'])}ä¸ª):")
            for api in priority["api_endpoints"][:20]:
                logger.info(f"   - {api['route']}")
        
        # æ€»ä½“é£é™©è¯„ä¼°
        total_high_risk = (len(priority["reservation_forms"]) + 
                          len(priority["admin_panels"]) + 
                          len(priority["sensitive_files"]))
        
        if total_high_risk > 5:
            logger.info("  é£é™©è¯„ä¼°: é«˜é£é™© - å‘ç°å¤šä¸ªæ•æ„Ÿç›®æ ‡")
        elif total_high_risk > 2:
            logger.info("  é£é™©è¯„ä¼°: ä¸­ç­‰é£é™© - å­˜åœ¨éƒ¨åˆ†æ•æ„Ÿç›®æ ‡")
        else:
            logger.info(" é£é™©è¯„ä¼°: ä½é£é™© - æš´éœ²é¢è¾ƒå°")

    async def query_crtsh(self, subdomains):
        #æŸ¥è¯¢è¯ä¹¦é€æ˜åº¦æ—¥å¿—
        print("[+] æŸ¥è¯¢è¯ä¹¦é€æ˜åº¦ (crt.sh)...")
        
        # ä½¿ç”¨ç³»ç»Ÿä»£ç†è®¾ç½®
        async with aiohttp.ClientSession() as session:
            try:
                # crt.sh API
                url = f"https://crt.sh/?q=%.{self.target}&output=json"  #ä¿®æ­£
                async with session.get(url, timeout=30) as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        for cert in data:
                            name_value = cert.get('name_value', '')
                            # æå–æ‰€æœ‰åŸŸå
                            domains = name_value.replace('*.', '').split('\n')
                            for domain in domains:
                                if domain and self.target in domain:
                                    subdomains.add(domain.strip())
                        print(f"    å‘ç° {len(data)} ä¸ªè¯ä¹¦")
            except Exception as e:
                print(f"[-] crt.shæŸ¥è¯¢å¤±è´¥: {e}")

    async def try_zone_transfer(self, subdomains):
        #å°è¯•DNSåŒºåŸŸä¼ è¾“
        print("[+] å°è¯•DNSåŒºåŸŸä¼ è¾“...")
        
        try:
            # è·å–NSè®°å½•
            import dns.resolver
            import dns.zone
            import dns.query
            
            resolver = dns.resolver.Resolver()
            answers = resolver.resolve(self.target, 'NS')
            
            for ns in answers:
                ns_str = str(ns).rstrip('.')
                print(f"    æµ‹è¯•NSæœåŠ¡å™¨: {ns_str}")
                
                try:
                    # å°è¯•åŒºåŸŸä¼ è¾“
                    zone = dns.zone.from_xfr(dns.query.xfr(ns_str, self.target))
                    print(f"[!] åŒºåŸŸä¼ è¾“æˆåŠŸ! NS: {ns_str}")
                    
                    # æå–æ‰€æœ‰è®°å½•
                    for name, node in zone.nodes.items():
                        subdomain = str(name) + '.' + self.target
                        if subdomain != f"@.{self.target}":
                            subdomains.add(subdomain)
                except Exception as e:  # æ³¨æ„ï¼šéœ€è¦ import logging
                    logging.warning(f"å¼‚å¸¸è¢«å¿½ç•¥: {type(e).__name__}: {str(e)}")
        except ImportError:
            print("[-] éœ€è¦å®‰è£…dnspython: pip install dnspython")
        except Exception as e:
            pass

async def main():
    """ä¸»å‡½æ•° - æ™ºèƒ½èµ„äº§æ˜ å°„ + è®¤è¯æ”¯æŒ"""
    import sys
    
    if len(sys.argv) > 1:
        target_domain = sys.argv[1]
    else:
        target_domain = input("è¯·è¾“å…¥ç›®æ ‡åŸŸå [é»˜è®¤: asanoha-clinic.com]: ").strip()
        if not target_domain:
            target_domain = "asanoha-clinic.com"
    
    # è®¤è¯é…ç½®ç¤ºä¾‹ï¼ˆå¯æ ¹æ®éœ€è¦ä¿®æ”¹ï¼‰
    auth_config = None
    
    # ç¤ºä¾‹1: ç®¡ç†å‘˜åå°ç™»å½•
    # auth_config = {
    #     'login_url': f'https://{target_domain}/admin/login',
    #     'username': 'admin',
    #     'password': 'password123',
    #     'heartbeat_endpoint': '/admin/api/status'
    # }
    
    # ç¤ºä¾‹2: ç”¨æˆ·ç™»å½•ï¼ˆè®¿é—®å†…éƒ¨èµ„äº§ï¼‰
    # auth_config = {
    #     'login_url': f'https://{target_domain}/login',
    #     'username': 'user',
    #     'password': 'userpass',
    #     'heartbeat_endpoint': '/api/profile'
    # }
    
    # ç¤ºä¾‹3: JWT Tokenæ–¹å¼
    # auth_config = {
    #     'jwt_token': 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...'
    # }
    
    # ğŸš€ å²è¯—çº§é“¾å¼è¿½è¸ªé…ç½®
    enable_epic_mode = True  # å¯ç”¨å²è¯—çº§æ¨¡å¼
    chain_config = ChainTrackingConfig(
        max_scan_depth=3,      # æœ€å¤§æ‰«ææ·±åº¦ï¼š3å±‚
        max_domain_count=10,   # æœ€å¤§åŸŸåæ•°é‡ï¼š10ä¸ªï¼ˆé˜²æ­¢çˆ†ç‚¸å¼å¢é•¿ï¼‰
        scan_interval=2.0,     # æ‰«æé—´éš”ï¼š2ç§’ï¼ˆé¿å…è§¦å‘å‘Šè­¦ï¼‰
        enable_internal_scan=True,   # å¯ç”¨å†…ç½‘åŸŸåæ‰«æ
        enable_ip_scan=False,        # ç¦ç”¨IPåœ°å€æ‰«æ
        scope_domains=[]  # ç©ºåˆ—è¡¨è¡¨ç¤ºåªæ‰«æä¸»åŸŸåçš„å­åŸŸå
    )
    
    config = AssetMapperConfig(
        max_crawl_pages=50,  # é™åˆ¶çˆ¬å–é¡µé¢æ•°
        max_js_files=30,     # é™åˆ¶JSæ–‡ä»¶åˆ†ææ•°
        concurrent_limit=15,  # é€‚ä¸­çš„å¹¶å‘æ•°
        use_dynamic_ip=True,  # å¯ç”¨åŠ¨æ€IPæ± ï¼ˆç»ˆæç»„åˆæ‹³ï¼‰
        use_user_agent=True,  # å¯ç”¨User-Agentè½®æ¢
        enable_authentication=bool(auth_config),
        auth_config=auth_config or {}
    )
    
    print("="*80)
    if enable_epic_mode:
        print("ğŸš€ å²è¯—çº§é“¾å¼è¿½è¸ªèµ„äº§æ˜ å°„å™¨")
        print("ğŸ“¡ è‡ªåŠ¨å‘ç°å’Œæ‰«ææ•´ä¸ªèµ„äº§ç½‘ç»œ")
    else:
        print("ç›®æ ‡ å¯åŠ¨èµ„äº§æ˜ å°„å™¨")
    print("="*80)
    print(f"ğŸ¯ åˆå§‹ç›®æ ‡: {target_domain}")
    print(f"è®¤è¯ è®¤è¯æ¨¡å¼: {'å¯ç”¨' if auth_config else 'ç¦ç”¨'}")
    
    if enable_epic_mode:
        print(f"ğŸš€ é“¾å¼è¿½è¸ªé…ç½®:")
        print(f"   ğŸ“Š æœ€å¤§æ‰«ææ·±åº¦: {chain_config.max_scan_depth} å±‚")
        print(f"   ğŸ“Š æœ€å¤§åŸŸåæ•°é‡: {chain_config.max_domain_count} ä¸ª")
        print(f"   â±ï¸  æ‰«æé—´éš”: {chain_config.scan_interval} ç§’")
        print(f"   ğŸ” å†…ç½‘æ‰«æ: {'å¯ç”¨' if chain_config.enable_internal_scan else 'ç¦ç”¨'}")
        print(f"   ğŸŒ IPæ‰«æ: {'å¯ç”¨' if chain_config.enable_ip_scan else 'ç¦ç”¨'}")
    
    # æ˜¾ç¤ºç»•è¿‡æ¨¡å¼é…ç½®
    if config.use_dynamic_ip and DYNAMIC_IP_AVAILABLE:
        print(f"ç»•è¿‡æ¨¡å¼: ç»ˆæç»„åˆæ‹³ (åŠ¨æ€IPæ±  + User-Agentè½®æ¢)")
    elif config.use_user_agent and USER_AGENT_AVAILABLE:
        print(f"ç»•è¿‡æ¨¡å¼: User-Agentè½®æ¢æ¨¡å¼")
    else:
        print(f"ç»•è¿‡æ¨¡å¼: åŸºç¡€æ¨¡å¼")
    
    if auth_config:
        print("  å¯ç”¨è®¤è¯æ¨¡å¼ - å¯è®¿é—®è®¤è¯åå†…éƒ¨èµ„äº§ï¼")
        print("   é¢„æœŸå‘ç°: å†…éƒ¨APIã€ç®¡ç†ç«¯ç‚¹ã€éšè—åŠŸèƒ½")
    else:
        print("  æ— è®¤è¯æ¨¡å¼ - ä»…è®¿é—®å…¬å¼€èµ„äº§")
        print("   æç¤º: ä¿®æ”¹mainå‡½æ•°ä¸­çš„auth_configæ¥å¯ç”¨è®¤è¯")
    
    if config.use_dynamic_ip or config.use_user_agent:
        print("  å¯ç”¨ç»•è¿‡å¢å¼º - æé«˜WAFç»•è¿‡èƒ½åŠ›ï¼")
        if config.use_dynamic_ip:
            print("   - 500ä¸ªåŠ¨æ€IPè½®æ¢")
        if config.use_user_agent:
            print("   - æ™ºèƒ½User-Agentä¼ªè£…")
    
    async with AssetMapper(target_domain, config, auth_config, 
                          enable_chain_tracking=enable_epic_mode, 
                          chain_config=chain_config) as mapper:
        results = await mapper.run()
        
        # è¾“å‡ºå…³é”®ç»Ÿè®¡ä¿¡æ¯
        logger.info("=== æ‰«æç»“æœæ‘˜è¦ ===")
        logger.info(f"å­åŸŸå: {len(results.subdomains)}")
        logger.info(f"è¡¨å•: {len(results.forms)}")
        logger.info(f"APIè·¯ç”±: {len(results.api_routes)}")
        logger.info(f"åå°è·¯å¾„: {len(results.admin_panels)}")
        logger.info(f"æ•æ„Ÿæ–‡ä»¶: {len(results.files)}")
        logger.info(f"æŠ€æœ¯æ ˆ: {len(results.technologies)}")
        
        if auth_config and mapper.auth_manager:
            auth_stats = mapper.auth_manager.get_auth_stats()
            print(f"\nè®¤è¯ è®¤è¯ç»Ÿè®¡:")
            print(f"    è®¤è¯è¯·æ±‚: {auth_stats.get('authenticated_requests', 0)}")
            print(f"    è®¤è¯å¤±è´¥: {auth_stats.get('auth_failures', 0)}")
            print(f"    ä¼šè¯æ¢å¤: {auth_stats.get('session_recoveries', 0)}")
        
        return results

if __name__ == "__main__":
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    logger.setLevel(logging.INFO)
    
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("æ‰«æè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"ç¨‹åºå¼‚å¸¸é€€å‡º: {type(e).__name__}: {e}")
