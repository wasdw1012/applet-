
import asyncio
import re
import json
from datetime import datetime
from anti_waf_engine import StealthHTTPClient

class HTMLContentExtractor:
    def __init__(self):
        self.session_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        # æµ‹è¯•æ‰€æœ‰è¿”å›ç›¸åŒHTMLçš„ç«¯ç‚¹
        self.test_urls = [
            "https://secure.biograph.com/backup.sql",
            "http://secure.biograph.com/api",
            "http://secure.biograph.com/config.txt",
            "http://secure.biograph.com/admin.php",
            "http://secure.biograph.com/wp-admin/"
        ]
        self.html_content = ""
        self.findings = {}
    
    async def download_html_content(self):
        """ä¸‹è½½HTMLå†…å®¹è¿›è¡Œåˆ†æ"""
        print("ğŸ”» ä¸‹è½½HTMLå†…å®¹è¿›è¡Œæ·±åº¦åˆ†æ...")
        
        async with StealthHTTPClient() as client:
            try:
                # ä½¿ç”¨ç¬¬ä¸€ä¸ªURLä¸‹è½½å†…å®¹
                async with await client.get(self.test_urls[0], timeout=15) as response:
                    if response.status == 200:
                        content = await response.read()
                        self.html_content = content.decode('utf-8', errors='ignore')
                        print(f"âœ… æˆåŠŸè·å–HTMLå†…å®¹: {len(self.html_content)}å­—ç¬¦")
                        return True
                    else:
                        print(f"âŒ ä¸‹è½½å¤±è´¥: HTTP {response.status}")
                        return False
            except Exception as e:
                print(f"âŒ ä¸‹è½½å¼‚å¸¸: {e}")
                return False
    
    def extract_application_info(self):
        """æå–åº”ç”¨ç¨‹åºä¿¡æ¯"""
        print("\nğŸ” åº”ç”¨ç¨‹åºä¿¡æ¯æå–:")
        
        findings = {
            'title': '',
            'meta_tags': [],
            'script_sources': [],
            'css_sources': [],
            'api_endpoints': [],
            'version_info': [],
            'third_party_services': []
        }
        
        # æå–é¡µé¢æ ‡é¢˜
        title_match = re.search(r'<title[^>]*>(.*?)</title>', self.html_content, re.IGNORECASE | re.DOTALL)
        if title_match:
            findings['title'] = title_match.group(1).strip()
            print(f"  ğŸ“„ é¡µé¢æ ‡é¢˜: {findings['title']}")
        
        # æå–metaæ ‡ç­¾
        meta_pattern = r'<meta\s+([^>]+)>'
        meta_matches = re.findall(meta_pattern, self.html_content, re.IGNORECASE)
        for meta in meta_matches:
            if 'name=' in meta or 'property=' in meta:
                findings['meta_tags'].append(meta)
                print(f"  ğŸ·ï¸ Metaæ ‡ç­¾: {meta[:60]}...")
        
        # æå–JavaScriptæ–‡ä»¶
        script_pattern = r'<script[^>]+src=["\']([^"\']+)["\']'
        script_sources = re.findall(script_pattern, self.html_content, re.IGNORECASE)
        for src in script_sources:
            findings['script_sources'].append(src)
            print(f"  ğŸ“œ JSæ–‡ä»¶: {src}")
            
            # æ£€æŸ¥ç‰ˆæœ¬ä¿¡æ¯
            version_match = re.search(r'[\d.]+', src)
            if version_match and len(version_match.group()) > 3:
                findings['version_info'].append({
                    'type': 'javascript_version',
                    'value': version_match.group(),
                    'source': src
                })
        
        # æå–CSSæ–‡ä»¶
        css_pattern = r'<link[^>]+href=["\']([^"\']+\.css[^"\']*)["\']'
        css_sources = re.findall(css_pattern, self.html_content, re.IGNORECASE)
        for src in css_sources:
            findings['css_sources'].append(src)
            print(f"  ğŸ¨ CSSæ–‡ä»¶: {src}")
        
        self.findings.update(findings)
        return findings
    
    def extract_inline_javascript(self):
        """æå–å†…è”JavaScriptä»£ç """
        print("\nğŸ’» å†…è”JavaScriptåˆ†æ:")
        
        # æå–æ‰€æœ‰scriptæ ‡ç­¾å†…å®¹
        inline_script_pattern = r'<script[^>]*>(.*?)</script>'
        inline_scripts = re.findall(inline_script_pattern, self.html_content, re.DOTALL | re.IGNORECASE)
        
        js_findings = {
            'config_objects': [],
            'api_calls': [],
            'environment_vars': [],
            'debug_info': []
        }
        
        for script_content in inline_scripts:
            if len(script_content.strip()) < 10:
                continue
                
            print(f"  ğŸ” åˆ†æè„šæœ¬æ®µè½: {len(script_content)}å­—ç¬¦")
            
            # æœç´¢é…ç½®å¯¹è±¡
            config_patterns = [
                r'window\.__CONFIG__\s*=\s*({[^}]+})',
                r'const\s+config\s*=\s*({[^}]+})',
                r'var\s+config\s*=\s*({[^}]+})',
                r'window\.APP_CONFIG\s*=\s*({[^}]+})'
            ]
            
            for pattern in config_patterns:
                matches = re.findall(pattern, script_content, re.DOTALL)
                for match in matches:
                    try:
                        config_data = json.loads(match)
                        js_findings['config_objects'].append(config_data)
                        print(f"    ğŸ“‹ é…ç½®å¯¹è±¡: {len(str(config_data))}å­—ç¬¦")
                    except:
                        # å³ä½¿JSONè§£æå¤±è´¥ï¼Œä¹Ÿè®°å½•åŸå§‹å†…å®¹
                        js_findings['config_objects'].append({'raw': match})
                        print(f"    ğŸ“‹ é…ç½®å¯¹è±¡(åŸå§‹): {match[:50]}...")
            
            # æœç´¢APIè°ƒç”¨
            api_patterns = [
                r'fetch\(["\']([^"\']+)["\']',
                r'axios\.(?:get|post|put|delete)\(["\']([^"\']+)["\']',
                r'\.ajax\s*\(\s*{[^}]*url\s*:\s*["\']([^"\']+)["\']',
                r'/api/[^"\')\s]+'
            ]
            
            for pattern in api_patterns:
                matches = re.findall(pattern, script_content, re.IGNORECASE)
                for match in matches:
                    if match not in js_findings['api_calls']:
                        js_findings['api_calls'].append(match)
                        print(f"    ğŸ”— APIè°ƒç”¨: {match}")
            
            # æœç´¢ç¯å¢ƒå˜é‡
            env_patterns = [
                r'process\.env\.(\w+)',
                r'NODE_ENV["\']?\s*[:=]\s*["\']([^"\']+)["\']',
                r'REACT_APP_(\w+)',
                r'VUE_APP_(\w+)'
            ]
            
            for pattern in env_patterns:
                matches = re.findall(pattern, script_content, re.IGNORECASE)
                for match in matches:
                    js_findings['environment_vars'].append(match)
                    print(f"    ğŸŒ ç¯å¢ƒå˜é‡: {match}")
        
        self.findings['javascript'] = js_findings
        return js_findings
    
    def extract_network_resources(self):
        """æå–ç½‘ç»œèµ„æºä¿¡æ¯"""
        print("\nğŸŒ ç½‘ç»œèµ„æºåˆ†æ:")
        
        network_findings = {
            'external_domains': [],
            'cdn_resources': [],
            'websocket_endpoints': [],
            'iframe_sources': []
        }
        
        # æå–å¤–éƒ¨åŸŸå
        domain_pattern = r'https?://([^/"\'\s]+)'
        domains = re.findall(domain_pattern, self.html_content, re.IGNORECASE)
        
        for domain in set(domains):
            if domain not in ['secure.biograph.com', 'biograph.com']:
                network_findings['external_domains'].append(domain)
                print(f"  ğŸŒ å¤–éƒ¨åŸŸå: {domain}")
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºCDN
                cdn_indicators = ['cdn', 'static', 'assets', 'cloudfront', 'fastly', 'cloudflare']
                if any(indicator in domain.lower() for indicator in cdn_indicators):
                    network_findings['cdn_resources'].append(domain)
                    print(f"    ğŸ“¦ CDNèµ„æº: {domain}")
        
        # æœç´¢WebSocketç«¯ç‚¹
        websocket_pattern = r'wss?://([^"\')\s]+)'
        websockets = re.findall(websocket_pattern, self.html_content, re.IGNORECASE)
        for ws in set(websockets):
            network_findings['websocket_endpoints'].append(ws)
            print(f"  ğŸ”Œ WebSocket: {ws}")
        
        # æå–iframeæº
        iframe_pattern = r'<iframe[^>]+src=["\']([^"\']+)["\']'
        iframes = re.findall(iframe_pattern, self.html_content, re.IGNORECASE)
        for iframe in iframes:
            network_findings['iframe_sources'].append(iframe)
            print(f"  ğŸ–¼ï¸ IFrame: {iframe}")
        
        self.findings['network'] = network_findings
        return network_findings
    
    def search_sensitive_patterns(self):
        """æœç´¢æ•æ„Ÿä¿¡æ¯æ¨¡å¼"""
        print("\nğŸš¨ æ•æ„Ÿä¿¡æ¯æœç´¢:")
        
        sensitive_findings = {
            'api_keys': [],
            'tokens': [],
            'internal_ips': [],
            'debug_info': [],
            'error_messages': []
        }
        
        # APIå¯†é’¥æ¨¡å¼
        api_key_patterns = [
            r'["\']([A-Za-z0-9+/]{40,}={0,2})["\']',  # Base64ç¼–ç çš„å¯†é’¥
            r'api[_-]?key["\']?\s*[:=]\s*["\']([^"\']{20,})["\']',
            r'access[_-]?token["\']?\s*[:=]\s*["\']([^"\']{20,})["\']'
        ]
        
        for pattern in api_key_patterns:
            matches = re.findall(pattern, self.html_content, re.IGNORECASE)
            for match in matches:
                if len(match) >= 20:  # è¿‡æ»¤å¤ªçŸ­çš„åŒ¹é…
                    sensitive_findings['api_keys'].append(match)
                    print(f"  ğŸ”‘ æ½œåœ¨APIå¯†é’¥: {match[:8]}...{match[-4:]}")
        
        # å†…ç½‘IPæ¨¡å¼
        internal_ip_pattern = r'\b(?:10\.|172\.(?:1[6-9]|2[0-9]|3[01])\.|192\.168\.)[\d.]+(?::\d+)?\b'
        internal_ips = re.findall(internal_ip_pattern, self.html_content)
        for ip in set(internal_ips):
            sensitive_findings['internal_ips'].append(ip)
            print(f"  ğŸ  å†…ç½‘IP: {ip}")
        
        # è°ƒè¯•ä¿¡æ¯
        debug_patterns = [
            r'console\.(?:log|error|warn|debug)\([^)]+\)',
            r'debugger;',
            r'DEBUG\s*=\s*true',
            r'development.*mode'
        ]
        
        for pattern in debug_patterns:
            matches = re.findall(pattern, self.html_content, re.IGNORECASE)
            for match in matches:
                sensitive_findings['debug_info'].append(match)
                print(f"  ğŸ› è°ƒè¯•ä¿¡æ¯: {match[:50]}...")
        
        self.findings['sensitive'] = sensitive_findings
        return sensitive_findings
    
    def generate_comprehensive_report(self):
        """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
        report = {
            'session_id': self.session_id,
            'analysis_time': datetime.now().isoformat(),
            'html_content_length': len(self.html_content),
            'target_urls': self.test_urls,
            'findings': self.findings,
            'summary': {
                'total_js_files': len(self.findings.get('script_sources', [])),
                'total_css_files': len(self.findings.get('css_sources', [])),
                'external_domains': len(self.findings.get('network', {}).get('external_domains', [])),
                'api_endpoints_found': len(self.findings.get('javascript', {}).get('api_calls', [])),
                'sensitive_items': len(self.findings.get('sensitive', {}).get('api_keys', [])),
                'risk_assessment': self._assess_risk()
            }
        }
        
        report_file = f"html_content_analysis_{self.session_id}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        return report_file
    
    def _assess_risk(self):
        """è¯„ä¼°é£é™©ç­‰çº§"""
        sensitive_count = len(self.findings.get('sensitive', {}).get('api_keys', []))
        debug_count = len(self.findings.get('sensitive', {}).get('debug_info', []))
        api_count = len(self.findings.get('javascript', {}).get('api_calls', []))
        
        if sensitive_count > 0:
            return "HIGH"
        elif debug_count > 2 or api_count > 5:
            return "MEDIUM"
        else:
            return "LOW"
    
    def print_summary(self):
        """æ‰“å°åˆ†ææ‘˜è¦"""
        print(f"\nğŸ¦… HTMLå†…å®¹åˆ†æå®Œæˆæ‘˜è¦:")
        print(f"ğŸ“„ HTMLé•¿åº¦: {len(self.html_content)}å­—ç¬¦")
        print(f"ğŸ“œ JavaScriptæ–‡ä»¶: {len(self.findings.get('script_sources', []))}ä¸ª")
        print(f"ğŸ¨ CSSæ–‡ä»¶: {len(self.findings.get('css_sources', []))}ä¸ª")
        print(f"ğŸŒ å¤–éƒ¨åŸŸå: {len(self.findings.get('network', {}).get('external_domains', []))}ä¸ª")
        print(f"ğŸ”— APIç«¯ç‚¹: {len(self.findings.get('javascript', {}).get('api_calls', []))}ä¸ª")
        print(f"ğŸ”‘ æ•æ„Ÿä¿¡æ¯: {len(self.findings.get('sensitive', {}).get('api_keys', []))}ä¸ª")
        print(f"ğŸš¨ é£é™©ç­‰çº§: {self._assess_risk()}")
    
    async def execute_full_analysis(self):
        """æ‰§è¡Œå®Œæ•´åˆ†æ"""
        print("ğŸ” HTMLå†…å®¹æ·±åº¦åˆ†æå¯åŠ¨!")
        
        if not await self.download_html_content():
            return None
        
        # æ‰§è¡Œå„ç§åˆ†æ
        self.extract_application_info()
        self.extract_inline_javascript()
        self.extract_network_resources()
        self.search_sensitive_patterns()
        
        # ç”ŸæˆæŠ¥å‘Š
        report_file = self.generate_comprehensive_report()
        self.print_summary()
        
        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Š: {report_file}")
        return report_file

async def main():
    extractor = HTMLContentExtractor()
    await extractor.execute_full_analysis()

if __name__ == "__main__":
    asyncio.run(main()) 